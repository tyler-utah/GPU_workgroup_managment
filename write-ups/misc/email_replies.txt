We sent the following email to several people to understand why
GPUs don't support stronger forward progress properties:

Our latest research interests have been in thinking about forward 
progress properties across workgroups (or blocks in CUDA) for GPUs and 
the types of programs that can be written under these constraints. We 
are trying to more clearly understand the reasons for such 
forward progress constraints and what they may be in the near future. 

Our recently accepted OOPSLA paper "Portable Inter-Workgroup Barrier 
Synchronisation for GPUs" (attached), describes an inter-workgroup 
barrier that synchronises only across workgroups that were observed to 
be concurrently executing at the start of the kernel. This barrier 
requires the inter-workgroup forward progress guarantee that once a 
workgroup begins execution, it will continue to be fairly executed 
until it finishes execution. We show in the paper that a wide range of 
GPUs (Nvidia, AMD, ARM, Intel) appear to have this forward progress 
guarantee and we did not observe any cases where it was violated. 

However, this forward progress property that we rely on is not 
provided by the OpenCL specification. In fact, OpenCL specifically 
states that it offers no forward progress guarantees between 
workgroups. Similarly, CUDA provides no such guarantees. 

We have chatted informally with several people involved in the 
specification and from various vendors. We came away with the 
impression that the community is reluctant to commit to forward 
progress guarantees between workgroups. We would like to more clearly 
understand the reasons behind this reluctance to help direct our 
future research. Our current understanding is that forward progress 
between workgroups may not be supported in the future due to: 

GPU multi-tasking - A high priority GPU task (e.g. the OS graphics 
layer) may need GPU resources currently in use (e.g. for a compute 
kernel). The high priority task would like to be able to quickly obtain 
resources, which may reduce the number of workgroups that are able to 
execute the compute kernel. 

Energy throttling - GPUs running on mobile devices may wish to 
dynamically reduce the number of executing workgroups to conserve 
energy and increase battery life. 

Do these reasons match your views? Would you be able to provide 
additional insights? 

--------------------------

From Paul Kelly:

Hi Ally, Tyler

Thanks for getting in touch about this – it’s something we have
thought about.  We have seen clear performance improvements (eg in
conjugate gradient solvers, avoiding a separate kernel launch for the
matrix-vector multiply and the dot product) from doing this (albeit
unsafely at the time).  When we told NVidia happily how we had
achieved a bit speedup this way, the response was “yes but you *must*
not do that”.  Presumably for the reasons you give.

It is clearly useful to be able to dynamically manage the number of
workgroups (this is sometimes called “malleability” in HPC – where we
for example have an MPI job running on 100 processes and we decide we
want to squish it down to 75 to make room for another job).  As you
observe, current GPUs achieve malleability by reassigning workgroups
at kernel launch time only.

So jobs that hog the GPU for a long period and do not return for a
relaunch deny the OS the opportunity to exploit malleability to
improve scheduling.  This could impact multitasking (leading to
graphics glitches for example), power management, and perhaps
virtualisation.

And the GPU vendors want to reserve the right to do something more
dynamic.

This incidentally is consistent with the rumours I hear about where
NVidia are going with future GPU architectures that will support a
more dynamic population of workgroups – you perhaps know more about
this that I do (I think much is known but under NDA).

One question I find myself asking is why kernel relaunch is expensive.
The common case is basically to do what you have implemented.  Your
barrier library should be able to synchronise, then query the driver
(or a shared-memory flag) to see if a reschedule is requested, and if
not continue.  So this whole thing *should* be a non-issue?

Hope this helps!

Paul


--------------------------

From Lee Howes

Hi Tyler,

Yes, I have some experience of this. I wrote or was involved in the
current text in the OpenCL and HSA specs on forward progress
guarantees, and made sure that Qualcomm's design had a certain basic
level of functionality.

It is definitely the case that the community is reluctant to commit to
stronger guarantees. I think this is misguided, but can see the
reason. There is definitely a performance cost to the more simple fair
scheduling algorithms out there and they can seriously harm the
performance of traditional graphics code. You can work around that by
having multiple scheduling schemes for different use cases, or you can
try to develop a more sophisticated scheme (which is what NVIDIA has
done, if I understand my conversations with their engineers on the
subject). One of the big disappointments of HSA is that it talked
vaguely about forward progress, but never really specified what that
meant, so the guarantee is, in my opinion, completely worthless. The
more recent HSA spec improved this a bit but an unwillingness to make
a strong guarantee still shows through.

So what leads to the weakness here? Well, really it is two things. The
first is that thread to thread forward progress is not guaranteed. The
scheduler tends to be something simple and will not force a thread out
of a spin loop, for example. That means you can make no guarantees
between threads in a work-group. On top of that there are the obvious
intra-thread issues, but we can fix that with something like OpenCL's
subgroup queries, which allow the compiler to define the granularity
of control flow, so that is feasible. Then there is the question of
how work-groups are scheduled - so in your case you assume that
workgroups are run on different cores (or at last by not launching too
many you can rely on that) and that the different cores run
independently. That is probably largely true, but I'd be wary of
relying on it. I don't know how well you can guarantee that one
workgroup won't take priority in the memory system and block another
from progressing. The hardware scheduler might also schedule two
groups on the same core by surprise (look at how Bulldozer distributed
threads in the first generation, it ended up putting the first two
threads on the same module and so wasting floating point units).

GPU multitasking and throttling may be an issue. Certainly early on in
HSA discussions we talked about that. I've not seen anyone implement
it yet - the GPU scheduling I've seen will tend to yank whole kernels
off for another. It does give more freedom on how you do that though
if you can get away with just pulling one core out of use and
replacing it with some other workload without having to worry about
context switching other cores. It is easier to design a scheduler that
pulls the whole kernel off at once, but you have to be really careful
about how you do that given how much register state there is. Context
switches need significant memory traffic each time.

I would like to see stronger guarantees, but I never quite got to a
design in HSA or OpenCL that I could get all vendors to agree on. The
best we got to was that it could at least be disabled for different
workloads, and even then vendors are wary. One example of why is that
if they want to run openCL code and OpenGL code simultaneously, they
have to fall back to the OpenCL scheduling rules and thus slow the GL
code significantly. Even within the committee it was hard to get very
clear information from the different vendors what their hardware was
capable of, for obvious reasons. You might have more luck there but it
won't help much if you want to publish the results :)

Lee

--------------------------

From Andrew Richards

Hi Ally,

I think the reasons you cite are probably the right ones. 

Many GPUs do not allow graphics and compute to take over the GPU at
the same time, but that is not ideal and GPU vendors may want to
change that in future. It makes the system unresponsive for
long-running kernels. So, even if you have occupancy guarantees now,
you may lose them in future and the vendors won't want software to
hang with a new piece of hardware.

A high priority task might take over parts of the GPU. I'm not sure
anyone currently does this, but it would be good if they did.

AMD's new system has preemption, although with a limited number of
hardware threads that need to be pre-allocated before kicking off the
kernel. I think this means that AMD's implementation of HSA has much
better forward progress guarantees.

Throttling is an issue. It's highly likely a core will be shut down to
reduce power consumption and heat.

OpenCL supports FPGAs. I don't know what independent forward progress
means in the case of an FPGA.

--------------------------
