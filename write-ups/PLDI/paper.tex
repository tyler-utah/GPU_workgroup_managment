%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[numbers,nocopyrightspace,10pt]{sigplanconf}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}
\usepackage{mathpartir}

\usepackage{booktabs}
\usepackage{subfig}


\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
\newcommand{\HEComment}[1]{\textcolor{orange}{HE: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}
\newcommand{\TSComment}[1]{\textcolor{purple}{TS: #1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray3}{gray}{0.9}
\definecolor{Gray2}{gray}{0.75}
\definecolor{Gray1}{gray}{0.6}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then,transmit},
  escapeinside={(*@}{@*)},
  numbers=left
}

\newcommand{\transmit}{\mathsf{transmit}}


\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}
\newcommand{\globalbarrier}{\mathsf{global\_barrier}}
\newcommand{\resizingglobalbarrier}{\mathsf{resizing\_global\_barrier}}
\newcommand{\getgroupid}{\mathsf{get\_group\_id}}
\newcommand{\getnumgroups}{\mathsf{get\_num\_groups}}
\newcommand{\getlocalid}{\mathsf{get\_local\_id}}
\newcommand{\getglobalid}{\mathsf{get\_global\_id}}
\newcommand{\getlocalsize}{\mathsf{get\_local\_size}}
\newcommand{\getglobalsize}{\mathsf{get\_global\_size}}

\newcommand{\keyword}[1]{\mathsf{#1}}

\newcommand{\NumAlgorithms}{8}

\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}
There is growing interest in accelerating irregular data-parallel
algorithms on GPUs.  These algorithms are typically \emph{blocking},
so require fair scheduling.  But GPU programming models (e.g.\ OpenCL)
do not mandate fair scheduling, and GPU schedulers are inherently
unfair in practice.  This issue is currently circumvented by
exploiting scheduling quirks particular to today's GPUs in a manner
that does not allow the GPU to be shared with other workloads (such as
graphics rendering tasks).  We propose \emph{cooperative kernels}, an
extension to the traditional GPU programming model geared towards
writing blocking algorithms.  Workgroups of a cooperative kernel are
fairly scheduled, supporting blocking algorithms, and co-scheduling of
other tasks is supported via a small set of language extensions that
create a contract between the programmer and the scheduler.  We
describe the semantics of the cooperative kernels programming model
and discuss our portable prototype implementation.  We evaluate the
technique by porting a set of irregular work stealing and graph
algorithms to our programming model and assessing the performance and
responsiveness of the approach across several GPUs.  Because our
implementation exploits no vendor-specific hardware, driver or
compiler support, our encouraging experimental results provide a
lower-bound on the efficiency with which cooperative kernels can be
implemented in practice.

\end{abstract}
    
\section{Introduction}\label{sec:intro}

\paragraph{The needs of irregular data-parallel algorithms}
%Acceleration of general-purpose computations on graphics processing
%units (GPUs) has tended to focus on \emph{regular} data-parallel
%algorithms, for which work to be processed can be evenly split between
%the workgroups that execute a GPU kernel ahead of time.  However,
Many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  The last decade has seen growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate linked data structures~\cite{owens-persistent,TPO10,DBLP:conf/ipps/KaleemVPHP16,DBLP:conf/ipps/DavidsonBGO14,DBLP:conf/hipc/HarishN07,DBLP:journals/topc/MerrillGG15,DBLP:conf/egh/VineetHPN09,DBLP:conf/ppopp/NobariCKB12,DBLP:conf/hpcc/SolomonTT10a,DBLP:conf/popl/PrabhuRMH11,DBLP:conf/ppopp/Mendez-LojoBP12,DBLP:conf/oopsla/PaiP16,DBLP:conf/ipps/KaleemVPHP16,DBLP:conf/oopsla/SorensenDBGR16,dlb-web,TPO10}.

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, many graph algorithms employ a level-by-level strategy, requiring a global barrier between each level, and 
work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling, which in the context of GPUs means fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if one workgroup holds a mutex, an unfair scheduler may cause
another workgroup to spin-wait forever for the mutex to be
released.  Similarly, an unfair scheduler can cause a workgroup to spin-wait
indefinitely at a global barrier so that other workgroups do not reach the barrier.

\paragraph{A degree of fairness: occupancy-bound execution}
GPU schedulers are \emph{not} fair. the current GPU
programming models---OpenCL~\cite{opencl2Spec}, \ADComment{Tyler, please add best CUDA since I've asked you to add it below in any case} CUDA~\cite{...}  and
HSA~\cite{HSAprogramming11}---specify almost no guarantees regarding scheduling of
workgroups, and current GPUs do not
provide fair scheduling in practice.  Roughly speaking, each workgroup
executing a GPU kernel is mapped to a hardware \emph{compute
  unit}.\footnote{In practice the situation can be more complex: if a
  kernel is sufficiently simple and if workgroups are small enough,
  multiple workgroups may be mapped to a single compute unit.  We assume, for the purpose of the current discussion, that the kernel is sufficiently complex that this does not happen.}
%
The simplest way for a GPU driver to handle 
the case where more workgroups are launched than there are compute
units is via an \emph{occupancy-bound}
execution model~\cite{DBLP:conf/oopsla/SorensenDBGR16}. \ADComment{Tyler, please add best cite for OB model; I guess our OOPSLA paper introduced the term, but we should cite that + Owens?}  With this model, once a workgroup has commenced execution on a
compute unit (it has become \emph{occupant}), the workgroup has
exclusive access to the compute unit until it has finished execution,
after which a subsequent workgroup can be bound to the compute unit.
Experiments suggest that the occupancy-bound execution model is widely
implemented by today's GPU devices and drivers~\cite{DBLP:conf/oopsla/SorensenDBGR16}. \ADComment{For the previous cite, should we cite other papers as well as ours?}  It is
evidently a simple model for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, which are
bound to separate compute units that operate in parallel.  Current GPU
implementations of irregular data parallel algorithms assume the
occupancy-bound execution model, which they exploit by launching 
no more workgroups than there are available compute
units~\cite{...}. \ADComment{Tyler, please pick best cite for previous
  claim.} This works because \emph{today}'s GPUs seem to provide the
occupancy-bound execution model.

\paragraph{Resistance to guaranteeing occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable.
% from the point-of-view of
%GPU vendors and end users.
First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all compute units, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.
%Precisely because
%of this problem, operating systems employ a GPU \emph{watchdog} to
%heuristically detect and kill long-running computations on the GPU
%being used for display rendering~\cite{DBLP:conf/iwocl/SorensenD16}.
Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets~\cite{DBLP:journals/comsur/Vallina-RodriguezC13}.  In the future, it will be desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling would require being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts
(including~\cite{PersonalCommunicationRichards,PersonalCommunicationHowes}
and various engineers from GPU vendors), is that (1) GPU vendors will
not commit to the occupancy-bound execution model that they currently
implement, due to the need for multi-tasking and energy throttling,
yet (2) GPU vendors will not guarantee fair scheduling, due to the
runtime overhead associated with supporting full preemption of
workgroups~\cite{ISCAPAPERSreeMentioned}.  Vendors instead wish to
retain the essence of the occupancy-bound model, which is
simple to implement efficiently, supporting preemption
only in key special cases.


\paragraph{Our proposal: cooperative kernels}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and is written using two additional
language primitives, $\offerkill$ and $\offerfork$, placed by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$, offering to
sacrifice itself to the scheduler.  This indicates that the workgroup
would ideally continue with its execution but that, if required, the
scheduler may destroy the workgroup for good: the cooperative kernel
is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commence execution directly after the $\offerfork$ program point.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel.
Functionally, the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled, while the cooperative kernel must be
robust to workgroups leaving and joining the computation in response
to $\offerkill$ and $\offerfork$.  Non-functionally, a cooperative
kernel must ensure that $\offerkill$ is executed frequently enough
that the scheduler can accommodate soft-real time constraints,
e.g.\ allowing a smooth frame-rate for graphics, or allowing compute
units to be powered down quickly when required.  The scheduler should
respond to $\offerkill$ and $\offerfork$ calls in a manner that avoids
under-utilisation of hardware resources by the cooperative kernel,
e.g.\ the scheduler should not kill workgroups unless they are
required for another purpose, and should facilitate forking of
additional workgroups when possible.

The cooperative kernels programming model has several appealing
properties:

\begin{enumerate}

\item By providing fair scheduling between workgroups, cooperative
  kernels meet the needs of blocking algorithms, including irregular
  data-parallel algorithms.

\item The model has no impact on the development of regular
  (non-cooperative) compute and graphics kernels: these can be programmed exactly as they
  are now.
%, as can graphics shading routines written using APIs such as
%OpenGL, Vulkan and DirectX.

\item The model is backwards-compatible: if $\offerkill$/$\offerfork$ are ignored, a cooperative kernel will behave
  exactly as a regular kernel does on current GPUs.

\item Cooperative kernels can be implemented over the occupancy-bound
  execution model that current GPUs provide---we present a
  proof-of-concept implementation that requires
  no special-purpose hardware or driver support.

\item Our experiments with a range of applications using three GPUs show that the model can enable efficient multi-tasking of cooperative and non-cooperative
  tasks.

\end{enumerate}

Placing the new primitives requires manual effort, but our experience porting a representative set of
GPU-accelerated irregular algorithms to use cooperative kernels
(\mysec\ref{sec:portingalgorithms}) suggests that this is straightforward in practice.

In summary, our main contributions are:

\begin{itemize}

\item \emph{Cooperative kernels}, an extended GPU programming model that support the scheduling requirements of blocking algorithms (\mysec\ref{sec:cooperativekernels}). 

\item A proof-of-concept portable implementation of cooperative
  kernels on top OpenCL 2.0
  (\mysec\ref{sec:implementation}).

\item Experimental results assessing the overhead and responsiveness of the cooperative kernels approach over a set of irregular algorithms across three GPUs (\mysec\ref{sec:experiments}).

\end{itemize}

We begin by providing an overview of the OpenCL programming model and
two motivating examples of irregular algorithms (\mysec\ref{sec:background}).  At the end of the paper we discuss related work (\mysec\ref{sec:relatedwork}) and avenues for future work (\mysec\ref{sec:conclusion}).

\section{Background and Motivating Examples}\label{sec:background}

We outline the industry-standard OpenCL programming model on which we
base cooperative kernels (\mysec\ref{sec:opencl}), and illustrate
OpenCL and the scheduling requirements of irregular algorithms using two examples: a work stealing queue and frontier-based graph traversal
(\mysec\ref{sec:openclexamples}).

\subsection{OpenCL Background}\label{sec:opencl}

% \paragraph{OpenCL concepts}
% An OpenCL program is typically executed by many \emph{threads} running
% in parallel. Threads are organized into \emph{workgroups} of equal
% size. The memory is organized in three levels: \emph{global} memory is
% accessible by any thread, \emph{local} memory is shared at the workgroup
% level (i.e., threads of different workgroups cannot access the same
% local memory), and \emph{private} memory is dedicated to thread-level
% storage and cannot be shared between threads. These concepts are
% illustrated over the two examples.


An OpenCL program is divided into \emph{host} and \emph{device}
components.  A host application runs on the CPU and launches one or
more \emph{kernels} that run on accelerator devices---GPUs in the
context of this paper.  A kernel is written in OpenCL C, based on C99.
All threads executing a kernel execute the same entry function with
identical arguments.  A thread can call $\getglobalid()$
to obtain its unique id, to access distinct data or follow different control flow paths to other threads.

The threads executing a kernel are divided into a number of \emph{workgroups}, each consisting of a fixed number of threads.  Builtin functions
$\getlocalid()$ and $\getgroupid()$ return a thread's local id within
its workgroup and the id of the workgroup, respectively.\footnote{For simplicity we restrict attention to one-dimensional kernels, which captures all the
GPU-accelerated irregular algorithms we are aware of, and use
e.g.\ $\getglobalid()$ to mean $\getglobalid(0)$.
}  The number
of threads per workgroup and the number of workgroups are obtained via
$\getlocalsize()$ and $\getnumgroups()$.  A thread's local and global
id are related via the equation $\getglobalid() = \getgroupid() \times
\getlocalsize() + \getlocalid()$.  The total number of threads executing the kernel is given by $\getglobalsize() = \getlocalsize()\times\getnumgroups()$.

Execution of the threads in a workgroup can be synchronised via a
workgroup barrier, which also ensures memory consistency.  This
barrier is a \emph{workgroup-level} function: it can only
appear in conditional code if all threads in a workgroup that reach
the barrier agree on the guards of all enclosing conditional
statements.  A \emph{global} barrier
(synchronising all threads of a kernel) is \emph{not} provided as
a primitive.

\paragraph{Memory spaces and memory model}
An OpenCL kernel has access to four memory spaces.  \emph{Shared virtual
memory} (SVM) is accessible to all threads and the host application concurrently.  \emph{Global} memory is shared among all threads executing a
kernel.  Each workgroup has its own portion of \emph{local} memory for fast intra-workgroup communication.  Every thread has
an allocation of very fast \emph{private} memory, where
function-local variables are allocated.

Communication within a workgroup can be achieved
using a workgroup barrier.  Finer-grained
communication within a workgroup, as well as inter-workgroup
communication and communication with the host while the kernel is
running, is enabled by a set of atomic data types and operations.  In
particular, fine-grained host/device communication is via atomic
operations on SVM.

\paragraph{Execution model}

OpenCL specifically makes no guarantees about fair scheduling between
workgroups executing the same kernel, stating~\cite[p.\ 31]{opencl2Spec}: \emph{``A
  conforming implementation may choose to serialize the workgroups 
%so
%  a correct algorithm cannot assume that workgroups will execute in
%  parallel.  
\dots There is no safe and portable way to synchronize across
  the independent execution of workgroups since once in the work-pool,
  they can execute in any order.''}  \ADComment{Tyler: please provide CUDA reference with page} CUDA similarly provides no guarantees~\cite{...}.

HSA provides limited, one-way guarantees,
stating~\cite[p. 46]{HSAprogramming11}: \emph{Work-group A can wait
  for values written by work-group B without deadlock provided ... A
  comes after B in work-group flattened ID order}. However, this
one-way communication is not sufficient to support blocking algorithms that use
mutexes and inter-workgroup barriers, both of which require \emph{symmetric} communication between
threads.


\subsection{Motivating Examples}\label{sec:openclexamples}

\begin{figure}

\begin{lstlisting}
(*@\label{line:wksteal:kernelfunc}@*)kernel work_stealing(global Task * queues) {
(*@\label{line:wksteal:getgroupid}@*)  int queue_id = get_group_id();
  while (more_work(queues)) {
(*@\label{line:wksteal:poporsteal}@*)    Task * t = pop_or_steal(queues, queue_id);
    if (t) {
      // task processing may create more work
(*@\label{line:wksteal:processtask}@*)      process_task(t, queues, queue_id);
    }
  }
}
\end{lstlisting}

\caption{An excerpt of a work stealing algorithm in OpenCL}\label{fig:workstealing}
\end{figure}

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph * g, 
                 global nodes * n0, global nodes * n1) {
  int level = 0;
  global nodes * in_nodes = n0;
  global nodes * out_nodes = n1;

  int tid = get_global_id();
  int stride = get_global_size();

(*@\label{line:graph:iterate}@*)  while(in_nodes.size > 0) {

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

(*@\label{line:graph:swap}@*)    swap(&in_nodes, &out_nodes);
(*@\label{line:graph:gb1}@*)    global_barrier();
(*@\label{line:graph:reset}@*)    reset(out_nodes);
    level++;
(*@\label{line:graph:gb2}@*)    global_barrier();
  }
}
\end{lstlisting}
\caption{An OpenCL graph traversal algorithm, using a global barrier for synchronisation}\label{fig:graphsearch}
\end{figure}

\paragraph{Work stealing example}
%
Work stealing enables dynamic balancing of tasks across several
processing units, and is useful when the number of tasks to be
processed in parallel is dynamic, because processing
one task may create an arbitrary number of new tasks.  In the context
of GPUs, each workgroup has an associated queue from which it
obtains tasks to process, and to which it stores newly created
tasks. If a workgroup's queue is empty, the workgroup tries to \emph{steal} a task from an other
queue. Workgroups keep on processing tasks until all the work is
done. Previous work addresses the feasibility and efficiency of work
stealing on GPUs~\cite{dlb-web,TPO10}.

\myfiglong\ref{fig:workstealing} shows a simplified version of an OpenCL
kernel that uses a work stealing queue. All threads start their
execution at the function marked with the $\mathsf{kernel}$ keyword
(line~\ref{line:wksteal:kernelfunc}). They receive a pointer to the task
queues, in global memory, initialized by the
host to contain the initial tasks. Each thread identifies
the workgroup it belongs to via the function $\getgroupid()$
(line~\ref{line:wksteal:getgroupid}). Here the group id is stored into
the variable $\mathsf{queue\_id}$ (which is, by default, in private
memory) and used to access the relevant task queue. The
$\mathsf{pop\_or\_steal}$ function (line~\ref{line:wksteal:poporsteal})
is called by all threads of a workgroup.  Inside
$\mathsf{pop\_or\_steal}$ only the thread with a local id equal to $0$,
the \emph{master thread}, will try to pop a
task from the workgroup's queue, or steal a task from a different
queue. The master thread uses local memory inside
$\mathsf{pop\_or\_steal}$ to ensure that the returned value is valid for
all threads of its workgroup. If a task was obtained, then all threads
of the workgroup participate in the processing of the task via 
$\mathsf{process\_task}$ (line~\ref{line:wksteal:processtask}).
Processing this task may lead to the creation of further tasks, which
the master thread will push to the workgroup's queue.

Although not depicted here, concurrent accesses to queues inside
$\mathsf{more\_work}$ and $\mathsf{pop\_or\_steal}$ are guarded by a
mutex per queue, implemented using atomic compare and swap operations
on global memory.  As a result, the kernel presents two opportunities
for spin-waiting: spinning to obtain a mutex, and spinning in the main
kernel loop to obtain a task to process.  The algorithm thus depends
on a fair scheduler, which, as discussed in \mysec\ref{sec:intro}, is
not guaranteed by current GPU programming models.

\paragraph{Graph traversal example} \myfiglong\ref{fig:graphsearch} shows the
skeleton of a frontier-based graph traversal algorithm; such algorithms have
been shown to be execute efficiently on GPUs~\cite{...}. \ADComment{Tyler, can you please work out which of the graph algorithm papers is the right one to cite here?}
The kernel is
given three arguments in global memory: a graph structure, and two
arrays that can contain graph nodes. Initially, $\keyword{n0}$ contains the
starting nodes to process. Threads keep three local variables: $\keyword{level}$, which indicates the frontier level currently being
processed, and $\keyword{in\_nodes}$ and $\keyword{out\_nodes}$, which point to
distinct node arrays. The nodes to be
processed for the current frontier are contained in $\keyword{in\_nodes}$, while $\keyword{out\_nodes}$ contains
the nodes to be processed during the next frontier.

The application iterates as long as the current frontier contains
nodes to process (line~\ref{line:graph:iterate}). At each frontier,
the nodes to be processed are evenly distributed between the executing
threads. This is done through \emph{stride} based processing. 
%
In this case, the stride is the total number of threads, obtained via 
$\getglobalsize()$.  Each thread processes nodes with the following sequence of indices:
\code{$\keyword{tid}$+$\keyword{stride}$*0},
\code{$\keyword{tid}$+$\keyword{stride}$*1},
\code{$\keyword{tid}$+$\keyword{stride}$*2}, etc., where $\keyword{tid}$ is a thread's global id.
A thread processes a node via $\keyword{process\_node}$, which may (a) push nodes to process in the next frontier to
$\keyword{out\_nodes}$ and (b) use the current frontier, $\keyword{level}$, in
the computation. After processing the frontier, the threads swap their
node array pointers (line~\ref{line:graph:swap}).

At this point, the GPU threads must wait for all other threads to
finishing processing the frontier before they can continue. To achieve
this, we use a global barrier construct
(line~\ref{line:graph:gb1}). After all threads reach this point, the
output node array is reset (line~\ref{line:graph:reset}) and the level
is incremented. Again, threads must wait until the output node is
reset at a global barrier (line~\ref{line:graph:gb2}). At this point
threads can continue to the next frontier.

The global barrier used in this application is not provided as a GPU
primitive, though previous works have shown that such a global
barrier can be implemented~\cite{XF10,DBLP:conf/oopsla/SorensenDBGR16},
based on CPU barrier designs~\cite[ch. 17]{HS08}.
 These barriers employ spinning to ensure threads wait at the barrier until all
threads have arrived, which requires fair
scheduling between workgroups for the barrier to operate correctly.
Again, fairness is not guaranteed by current GPU programming models.

The mutexes and barriers required by these two examples appear to run
reliably on current GPUs for kernels that are executed with no more
workgroups than there are compute units; this is due to the fairness
of the occupancy-bound execution model that current GPUs have been
shown, experimentally, to provide.  But, as discussed in
\mysec\ref{sec:intro}, this model is not endorsed
language standards or vendor implementations, and is unlikely to be respected in the future.
%
In \mysec\ref{sec:programmingguidelines} we show how the work stealing
and graph traversal examples of \myfigs\ref{fig:workstealing} and~\ref{fig:graphsearch} can be
updated to use our cooperative kernels programming model to resolve
the scheduling issue.


\section{Cooperative Kernels}\label{sec:cooperativekernels}

We describe our cooperative kernels programming model as an extension
to OpenCL.  However, the cooperative kernels concept is more general,
and could be applied to extend other GPU programming models, such as
CUDA and HSA.

We describe the semantics of cooperative kernels
(\mysec\ref{sec:semantics}) and summarise the important
differences between programming with regular vs.\ cooperative kernels
by showing how the motivating examples of \mysec\ref{sec:openclexamples} can be adapted to use cooperative kernels
(\mysec\ref{sec:programmingguidelines}).
We then focus on the non-functional
properties that a developer of a cooperative kernel, and the
implementer of a scheduler for cooperative kernels, should strive for
(\mysec\ref{sec:nonfunctional}).  The semantics of cooperative
kernels has been guided by the practical applications we have studied
(described in \mysec\ref{sec:portingalgorithms}), and we discuss several cases where we
might have taken different and also reasonable semantic decisions (\mysec\ref{sec:semanticalternatives}).
We conclude the section by discussing the backwards-compatible nature
of cooperative kernels with respect to blocking algorithms designed to
run on current GPU platforms
(\mysec\ref{sec:backwardscompatibility}).

\subsection{Semantics of Cooperative Kernels}\label{sec:semantics}

As with a regular OpenCL kernel (see \mysec\ref{sec:opencl}), a
cooperative kernel is launched by the host application.  The host
application passes parameters to the kernel and specifies a desired
number of workgroups, each consisting of a specified number of
threads.  Unlike in a regular kernel, the parameters to a cooperative kernel are immutable (though pointer
parameters can refer to mutable data).

Cooperative kernels are written using the following 
extensions: $\transmit$, a qualifier on the variables of a
thread; $\offerkill$ and $\offerfork$, the key functions that enable
cooperative scheduling; and $\globalbarrier$ and $\resizingglobalbarrier$
primitives for inter-workgroup synchronisation.
%
We present the semantics of these constructs intuitively, using English.
In Appendix~\ref{appendix:semantics} (see supplementary material) we present an abstract operational semantics for our language extensions in a simple GPU-like programming model.

\paragraph{Transmitted variables}

A variable declared in the root scope of the cooperative kernel can
optionally be annotated with a new $\transmit$ qualifier.  Annotating
a variable $v$ with $\transmit$ means that when a workgroup spawns new
workgroups by calling $\offerfork$, the workgroup should transmit its
current value for $v$ to the threads of the new workgroups, to serve
as an initial value for $v$.  We detail the semantics for this when we
describe $\offerfork$ below.

\paragraph{Active workroups}

If the host application launches a cooperative kernel requesting $N$
workgroups, this indicates that the kernel should be executed with a
maximum of $N$ workgroups, and that as many workgroups as possible, up
to this limit, are desired.  However, the scheduler may initially
schedule fewer than $N$ workgroups, and as explained below the number
of workgroups that execute the cooperative kernel can change during
the lifetime of the kernel.

The number of \emph{active workgroups}---workgroups executing the
kernel---is denoted $M$.  The active workgroups
have consecutive workgroup ids in the range $[0, M-1]$.
Initially, at least one workgroup is active;
if necessary the scheduler must postpone execution of the kernel until
a some compute unit becomes available.  

When executed by a cooperative
kernel, $\getnumgroups()$ returns $M$, the \emph{current} number of
active workgroups.  This is in contrast to $\getnumgroups()$ for
regular kernels, which returns the fixed number of workgroups that
were requested at kernel launch time (see \mysec\ref{sec:opencl}).

Fair scheduling \emph{is} guaranteed between active workgroups.  That is, if
some thread in an active workgroup is enabled then eventually some
thread in the active workgroup is guaranteed to execute an
instruction.

%Note that our programming model does not mandate that
%threads within a workgroup must be fairly scheduled, and indeed this
%is often not the case in GPU programming models, because threads are
%often organised into \emph{warps} or \emph{wavefronts} within a
%workgroup, exhibiting lock-step predicated execution that is
%inherently unfair.

\paragraph{Semantics for $\offerkill$}

The $\offerkill$ primitive allows the cooperative kernel to return
compute units to the scheduler by offering to sacrifice workgroups.
The idea is as follows: allowing the scheduler to arbitrarily and abruptly terminate execution
of workgroups might be drastic, yet the kernel
may contain specific program points at which a workgroup could
\emph{gracefully} leave the computation.  We give examples of these
for work stealing and graph traversal in \mysec\ref{sec:programmingguidelines}.

Similar to the OpenCL workgroup $\mathsf{barrier}$ primitive,
$\offerkill$, is a workgroup-level function---it must be encountered
uniformly by all threads in a workgroup (see
\mysec\ref{sec:opencl}).

Suppose a workgroup with id $m$ executes $\offerkill$.  
If the workgroup has the highest largest id of all active workgroups then it can be killed by the scheduler, except that workgroup 0 can never be killed (to ensure that the kernel computation does not terminate early).  More formally, if $m < M-1$
or $M=1$ then $\offerkill$ is a no-op.  If instead $M > 1$ and $m =
M-1$, the scheduler can choose to ignore the offer, so that $\offerkill$
executes as a no-op, or accept the offer, so that execution of the workgroup ceases and
the number of active workgroups $M$ is atomically decremented by one.  In the second case, the scheduler can assign some other task to
the compute unit on which the workgroup was executing.


\paragraph{Semantics for $\offerfork$}

Recall that
a desired limit of $N$ workgroups was specified when the cooperative kernel was launched, but that the number of active workgroups, $M$, may be smaller
than $N$, either because (due to competing workloads) the scheduler
did not provide $N$ workgroups initially, or because the kernel has
given up some workgroups via $\offerkill$ calls.  Through the
$\offerfork$ primitive (also a workgroup-level function), the kernel and scheduler can collaborate to allow new
workgroups to join the computation at an appropriate point and with
appropriate state.

Suppose a workgroup with id $m\leq M$ executes $\offerfork$.  Then the following occurs: an
integer $k \in [0, N-M]$ is chosen by the scheduler;
$k$ new workgroups are spawned with consecutive ids in the range $[M,
  M+k-1]$; the active workgroup count $M$ is atomically incremented by $k$.

The $k$ new workgroups commence execution at the program point
immediately following the $\offerfork$ call.  The variables that
describe the state of a thread are all uninitialised for the threads
in the new workgroups; reading from these variables without first
initialising them is an undefined behaviour.  There are two exceptions
to this: (1) because the parameters to a cooperative kernel are
immutable, the new threads have access to these parameters as part of
their local state and can safely read from them; (2) for each variable
$v$ annotated with $\transmit$, every new thread's copy of $v$ is
initialised to the value that thread 0 in workgroup $m$ held for $v$
at the point of the $\offerfork$ call.
%
In effect, thread 0 of the forking workgroup \emph{transmits} the relevant
portion of its local state to the threads of the forked workgroups.

%We discuss in \mysec\ref{sec:semanticalternatives} the rationale for
%having all new threads obtain this local state portion from thread 0,
%as well as our reasons for selecting which variables to transmit via
%annotations rather than transmitting the entire thread state.

Notice that $k=0$ is always a legitimate choice for the number of
workgroups to be spawned by an $\offerfork$ call, and if the number of
active workgroups $M$ is equal to the workgroup limit $N$, $k=0$ is
guaranteed.

\paragraph{Global barriers}

Many irregular algorithms require synchronisation across workgroups.
This can be achieved via a \emph{global barrier}, as illustrated in the graph traversal example of \mysec\ref{sec:openclexamples} and \myfig\ref{fig:graphsearch}.
Because a global barrier requires fair scheduling of workgroups it cannot be safely implemented in OpenCL, CUDA or HSA, and thus is not provided as a primitive in any of these programming models.

Workgroups of a \emph{cooperative} kernel \emph{are} fairly scheduled, so a
global barrier primitive can be provided, and we specify two variants: $\globalbarrier$
and $\resizingglobalbarrier$.

Our $\globalbarrier$ primitive is a kernel-level function: if it
appears in conditional code then it must be reached by \emph{all}
threads executing the cooperative kernel.  On reaching a
$\globalbarrier()$, a thread waits until all threads have arrived at
the barrier.  Once all threads have arrived, the threads may proceed
past the barrier with the guarantee that all global memory accesses
issued before the barrier have completed.  The $\globalbarrier$
primitive can be implemented by adapting an inter-workgroup barrier
design, e.g.~\cite{XF10}, to take account of a growing and shrinking number of workgroups, and the atomic operations provided by
the OpenCL 2.0 memory model enable a memory-safe
implementation~\cite{DBLP:conf/oopsla/SorensenDBGR16}.  However, implementing such a barrier is
involved, hence why we include this function as a primitive.

The $\resizingglobalbarrier$ primitive is also a kernel-level
function.  It is identical to $\globalbarrier$, except that it caters
for cooperation with the scheduler: by issuing a
$\resizingglobalbarrier$ the programmer indicates that the cooperative
kernel is prepared to proceed after the barrier with fewer workgroups,
or with additional workgroups joining the computation.

When all threads have reached $\resizingglobalbarrier$,
the number of active workgroups, $M$, is atomically set to a new value, $M'$ say, with $0 < M' \leq N$.
If $M' = M$ then the active workgroups remain unchanged.  If $M' < M$, workgroups $[M', M-1]$ are
killed.  If $M' > M$ then $M'-M$ new workgroups join the computation after the barrier,
as if they were forked from workgroup 0.  In particular, the
$\transmit$-annotated local state of thread 0 in workgroup 0 is
transmitted to the threads of the new workgroups.

The semantics of $\resizingglobalbarrier$ can be modelled via the following sequence of calls:

\lstset{basicstyle=\tt,numbers=none}
\begin{lstlisting}
  $\globalbarrier()$;
  if($\getgroupid()$ == 0) $\offerfork$();
  $\globalbarrier()$;
  $\offerkill()$;
  $\globalbarrier()$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt,numbers=left}

The enclosing $\globalbarrier$ calls ensure that the change in number
of active workgroups from $M$ to $M'$ occurs entirely within the
resizing barrier, so that from a programmer's perspective, $M$ changes atomically.  The middle $\globalbarrier$ ensures that forking occurs before killing, so that while the barrier may kill a workgroup with id $i$, it should not also fork a new workgroup with id $i$.  That is, workgroups $[0, \textrm{min}(M, M') - 1]$ should be left intact.

Because $\resizingglobalbarrier$ can be implemented in the above
manner, we do not regard it \emph{conceptually} as a primitive of our
programming model.  However, we discuss in
\mysec\ref{sec:resizingbarrier} how $\resizingglobalbarrier$ can be
implemented significantly more efficiently if it is treated as a
primitive operation that interacts directly with the scheduler.

\subsection{Programming With Cooperative Kernels}\label{sec:programmingguidelines}

We discuss the main semantic change induced by our programming
model---that the number of workgroups executing a kernel varies
dynamically---and illustrate the new language constructs and the
impact of this semantic change by adapting the work stealing and graph
traversal algorithms of \mysec\ref{sec:openclexamples} to use
cooperative kernels.

\paragraph{A changing number of workgroups}  Unlike in regular OpenCL,
the value returned by $\getnumgroups()$ is not fixed during the
lifetime of a cooperative kernel: it corresponds to the active group count $M$ and may change due to workgroups executing
$\offerkill$, $\offerfork$ and $\resizingglobalbarrier$.  Being a function of $\getnumgroups()$, the value returned by $\getglobalsize()$
is also subject to change.
A cooperative kernel must therefore be written in a manner that is
robust to changes in the values returned by these functions.

In general, their volatility means that use of these functions should be avoided.
However, the situation is more stable if a cooperative kernel does not directly
issue $\offerkill$ and $\offerfork$ calls directly.  In this case,
only calls to $\resizingglobalbarrier$ can cause the number of active
workgroups to change.  At any point
during execution, the threads of a kernel are executing between some
pair of resizing barrier calls, which we call a \emph{resizing barrier
  interval} (here we consider the kernel entry and exit points
conceptually to be special cases of resizing barriers).  The number of
workgroups executing the kernel is thus constant within each resizing
barrier interval, i.e.\ the result returned by repeated calls to
$\getnumgroups()$ will be identical during such an interval; the same holds for $\getglobalsize()$.
%
This guarantee can be exploited by algorithms that perform strided
data processing, which we illustrate below using the graph traversal
example.


\paragraph{Adapting work stealing}

\myfiglong\ref{fig:workstealing-cooperative} shows a cooperative
version of the work stealing example of \myfig\ref{fig:workstealing} from \mysec\ref{sec:openclexamples}.
%
In this example there is no state to transmit since a computation is
entirely parameterised by a task, which is retrieved from a queue
located in global memory. In the main loop, before trying to obtain a
task, we make the workgroup offer itself to be forked or killed
(lines~\ref{line:cwksteal:offerfork}-\ref{line:cwksteal:offerkill}). Note
that a workgroup may be killed even if its associated task queue is
not empty, since remaining tasks will be stolen by other
workgroups. Since $\offerfork$ may be the entry point of a workgroup,
the queue id must now be computed after it (line~\ref{line:cwksteal:getgroupid}). In particular, the queue
id cannot be transmitted since we want a newly spawned workgroup to
read its own queue and not the one of the forking workgroup.

% The queue id cannot be transmitted since it corresponds to the
% workgroup id.

\begin{figure}

\begin{lstlisting}
kernel work_stealing(global Task * queues) {
(*@\label{line:cwksteal:morework}@*)  while (more_work(queues)) {
(*@\label{line:cwksteal:offerfork}@*)    offer_fork();
(*@\label{line:cwksteal:offerkill}@*)    offer_kill();
(*@\label{line:cwksteal:getgroupid}@*)    int queue_id = get_group_id();
(*@\label{line:cwksteal:poporsteal}@*)    Task * t = pop_or_steal(queues, queue_id);
    if (t) {
      // task processing may create more work
(*@\label{line:cwksteal:processtask}@*)      process_task(t, queues, queue_id);
    }
  }
}
\end{lstlisting}

\caption{Cooperative version of the work stealing kernel of
  \myfig~\ref{fig:workstealing}, using $\offerfork$ and
  $\offerkill$}\label{fig:workstealing-cooperative}
\end{figure}

\paragraph{Adapting graph traversal} 
\myfiglong\ref{fig:cgraphsearch} shows a cooperative version of the
graph traversal kernel of \myfig\ref{fig:graphsearch} from
\mysec\ref{sec:openclexamples}.  On lines~\ref{line:cgraph:resizing1}
and ~\ref{line:cgraph:resizing2}, we change the original global
barriers into a resizing barriers. Several variables are marked to be
transmitted in the case of workgroups joining at the resizing barriers
(lines~\ref{line:cgraph:transmit1}, \ref{line:cgraph:transmit2} and
\ref{line:cgraph:transmit3}): $\keyword{level}$ must be restored so
that new workgroups know which frontier they are processing;
$\keyword{in\_nodes}$ and $\keyword{out\_nodes}$ must be restored so
that new workgroups know which of the node arrays to use for input and
output. Lastly, the static work distribution of the original kernel is
no longer valid in a cooperative kernel. This is because the stride
(which is based on $M$) may change after each resizing barrier
call. To fix this, we re-distribute the work after each resizing
barrier call by recomputing the thread id and stride
(lines~\ref{line:cgraph:rechunking1} and
\ref{line:cgraph:rechunking2}). This example exploits the fact that
the cooperative kernel does not issue $\offerkill$ nor $\offerfork$
directly: the value of $\keyword{stride}$ obtained from
$\getglobalsize()$ at line~\ref{line:cgraph:rechunking2} is stable
until the next resizing barrier at line~\ref{line:cgraph:resizing1},
and in particular can be safely used inside the $\keyword{for}$ loop.

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph *g, 
                 global nodes *n0, global nodes *n1) {

(*@\label{line:cgraph:transmit1}@*)  transmit int level = 0;
(*@\label{line:cgraph:transmit2}@*)  transmit global nodes *in_nodes = n0;
(*@\label{line:cgraph:transmit3}@*)  transmit global nodes *out_nodes = n1;

  while(in_nodes.size > 0) {

(*@\label{line:cgraph:rechunking1}@*)    int tid = get_global_id();
(*@\label{line:cgraph:rechunking2}@*)    int stride = get_global_size();

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

    swap(&in_nodes, &out_nodes);
(*@\label{line:cgraph:resizing1}@*)    resizing_global_barrier();
    reset(out_nodes);
    level++;
(*@\label{line:cgraph:resizing2}@*)    resizing_global_barrier();
  }
}
\end{lstlisting}
\caption{Cooperative version of the graph traversal kernel of \myfig\ref{fig:graphsearch}, using a resizing global barrier and $\transmit$ annotations}\label{fig:cgraphsearch}
\end{figure}

\paragraph{Patterns for irregular algorithms}
In \mysec\ref{sec:portingalgorithms} we describe the set of irregular GPU algorithms used
in our experiments, which largely captures the irregular blocking
algorithms that are available as open source GPU kernels.  These all
employ either work stealing or operate on graph data structures, and placing our new constructs follows a common, easy-to-follow pattern in each case.

Like the simplified example of \myfig\ref{fig:workstealing-cooperative}, the work stealing algorithms have a transactional flavour
and require little or no state to be carried between transactions.  Thus the point at which a workgroup is ready to process a new task is a natural point to place $\offerkill$ and $\offerfork$, and few or no $\transmit$ annotations are required.

The example of \myfig\ref{fig:cgraphsearch} is simpler than, but representative of,
most level-by-level graph algorithms.  These algorithms tend to
require some state to be transmitted, and in some cases
a global barrier is required in the midst of a transaction, in which
case $\globalbarrier$, but not $\resizingglobalbarrier$, is suitable
for achieving synchronization without disturbing the set of active
workgroups.  But often it is the case that on completing a level of
the graph algorithm, the next level could be processed by more or
fewer workgroups, something which $\resizingglobalbarrier$
facilitates.

Our practical experience suggests that cooperative kernels are a good
fit for at least these two classes of irregular algorithms, which is
of course partly because we were inspired by these algorithms in
designing the model.


\subsection{Non-Functional Requirements}\label{sec:nonfunctional}

The semantics presented in \mysec\ref{sec:semantics} describe the envelope of
behaviour that a developer of a cooperative kernel should be prepared
for.
%
However, the aim of cooperative kernels is to find a balance that
allows \emph{efficient} execution of algorithms that require fair scheduling, and
\emph{responsive} multi-tasking, so that the GPU can be shared between
cooperative kernels and other shorter tasks with soft real-time constraints.
%
To achieve this balance, an implementation of the cooperative
kernels model, and the programmer of a cooperative kernel, must strive
to meet the following non-functional requirements.

\paragraph{Sufficient $\offerkill$ calls}

The purpose of $\offerkill$ is to provide the scheduler with an
opportunity to destroy a workgroup in order to schedule
higher-priority tasks.  The scheduler relies on the cooperative kernel
to execute $\offerkill$ sufficiently frequently that soft real-time
constraints of other workloads can be met.
%
Using our work stealing example: in
\myfig\ref{fig:workstealing-cooperative} a workgroup offers itself to
the scheduler after processing each task.  If tasks are sufficiently
fast to process then the scheduler will have ample opportunities to
de-schedule workgroups.  But if tasks can be very time-consuming to
process then it might be necessary to rewrite the algorithm so that
tasks are shorter and more numerous, to achieve a higher rate of calls
to $\offerkill$.

Getting this non-functional requirement right depends on the
constraints of the tasks to be co-scheduled with a cooperative kernel,
on properties of the cooperative kernel itself, and on the efficiency
of the GPU that is used for execution.  In \mysec\ref{sec:sizingnoncoop} we conduct
experiments to understand the response rate that would be required to
co-schedule graphics rendering with a cooperative kernel in order to
achieve a smooth frame rate, and we evaluate responsiveness in
practice in \mysec\ref{sec:responsiveness}.

\paragraph{Sufficient workgroups for the cooperative kernel}

Recall that the cooperative kernel would ideally utilise the $N$
workgroups requested upon kernel launch, in order to conduct its
processing efficiently.  The scheduler should thus aim to provide the
cooperative kernel $N$ workgroups if other constraints allow it,
by accepting an $\offerkill$ only if a compute unit is required for another
task, and responding positively to $\offerfork$ calls if compute units are available.  Our
implementation of cooperative scheduling (\mysec\ref{sec:implementation}) is as
generous as possible to the cooperative kernel in this regard.

\ADComment{This could be shorter.}
However, these are not hard-and-fast rules: a scheduler might employ
heuristics based on historical workloads to avoid thrashing.  For
example, if graphics calls are known to arrive at a certain rate then
the scheduler might decide that it is worth accepting an $\offerkill$
from the cooperative kernel on the assumption that a graphics task
will be requested momentarily.  Similarly, the scheduler might provide
some, but not all, compute units to the cooperative kernel in response
to an $\offerfork$ call if it has reason to believe that the remaining
compute units are likely to be useful for other high priority tasks
that are likely to arrive imminently.  With some additional language extensions, it would be possible for the cooperative kernel to hint to the scheduler if, for example, the kernel is at a point of computation where there is less work to do than usual, so that $\offerkill$ calls could be accepted in order to save energy.


\subsection{Alternative Semantic Choices}\label{sec:semanticalternatives}

\ADComment{Move this to the appendix.}

\paragraph{Killing order}

We opted for a semantics whereby only the active workgroup with the
highest id can be killed.  This has an appealing property: it means
that the ids of active workgroups are contiguous, which is important
for processing of contiguous data.  The cooperative graph traversal
algorithm of \myfig\ref{fig:cgraphsearch} illustrates this: the algorithm is prepared
for $\getglobalsize()$ to change after each resizing barrier call, but
depends on the fact that $\getglobalid()$ returns a contiguous range
of thread ids.

A disadvantage of this decision is that it may provide sub-optimal
responsiveness from the point of view of the scheduler.  Suppose the
scheduler requires an additional compute unit, but the active thread
with the larget id is processing some computationally intensive work
and will take a while to reach $\offerkill$.  Our semantics means that
the scheuduler cannot take advantage of the fact that another active
workgroup may invoke $\offerkill$ sooner.

Cooperative kernels that do not require contiguous thread ids might me
more suited to a semantics in which workgroups can be killed in any
order, but where workgroup ids (and thus thread global ids) are not
guaranteed to be contiguous.

\paragraph{Keeping one workgroup alive}

Our semantics dictate that the workgroup with id 0 will not be killed
if it invokes $\offerkill$.  This avoids the possibility of the
cooperative kernel terminating early due to the programmer
inadvertantly allowing all workgroups to be killed, and the decision
to keep workgroup 0 alive fits well with our choice to kill workgroups
in descending order of id.

However, there might be a use case for a cooperative kernel reaching a
point where it would be acceptable for the kernel to exit, although
desirable for some remaining computation to be performed if competing
workloads allow it.  In this case, a semantics where all workgroups can be killed via $\offerkill$ would be appropriate, and the programmer would need to guard each $\offerkill$ with an id check in cases where killing all workgroups would be unacceptable.  For example:
%
\lstset{basicstyle=\tt,numbers=none}
\begin{lstlisting}
  if($\getgroupid{0}$ != 0) $\offerkill$();
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt,numbers=left}
%
would ensure that at least workgroup 0 is kept alive.

\paragraph{Transmission of partial state from a single thread}

Recall from the semantics of $\offerfork$ that newly forked workgroups
inherit the variable valuation associated with thread 0 of the forking
workgroup, but only for $\transmit$-annotated variables.  Alternative
choices here would be to have forked workgroups inherit values for
\emph{all} variables from the forking workgroup, and to have thread
$i$ in the forking workgroup provide the valuation for thread $i$ in
each spawned workgroup, rather than having thread 0 transmit the
valuation to all new threads.

We opted for transmitting only selected varibles based on the
observation that many of a thread's private variables are dead at the
point of issuing $\offerfork$ or $\resizingglobalbarrier$, thus it
would be wasteful to transmit them.  A live variable analysis could
instead be employed to over-approximate the variables that might be
accessed by newly arriving workgroups, so that these are automatically
transmitted.

In all cases, we found that a variable that needed to be transmitted
had the property of being uniform across the workgroup.  That is,
despite each thread having its own copy of the variable, each thread
is in agreement on the variable's value.  As an example, the
$\keyword{level}$, $\keyword{in\_nodes}$ and $\keyword{out\_nodes}$
variables used in \myfig\ref{fig:cgraphsearch} are all stored in thread-private
memory, but all threads in a workgroup agree on the values of these
variables at each $\resizingglobalbarrier$ call.  As a result,
transmitting the thread 0's valuation of the annotated variables is
equivalent to (and more efficient than) transmitting values on a
thread-by-thread basis.  We have not yet encountered a real-world
example where our current semantics would not suffice.



\subsection{Backwards Compatibility}\label{sec:backwardscompatibility}

A goal of coopeative kernels is to allow traditional, non-blocking GPU
tasks (e.g.\ graphics shaders in OpenGL, Vulkan and DirectX, and regular data-parallel computational kernels)
to be co-scheduled with blocking irregular algorithms.  It is
important to note that our new model has no effect on the manner in
which these traditional GPU workloads are programmed.

Furthermore, if our cooperative language extensions are
ignored---$\offerkill$ and $\offerfork$ are treated as no-ops,
$\transmit$ annotations are ignored, and $\globalbarrier$ and
$\resizingglobalbarrier$ calls are redirected to an existing global
barrier implementation (so that no resizing occurs in the latter
case)---then a cooperative kernel will behave just like its
non-cooperative counterpart would behave.  As a result, a developer
for whom today's occupancy-bound execution model suffices can upgrade
their kernel to a cooperative form and retain backwards-compatibility
with today's GPU platforms by pre-processing away our language
extensions.


\section{Prototype Implementation}\label{sec:implementation}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}

\ADComment{Give details of how host and device interact.}

\ADComment{Explain kernel merge tool briefly.  Explain library briefly.}

\ADComment{Give most attention to interesting performance and safety issues, e.g.\ reverse ticket lock, efficient barrier, the query functions...}

\ADComment{Host-device interaction through SVM.  Make clear that this is probably the most aggressive use of OpenCL 2.0 ever.}

\subsection{An Efficient Resizing Barrier}\label{sec:resizingbarrier}

\ADComment{Optimised barrier implementation.}

\subsection{Vendor Implementation Defects}

\ADComment{Briefly rant about our pain.  Compiler hang, compiler crashes, deadlocks, defunct processes.  Caution: don't make it sound so terrible that you can't trust our results.  Gently acknowledge that our implementation may be imperfect.}


\section{Applications and Experiments}\label{sec:experiments}

\subsection{Porting Irregular GPU Algorithms to Cooperative Kernels}\label{sec:portingalgorithms}

\begin{table}
\small
\centering
\begin{tabular}{ l c c c }
App. & resizing barriers & kill - fork & transmit vars\\
\hline
\rowcolor{Gray1}
color & 2 / 2 & 0 - 0 & 4\\
\rowcolor{Gray1}
mis & 3 / 3 & 0 - 0 & 0\\
\rowcolor{Gray1}
bc & 3 / 6 & 0 - 0 & 3\\
\rowcolor{Gray1}
p-sssp & 3 / 3 & 0 - 0 & 0 \\
\rowcolor{Gray2}
bfs & 2 / 2 & 0 - 0  & 4 \\
\rowcolor{Gray2}
l-sssp & 2 / 2 & 0 - 0  & 4 \\
\rowcolor{Gray3}
octree & 0 / 0 & 1 - 1 & 0\\
\rowcolor{Gray3}
game & 0 / 0 & 1 - 1 & 0\\
\end{tabular} \\
\vspace{.2cm}
\crule[Gray1]{.2cm}{.2cm} - Pannotia \hspace{.4cm} \crule[Gray2]{.2cm}{.2cm} - Lonestar GPU  \hspace{.4cm}  \crule[Gray3]{.2cm}{.2cm} - work stealing
\caption{Blocking GPU applications investigated}
\label{tab:applications}
\end{table}

\HEComment{Add citations for pannotia, lonestar, workstealing}

\mytablong~\ref{tab:applications} gives an overview of the 8 irregular
algorithms that we ported to cooperative kernels. Among them, 6 are
graph algorithms, based on Pannotia and Lonestar GPU application suite,
using global barriers; we indicate how many barriers become resizing
ones out of the total number of barriers, and how many variables need to
be transmitted. In all cases all barriers were converted, except for bc
which contains barriers deeper in the call stack, a case not supported
by our prototype. Still, these barriers could in principle also be
converted.  The remaining 2 algorithms are workstealing applications:
each required the addition of $\offerfork$ and $\offerkill$ at the start
of the main loop, and no variables needed to be transmitted. For all
these examples the porting to cooperative kernels was pretty
straightforward and did not require to bend the original algorithm.

\ADComment{Describe the algorithms that we ported.}

\ADComment{Give stats on how many variables had to be marked with
  $\transmit$ (i.e., occurred in the restoration context), how many
  barriers we changed to resizing, how many offer kill and offer fork
  calls we included.}

  \TSComment{check out Table~\ref{tab:applications}. The barrier column
  shows how many of the original barriers were ported to resizing
  barriers. In all cases except bc we converted all of them. The only
  reason why bc is different is because barriers occur deeper in the
  call stack and our prototype cannot handle that. The algorithm itself
  is compatable (i.e. changing these barriers is as easy as any other,
  there is nothing special about them.)}

\ADComment{Make sure we address the point that placing the primitives was pretty easy.}

\subsection{The GPUs we Tested}

\ADComment{Describe the platforms.  Explain why we could not try other
platforms.}

\HEComment{I consider that the reason why we need SVM fine grain and
  atomics is discussed in the implementation section}

Our implementation approach requires two optional features of OpenCL
$2.0$, namely SVM fine-grained buffers and SVM atomics, which are
currently not widely supported. \nvidia chips are ruled out since
\nvidia does not support OpenCL $2.0$. Intel drivers meet our
requirements on Windows, but not on Linux, although we tried the
upstream driver that requires a patched Linux kernel. AMD support SVM
atomics only their Kaveri family of chips and only on Linux, requiring
to compile the latest driver. Among the GPUs available to us, only 4
turned out to provide our requirements: Intel HD 520, HD 5500 and IRIS
6100, and AMD Radeon R7.

\HEComment{To decide: do we mention carrot AMD, in which terms?}

\subsection{Sizing Non-Cooperative Kernels}\label{sec:sizingnoncoop}

A cooperative kernel can yield workgroups to let a non-cooperative,
transient one execute. We size non-cooperative kernels to represent
approximatively the workload required by a system to maintain smooth
screen animations.

\paragraph{Experimental setup} We repetitively launch a kernel with a
controlled workload and detect from which workload size the screen
animation starts to jitter. Given the kernel workload and its
scheduling frequency, we can deduce what is the workload and frequency
needed by the system to maintain a smooth animation. We use this
approach to measure the system workloads needed to maintain smooth
animations of three types: (\emph{light}) emphasis of desktop icons
under the mouse pointer, (\emph{medium}) window dragging over the
desktop, and (\emph{heavy}) WebGL animations in a browser.

\begin{table}
\small
\centering
\begin{tabular}{ l c c }
Workload & Duration (ms) & Period (ms)\\
\hline
light & 3 & 70\\
medium & 3 & 40\\
heavy & 10 & 40\\
\end{tabular}
\caption{Caracteristics of non-cooperative kernels workload}
\label{tab:noncooperativeworkload}
\end{table}

\paragraph{Results}
\mytab~\ref{tab:noncooperativeworkload} illustrates the caracteristics
found for the three types of workloads. For the medium and heavy
workloads, the 40ms period corresponds to an execution every $25^{th}$
of seconds, which coincides with the human persistence of vision. The
3ms execution duration of both light and medium configurations may
indicate a lower bound of GPU execution time for system workloads.

\subsection{The Overhead of Moving to Cooperative Kernels}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% chip: hd520
% overall - 
% geomean: 1.19835906125
% max ('color', 'ecology', 1.749620120141443)
% ----- 
% barrier 
% max - ('color', 'ecology', 1.749620120141443)
% geomean: 1.18027192048
% ----- 
% workstealing
% max - ('octree', 'NA', 1.4165788173996423)
% geomean: 1.4165788174
% ----- 
% chip: iris
% overall - 
% geomean: 1.07926294253
% max ('sssp', 'usa_ny', 1.3661910830147503)
% ----- 
% barrier 
% max - ('sssp', 'usa_ny', 1.3661910830147503)
% geomean: 1.07122874447
% ----- 
% workstealing
% max - ('octree', 'NA', 1.2333896396396398)
% geomean: 1.11616906144
% ----- 
% chip: hd5500
% overall - 
% geomean: 1.15074583378
% max ('bc', '1k_128k', 1.4466039296624815)
% ----- 
% barrier 
% max - ('bc', '1k_128k', 1.4466039296624815)
% geomean: 1.16116637952
% ----- 
% workstealing
% max - ('octree', 'NA', 1.1814511945151949)
% geomean: 1.1049985913
% ----- 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
\small
\centering
\begin{tabular}{ l l l l }
Chip & HD520 & HD5500 & Iris\\
\hline
overall geo. mean  & $1.20$ & $1.15$ & $1.08$ \\
overall max & $1.75^{\ast}$ & $1.45^{\dagger}$ & $1.37^{\ddagger}$ \\

\hline
barrier geo. mean & $1.18$ & $1.16$ & $1.07$ \\
barrier max & $1.75^{\ast}$ & $1.45^{\dagger}$ & $1.37^{\ddagger}$ \\

\hline
wk.steal. geo. mean & $1.42$ & $1.10$ & $1.12$ \\
wk.steal. max & $1.42^{\diamond}$ & $1.18^{\diamond}$ & $1.23^{\diamond}$ \\

\hline
\end{tabular}\\
{\footnotesize
$^{\ast}$color ecology, $^{\dagger}$bc 1k\_128k, $^{\ddagger}$sssp usa\_ny, $^{\diamond}$octree
}
\caption{Overhead of cooperative kernels: execution time slowdown}
\label{tab:overhead}
\end{table}

\begin{figure}
\includegraphics[width=\columnwidth]{cooperative_overhead.pdf}
\caption{Overhead: ratio of execution time}\label{fig:overhead}
\end{figure}

\paragraph{Experimental setup}
Cooperative scheduling primitives entails an overhead even if no
killing, forking or resizing happen. We measure this overhead via the
execution time slowdown between the original kernel and the
cooperative one, forcing the scheduler to never modify the number of
active workgroups. Since the cooperative kernel contains the
non-cooperative kernel code anyway, its occupancy bound may be less
than the original kernel; we make sure to use this occupancy bound as
the number of workgroups for the original kernel in order to compare
relevant execution times. \HEComment{Do we need to mention the
  possible occupancy reduction as an overhead?}

\paragraph{Results}
\mytab~\ref{tab:overhead} lists both geometric mean and maximum value
of measured overheads. The slowdown is below 2 even in the worst case,
and closer to 1 in general. We consider these results as encouraging,
especially since our prototype performance is likely a far upper bound
of what a scheduler implemented directly in the runtime could achieve.

\subsection{Performance and Responsiveness of Cooperative Scheduling}\label{sec:responsiveness}

\ADComment{Preamble}


Cooperative scheduling enables to concurrently run non-persistent
kernels alongside blocking ones. In terms of performance, we are
interested to see whether the variations of workgroups for the blocking
kernel.

Performance of cooperative scheduling corresponds to execution time of
cooperative kernels compared to the serial

In order to simulate non-persistent workloads alongside kernels of
blocking algorithms, we use a matrix multiplication kernel where
matrix dimensions are tuned to trigger the relevant execution
duration.

we compare :

time to execute original kernel

time to execute non-persistent tasks

time to execute both in cooperation

also, response time for non-persistent

use matmult to represent system workload, ref to previous section

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}

\begin{figure*}
\includegraphics[width=.7\columnwidth]{iris_octree_NA.pdf} 
\includegraphics[width=.7\columnwidth]{hd520_mis_ecology.pdf}
\includegraphics[width=.7\columnwidth]{hd5500_bfs_rmat22.pdf}
\caption{Example gather time and non-cooperative timing results}\label{fig:fine-grained-timing}
\end{figure*}


\section{Related Work}\label{sec:relatedwork}

\HEComment{cooperative multitasking: OS: RISC OS, IBM CICS, Windows
  before 95, old MacOS, maybe Oberon? TRON?. In GPU context: some
  paper mention ``cooperative'' multitasking but they simply refer to
  the fact that kernels should not compute for too long in order to
  let other have access to it. Also, many prog languages implements
  variations of coroutines, We can see blocking channels of CSP (or
  Golang more recently) as yielding (sending a message is yielding
  computation to the process which receives it)}

Cooperative multitasking was offered in ancient operating systems
(e.g. Windows before the 95 edition) and is still used nowadays in
RISC OS for instance. Many programming languages offer cooperative
multitasking via variations of coroutines, e.g. coroutine objects in
Python\footnote{\url{https://docs.python.org/3/reference/datamodel.html\#coroutine-objects}}
or Lua\footnote{\url{https://www.lua.org/pil/9.1.html}}, or Ruby's
fiber \footnote{\url{http://ruby-doc.org/core-2.1.1/Fiber.html}}. In
the context of GPU resources, cooperative multitasking seems to refer
to sizing kernels such that they do not compute for too long in order
to yield the whole GPU regularly~\cite{adriaens2012case}. Our
contribution present a much more flexible cooperative multitasking,
where the long-running application is never totally stopped.

\HEComment{ RISC OS see :
\url{http://www.riscos.info/index.php/Preemptive_multitasking}
}

% ###
% Cooperative multitasking for GPU-accelerated grid systems

% http://onlinelibrary.wiley.com/doi/10.1002/cpe.1722/full

% @article {CPE:CPE1722,
% author = {Ino, Fumihiko and Ogita, Akihiro and Oita, Kentaro and Hagihara, Kenichi},
% title = {Cooperative multitasking for GPU-accelerated grid systems},
% journal = {Concurrency and Computation: Practice and Experience},
% volume = {24},
% number = {1},
% publisher = {John Wiley & Sons, Ltd},
% issn = {1532-0634},
% url = {http://dx.doi.org/10.1002/cpe.1722},
% doi = {10.1002/cpe.1722},
% pages = {96--107},
% keywords = {multitasking, GPU, CUDA, grid computing},
% year = {2012},
% }

% Not super interesting: they just divide blocking computation in
% smaller independent kernels, and fires them at regular intervals to
% let the system be responsive.

\paragraph{Irregular algorithms and persistent kernels}
%
There has been a lot of work on accelerating blocking irregular
algorithms using GPUs~\cite{...}, and on the \emph{persistent threads}
programming style for long-running kernels.  These approaches rely on
the occupancy-bound execution model, and aim to take ownership of the
GPU by scheduling as many workgroups as the GPU has compute units,
relying on fair scheduling between these workgroups.  These approaches
render the GPU unavailable for other tasks, and are not future-proof
because (as discussed throughout the paper) the occupancy-bound
execution model is not guaranteed by GPU programming models and is
unlikely to be guaranteed in practice by future GPU platforms.
%
\ADComment{Now say that we could update these approaches to use cooperation.}

\paragraph{GPU multitasking}
%
\cite{DBLP:journals/tog/SteinbergerKBKDS14} \ADComment{Shows how to
  keep a GPU busy by flooding it with work.  Would absolutely not
  stand up to preemption.}

\cite{DBLP:conf/usenix/KatoLRI11} \ADComment{Idea seems to be to
  accept that GPUs do not offer epreemption, but find a reasonable way
  to schedule tasks to maximise responsiveness or throughput.}

\paragraph{Preemption support for GPUs}
%
\cite{DBLP:conf/asplos/ParkPM15} \ADComment{Investigates how one might
  implement preemption, i.e.\ shows that it can be achieved.
  Simulator only preemption.  Not sure whether their approach would
  give fair scheduling.}
%
Proposals to implement preemption, but these require hardware support and may be overkill.  Advantage: fully automatic; disadvantage: doesn't capture domain knowledge.
%
\ADComment{Kepler preemption support?  Mention in related work?}

\ADComment{Sreepathi mentioned the NCCL framework.}


Dynamic parallelism could be extended to give barriers, but doesn't work with work stealing (less general).  It is in recent OpenCL.

\cite{DBLP:conf/ics/WuCLSV15} \ADComment{Basic idea: map tasks to
  SMs, not to workgroups, so that tasks that would benefit from being
  processed by the same SM can be scheduled to the same SM.  Has
  little to do with preemption, but more to do with irregular
  computation.}

\cite{DBLP:conf/isca/TanasicGCRNV14} \ADComment{I have not read, but
  it's about enabling preemptive multiprogramming, so we should read
  it carefully.}

\cite{DBLP:conf/ppopp/Muyan-OzcelikO16} \ADComment{I have not read; we
  should understand this one carefully.}


\section{Conclusions and Future Work}\label{sec:conclusion}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\clearpage

\bibliographystyle{abbrvnat}
\bibliography{tyler}

\appendix

\section{Operational Semantics for Cooperative Kernels}\label{appendix:semantics}

\ADComment{TODO: write it up properly.}

\ADComment{TODO: remark that we do not model transmit faithfully.}

Thread state: $(l, \mathit{ss})$, where $l \in L$ is the thread's
local state, and $\mathit{ss}$ is a semi-colon-separated sequence of
program statements that the thread remains to execute.

Workgroup state: $((l_1, \mathit{ss}_1), \dots, (l_d,
\mathit{ss}_d))$, where each $(l_i, \mathit{ss}_i)$ is a thread state.

Kernel state: a pair $(\sigma, (w_1, w_2, \dots, w_M, \bot, \dots, \bot))$, where $\sigma$ is the shared state,
each $w_i$ is a workgroup state, which are
followed by $N-M$ occurrences of $\bot$ to indicate absent workgroups.

We treat thread-level semantics abstractly by assuming a transition
relation $(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l',
\mathit{ss}'))$ that is defined unless $\mathit{ss} =
\mathsf{special}(); \mathit{tt}$, where $\mathsf{special}$ is one of
$\offerkill$, $\offerfork$, $\globalbarrier$ or
$\resizingglobalbarrier$.

To abstractly accounts for the interaction between global barrier
synchronization and the GPU memory model, we assume a function
$\mathsf{sync}$ that is applied to a global state.  This returns a new
global state in which the shared state and thread local states have
been updated to reflect the fact that all stores to memory and loads
from memory to thread-local storate have completed.

\begin{figure*}
\begin{center}

\[
\inferrule{
w_i(j) = (l, \mathit{ss})
\\
(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l', \mathit{ss}'))
\\
w_i' = w_i[j \mapsto (l', \mathit{ss}')]
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma', (\dots, w_i', \dots))
}
(\textsc{Thread-Step})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerkill();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma, (\dots, w_i', \dots))
}
(\textsc{Kill-No-Op})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_M(j) = (l_j, \offerkill();\mathit{ss})
\\
M > 0
}
{
(\sigma, (\dots, w_{M-1}, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_{M-1}, \bot, \bot, \dots, \bot))
}
(\textsc{Kill})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerfork();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
\\
k \in [0, N - M]
\\
\forall a \in [1, k] \;.\; w_{M+a} = ((l_1, \mathit{ss}), \dots, (l_1, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_i', \dots, w_M, w_{M+1}, \dots, w_{M+k}, \bot, \dots, \bot))
}
(\textsc{Fork})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \globalbarrier();\mathit{ss})
\\
\forall i \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow \mathsf{sync}(\sigma , (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Barrier})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \resizingglobalbarrier();\mathit{ss})
\\
\forall j\;.\;w_1'(j) = (l_{1,j}, \globalbarrier(); \offerfork(); \globalbarrier(); \globalbarrier();\mathit{ss})
\\
\forall i \neq 1 \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \globalbarrier(); \globalbarrier(); \offerkill(); \globalbarrier();\mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Resizing-Barrier})
\]

\end{center}

\caption{Abstract operational semantics for our cooperative kernels language extensions}

\end{figure*}



\end{document}
