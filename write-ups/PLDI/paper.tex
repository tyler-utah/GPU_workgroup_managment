%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace,10pt]{sigplanconf}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}
\usepackage{mathpartir}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
\newcommand{\HEComment}[1]{\textcolor{orange}{HE: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}
\newcommand{\TSComment}[1]{\textcolor{purple}{TS: #1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}

\newcommand{\transmit}{\mathsf{transmit}}


\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}
\newcommand{\globalbarrier}{\mathsf{global\_barrier}}
\newcommand{\resizingglobalbarrier}{\mathsf{resizing\_global\_barrier}}
\newcommand{\getgroupid}{\mathsf{get\_group\_id}}
\newcommand{\getnumgroups}{\mathsf{get\_num\_groups}}
\newcommand{\getlocalid}{\mathsf{get\_local\_id}}
\newcommand{\getglobalid}{\mathsf{get\_global\_id}}
\newcommand{\getlocalsize}{\mathsf{get\_global\_id}}

\newcommand{\NumAlgorithms}{8}

\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

\ADComment{Open to attack: why not just buy another GPU?}

\paragraph{The needs of irregular data-parallel algorithms}
Acceleration of general-purpose computations on graphics processing
units (GPUs) has tended to focus on \emph{regular} data-parallel
algorithms, for which work to be processed can be evenly split between
the workgroups that execute a GPU kernel ahead of time.  However, many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  There is growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate linked data structures, with several recent notable
successes~\cite{...}.

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, many graph algorithms employ a level-by-level strategy, requiring a global barrier between each level.
As another example, work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular
data-parallel algorithms accelerated on a GPU, this translates to
requiring fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if workgroup $A$ holds a mutex, an unfair scheduler may cause
another workgroup $B$ to spin-wait forever for the mutex to be
released.  Similarly, an unfair scheduler can a workgroup to spin-wait
at a global barrier so that other workgroups do not make progress
towards the barrier.

\paragraph{A degree of fairness: occupancy-bound execution}
GPU schedulers are \emph{not} fair. the current GPU
programming models---OpenCL~\cite{...}, CUDA~\cite{...}  and
HSA~\cite{...}---specify almost no guarantees regarding scheduling of
workgroups, and implementations of these programming models do not
provide fair scheduling in practice.  Roughly speaking, each workgroup
executing a GPU kernel is mapped to a hardware \emph{compute
  unit}.\footnote{In practice the situation can be more complex: if a
  kernel is sufficiently simple and if workgroups are small enough,
  multiple workgroups may be mapped to a single compute unit.  We assume, for the purpose of the current discussion, that the kernel is sufficiently complex that this does not happen.}
%
If a kernel is executed with more workgroups than there are compute
units then at least two workgroups must share a single compute unit.
The simplest way for a GPU driver to handle this is via an \emph{occupancy-bound}
execution model~\cite{...}.  With this model, once a workgroup has commenced execution on a
compute unit (it has become \emph{occupant}), the workgroup has
exclusive access to the compute unit until it has finished execution,
after which a subsequent workgroup can be bound to the compute unit.
Experiments suggest that the occupancy-bound execution model is widely
implemented by today's GPU devices and drivers~\cite{...}.  It is
evidently a simple model for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, because
they are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume that the target GPU device provides the occupancy-bound
execution model.  They exploit the model by scheduling no more workgroups
than there are available compute units~\cite{...}.  This works
because \emph{today}'s GPUs seem to provide the occupancy-bound
execution model.

\paragraph{Resistance to guaranteeing occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable from the point-of-view of
GPU vendors and end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all compute units, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.  Precisely because
of this problem, operating systems employ a GPU \emph{watchdog} to
heuristically detect and kill long-running computations on the GPU
being used for display rendering~\cite{...}.  Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets~\cite{...}.  In the future, it will be desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling would require being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts~\cite{CitePersonalCommunication}, is that (1)
GPU vendors will not commit to the occupancy-bound execution model
that they currently implement, due to the need for multi-tasking and
energy throttling, yet (2) GPU vendors will not guarantee fair
scheduling, due to the runtime overhead associated with supporting
full preemption of workgroups~\cite{ISCAPAPERSreeMentioned}.  Vendors
instead wish to retain the essence of the occupancy-bound execution
model, which is simple to implement efficiently, and provide
support for preemption only in key special cases.


\paragraph{Our proposal: cooperative kernels}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and is written using two additional
language primitives, $\offerkill$ and $\offerfork$, which must be
placed carefully by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$, offering to
sacrifice itself to the scheduler.  This indicates to the scheduler
that the workgroup would ideally continue with its execution, but that
if required the scheduler may destroy the workgroup for good: the
cooperative kernel is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates to the scheduler that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commence execution directly after the $\offerfork$ program point.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel, with
both functional and non-functional requirements.  Functionally, the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled, while the cooperative kernel must be
robust to workgroups leaving and joining the computation in response
to $\offerkill$ and $\offerfork$.  Non-functionally, a cooperative
kernel must ensure that $\offerkill$ is executed frequently enough
that the scheduler can accommodate soft-real time constraints,
e.g.\ allowing a smooth frame-rate for graphics, or allowing compute
units to be powered down quickly when required.  The scheduler should
respond to $\offerkill$ and $\offerfork$ calls in a manner that avoids
under-utilisation of hardware resources by the cooperative kernel,
e.g.\ the scheduler should not kill workgroups unless they are
required for another purpose, and should facilitate forking of
additional workgroups when possible.

The cooperative kernels programming model has several appealing
properties:

\begin{enumerate}

\item By providing fair scheduling between workgroups, cooperative
  kernels meet the needs of blocking algorithms, including irregular
  data-parallel algorithms.

\item The model has no impact on the development of regular
  (non-cooperative) kernels: these can be programmed exactly as they
  are now, as can graphics shading routines written using APIs such as
  OpenGL, Vulkan and DirectX.

\item The model is backwards-compatible: a non-blocking algorithm
  implementation that works on today's GPU platforms can be upgraded
  to use our language extensions for cooperative scheduling, and will
  behave exactly as it does at present if the new $\offerkill$ and
  $\offerfork$ functions are simply ignored.

\item The model can be implemented on top of the occupancy-bound
  execution model that current GPU platforms provide---we present a
  proof-of-concept implementation on top of OpenCL 2.0 that requires
  no special-purpose hardware or driver support.

\item Our experiments across four GPUs from two vendors show that our
  portable, proof-of-concept implementation of cooperative kernels can
  provide efficient multi-tasking of cooperative and non-cooperative
  tasks.  A GPU vendor implementation could likely provide even better
  efficiency, by leveraging platform-specific hardware, driver and
  compiler features.

\end{enumerate}

Placing the $\offerkill$ and $\offerfork$ primitives requires manual
effort and care, but our experience porting a representative set of
GPU-accelerated irregular algorithms to use cooperative kernels
suggests that this is straightforward in practice.  We discuss this in
detail in Section~\ref{sec:portingalgorithms}.

In summary, our main contributions are:

\begin{itemize}

\item \emph{Cooperative kernels}, an extension to the GPU programming model to support the scheduling requirements of blocking algorithms.  We describe the semantics of the programming model in detail (Section~\ref{sec:cooperativekernels}). 

\item A proof-of-concept portable implementation of cooperative
  kernels on top of the OpenCL 2.0 programming model
  (Section~\ref{sec:implementation}).

\item An experience report porting \NumAlgorithms{} irregular GPU
  algorithms to use cooperative kernels, and experimental results
  assessing the performance overhead associated with cooperation and
  the responsiveness to high priority tasks that cooperation enables,
  across four GPUs from two vendors (Section~\ref{sec:experiments}).

\end{itemize}

We begin by providing an overview of the OpenCL programming model and
two motivating examples of irregular algorithms (Section~\ref{sec:background}).  At the end of the paper we discuss related work (Section~\ref{sec:relatedwork}) and avenues for future work (Section~\ref{sec:conclusion}).

\section{Background and Examples}\label{sec:background}

We use two examples of irregular data-parallel algorithms---a work
stealing queue and breadth-first graph search---to introduce the
industry-standard OpenCL programming model on which we base cooperative
kernels.

% \paragraph{OpenCL concepts}
% An OpenCL program is typically executed by many \emph{threads} running
% in parallel. Threads are organized into \emph{workgroups} of equal
% size. The memory is organized in three levels: \emph{global} memory is
% accessible by any thread, \emph{local} memory is shared at the workgroup
% level (i.e., threads of different workgroups cannot access the same
% local memory), and \emph{private} memory is dedicated to thread-level
% storage and cannot be shared between threads. These concepts are
% illustrated over the two examples.

\ADComment{Points to make are: there is spinning, either on mutex
implementation or main loop, so fairness is required.  Note OpenCL
features: kernel keyword, global qualifier, ``get'' functions.  Note
that pop\_or\_steal is executed by all, but internally the master
manipulates the queue.  t is a single task, but is chunky enough that
all threads can process part of it.}

\begin{figure}

\begin{lstlisting}
kernel work_stealing(global Task *pools) {
  while (more_work(pools)) {
    int pool_id = get_group_id();
    Task *t = pop_or_steal(pools, pool_id);
    if (t) {
      // task processing may create more work
      process_task(t, pools, pool_id);
    }
  }
}
\end{lstlisting}

\caption{An excerpt of a work stealing algorithm in OpenCL}\label{fig:workstealing}
\end{figure}

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph *g, 
                 global nodes *n0, global nodes *n1) {

  int level = 0;
  global nodes * in_nodes = n0;
  global nodes * out_nodes = n1;

  int tid = get_global_id();
  int stride = get_global_size();

(*@\label{line:graph:iterate}@*)  while(in_nodes.size > 0) {

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

(*@\label{line:graph:swap}@*)    swap(in_nodes, out_nodes);
(*@\label{line:graph:gb1}@*)    global_barrier();
(*@\label{line:graph:reset}@*)    reset(out_nodes);
    level++;
(*@\label{line:graph:gb2}@*)    global_barrier();
  }
}
\end{lstlisting}
\caption{An OpenCL graph traversal algorithm, using a global barrier for synchronisation}\label{fig:graphsearch}
\end{figure}

\paragraph{OpenCL kernels}

An OpenCL program is divided into \emph{host} and \emph{device}
components.  A host application runs on the CPU and launches one or
more \emph{kernels} that run on accelerator devices---GPUs in the
context of this paper.  A kernel is writen in OpenCL C, based on C99.
All threads executing a kernel execute the same entry function with
identical arguments, but a thread may query its unique thread id in
order to access different data or follow different control flow paths
to other threads.



Figure~\ref{fig:workstealing} shows a simplified version of an OpenCL
kernel that uses a work stealing queue.  \ADComment{Describe some
  aspects of the example, saying that the mutex is not a primitive,
  and can be coded up using the memory model but is not safe due to
  the execution model.  Make it clear that the lock function uses
  busy-waiting.}  

\paragraph{Work stealing}
Work stealing is generally used for computational work that can be
splitted into independently processable tasks. The processing of a given
task may create an arbitrary number of new tasks in such a way that it
is typically hard to know the total number of tasks in advance. Work
stealing enables the dynamic balance of task processing over several
processing units, OpenCL workgroups in our case. Each workgroup has an
associated task pool from which it obtains tasks to process and to where
it stores newly created tasks. When a workgroup's pool becomes empty, a
workgroup---rather than terminating---tries to \emph{steal} a task from
an other pool. Hence, workgroups keep on processing tasks until all the
work is done.

\myfiglong\ref{fig:workstealing} illustrates an OpenCL implementation of
a work stealing application. All threads start their execution at the
function marked with the $\mathsf{kernel}$ keyword. They receive a
pointer to the task pools that reside in global memory, and which were
initialized by the host program beforehand to contain the initial
tasks. Each thread can identify the workgroup it belongs to via the
function $\getgroupid$, and OpenCL provides a series of primitives in the
same vein to access for instance the thread global id, or its local id
within a workgroup. Here the group id is stored into the variable
$\mathsf(pool\_id)$ (which is, by default, in private memory) and used
to access the relevant task pool. The $\mathsf(pop\_or\_steal)$ function
is called by all threads of a workgroup, although inside only the thread
with a local id equal to $0$, also called \emph{master thread} of the
workgroup, will try to pop or steal a task from the pools. The master
thread uses local memory inside $\mathsf(pop\_or\_steal)$ to ensure that
the returned value is valid for all threads of its workgroup. If a task
was obtained, then all threads of the workgroup can participate in the
processing of the task inside the $\mathsf{process\_task}$
function. \HEComment{Manage to say that a task is considered big enough
to benefit from parallel computation} If some new tasks are created
during the processing, it is again the master thread who will be in
charge of pushing them to the pool. Although not depicted here,
concurrent accesses to each pool is guarded by a mutex which is
implemented by atomic compare and swap operations on global
memory. Henceforth, master threads may be spinning on mutexes, and
workgroups are also spinning on the main loop, so a fair scheduling of
workgroups is desirable.

\ADComment{Describe some
  aspects of the example, saying that the global barrier is not a
  primitive, and can be coded up using the memory model but is not
  safe due to the execution model.  Make it clear that the barrier
  uses busy-waiting.}

\paragraph{Graph traversal} Figure~\ref{fig:graphsearch} shows the
skeleton of a frontier based graph traversal algorithm (this approach
has been shown to be efficient on GPUs in~\ref{...}). The kernel is
given three arguments in global memory: a graph structure, and two
arrays that can contain graph nodes. Initially, {\tt n0} contains the
starting nodes to process. Threads keep three local variables: {\tt
  level} which indicates the frontier level which is currently being
processed, {\tt in\_nodes} and {\tt out\_nodes} which point to
distinct node arrays. {\tt in\_nodes} contains the nodes to be
processed for the current frontier, while {\tt out\_nodes} contains
the nodes to be processed during the next frontier.

The application iterates as long as the current frontier contains
nodes to process (line~\ref{line:graph:iterate}). At each frontier,
the nodes to be processed are evenly distributed between the executing
threads. This is done through \emph{stride} based processing. A thread
retrieves its global thread id and the total number of threads
(obtained through the {\tt get\_global\_id()} and {\tt
  get\_global\_size()} OpenCL primitives). A thread then processes
each node associated with id (i.e.\ indices with its thread id value
added to any integer multiple of the total number of threads up to the
list size). A thread processes a node via the {\tt process\_node}
function, which may (a) push nodes to process in the next frontier to
the {\tt out\_nodes} list and (b) use the frontier level in
computation. After processing the frontier, the threads swap their
local node array pointers (line~\ref{line:graph:swap}), i.e.\ the {\tt
  out\_nodes} of the current frontier becomes the input of the next
frontier.

At this point, the GPU threads must wait for all other threads to
finishing processing the frontier before they can continue. To achieve
this, we use a global barrier construct
(line~\ref{line:graph:gb1}). After all threads reach this point, the
output node array is reset (line~\ref{line:graph:reset}) and the level
is incremented. Again, threads must wait until the output node is
reset at a global barrier (line~\ref{line:graph:gb2}). At this point
threads can continue to the next frontier.

The global barrier used in this application is not provided as a GPU
primitive, though many previous works have shown that such a global
barrier can be implemented~\cite{...}. These implementations use well
defined atomic memory operations to ensure memory consistency and use
spinning to ensure threads wait at the barrier for the remaining
threads to reach the barrier. Even though such barriers appear to run
reliably on current GPUs, they violate GPU language specifications,
which do not provide fair scheduling of workgroups. These barriers
require fair workgroup scheduling in order to function correctly. That
is, \emph{all} workgroups must be fairly scheduled in order to reach
the barrier, otherwise, the workgroups waiting at the barrier will not
make progress.


\paragraph{Execution model}

The threads executing a kernel are hierarchically organised.  A kernel
is executed by a number of \emph{workgroups}, and each workgroup
consists of a fixed number of \emph{threads}.  Within a workgroup,
threads may also be organised into \emph{subgroups} (termed
\emph{warps} and \emph{wavefronts} on NVIDIA and AMD architectures,
respectively).

Two barrier primitives are provided in OpenCL.  A workgroup barrier
synchronises all threads in a workgroup and ensures memory
consistency.  This barrier is a \emph{workgroup-level} function: it
must be encountered uniformly by all or none of the threads of a
workgroup, thus can only appear in conditional code if all threads in
a workgroup that reach the barrier agree on the guards of all
enclosing conditional statements.  A subgroup barrier works
analogously at the subgroup level.  As discussed below, a
\emph{global} barrier (synchronising all threads running a kernel) is
not provided as a primitive.

OpenCL specifically makes no guarantees about fair scheduling between
workgroups \TSComment{This should explicitly say \emph{workgroups
    executing the same kernel}, otherwise HSA has some weirdness},
stating~\cite{...}: \emph{``A conforming implementation may choose to
  serialize the workgroups so a correct algorithm cannot assume that
  workgroups will execute in parallel.  There is no safe and portable
  way to synchronize across the independent execution of workgroups
  since once in the work-pool, they can execute in any order.''}
OpenCL is not alone in this regard: CUDA similarly provides no
guarantees~\cite{...}. 

HSA provides limited, one-way guarantees,
stating~\cite[p. 46]{HSAprogramming11}: \emph{Work-group A can wait
  for values written by work-group B without deadlock provided ... A
  comes after B in work-group flattened ID order}. However, this
one-way communication is not sufficient so support either the
work-stealing or graph application examples of
Figure~\ref{fig:graphsearch} and \ref{fig:workstealing}. As described
above, both require \emph{symmetric} communication between
threads. \ADComment{Tyler, can you state the HSA guarantee?},
\TSComment{How is this?}~\cite{...}.

Revisiting the examples of Figures~\ref{fig:workstealing}
and~\ref{fig:graphsearch}: the mutex function of
Figure~\ref{fig:workstealing} will cause starvation if a workgroup
that holds the mutex is not scheduled again and so does not release
the mutex.  A similar issue affects the global barrier function of
Figure~\ref{fig:graphsearch}.  Because the barrier is implemented
using busy-waiting, a set of workgroups waiting at the barrier may be
continuously scheduled, starving a workgroup that has not yet arrived
at the barrier and meaning that no workgroup proceeds past the
barrier.

\paragraph{Identity and size functions}

Builtin functions $\getlocalid()$ and $\getglobalid()$ provide a
thread with an id that is unique within its workgroup and unique
across all workgroups, respectively.  Each workgroup has an id,
obtained via $\getgroupid()$.  The number of threads per workgroup and
the number of workgroups are obtained via $\getlocalsize()$ and
$\getnumgroups()$.  A thread's local and global id are related via the
equation $\getglobalid() = \getgroupid() \times \getlocalsize() +
\getlocalid()$.

In general, an OpenCL kernel is organised as a multi-dimensional grid
of multi-dimensional workgroups, and the above functions each take
an integer dimension argument.  For simplicity, we consider only the
one-dimensional case throughout the paper, which captures all the
GPU-accelerated irregular algorithms we are aware of, and use
e.g.\ $\getglobalid()$ to mean $\getglobalid(0)$.


\paragraph{Memory model}

An OpenCL kernel has access to four memory spaces.  Shared virtual
memory (SVM) is accessible to all threads, and to the host
application.  Atomic operations on this memory space enable
fine-grained communication between host and device.  Our
implementation of cooperative kernels
(Section~\ref{sec:implementation}) depends on this recent OpenCL
feature.  Global memory is shared among all threads executing a
kernel, but can only be safely accessed by the host application before
or after kernel execution.  Local memory is shared among all threads
in the same workgroup: each workgroup has its own portion of local
memory, which can be used for fast intra-workgroup communication.
Each thread has an allocation of very fast private memory, where
function-local variables are allocated.

Fine-grained communication between threads, and also with the host in
the case of SVM, is enabled via a set of atomic data types and
operations, based on the C11 memory model~\cite{...}.
\ADComment{Perhaps say more.}  \ADComment{Can we illustrate using the
  examples?}


\section{The Cooperative Kernels Programming Model}\label{sec:cooperativekernels}

We describe our cooperative kernels programming model as an extension
to OpenCL.  However, the cooperative kernels concept is more general,
and could be applied to extend other GPU programming models, such as
CUDA and HSA.

We describe the semantics of cooperative kernels
(Section~\ref{sec:semantics}), then summarise the important
differences between programming with regular vs.\ cooperative kernels
(Section~\ref{sec:programmingguidelines}) and the non-functional
properties that a developer of a cooperative kernel, and the
implementer of a scheduler for cooperative kernels, should strive for
(Section~\ref{sec:nonfunctional}).  The semantics of cooperative
kernels has been guided by the applications that we have studied
(described in Section~\ref{sec:portingalgorithms}), and we discuss several cases where we
might have taken different, equally reasonable, semantic decisions (Section~\ref{sec:semanticalternatives}).
We conclude the section by discussing the backwards-compatible nature
of cooperative kernels with respect to blocking algorithms designed to
run on current GPU platforms
(Section~\ref{sec:backwardscompatibility}).

\subsection{Semantics of Cooperative Kernels}\label{sec:semantics}

As with a regular OpenCL kernel (see Section~\ref{sec:background}), a
cooperative kernel is launched by the host application.  The host
application passes parameters to the kernel and specifies a desired
number of workgroups, each consisting of a specified number of
threads.  The parameters to a cooperative kernel are \emph{immutable}, as if
they were qualified with the \texttt{const} keyword (though pointer
parameters can refer to mutable data).

A cooperative kernel is written in OpenCL C plus the following
language extensions: $\transmit$, a
new qualifier on the local variables of a thread; $\offerkill$ and
$\offerfork$, the key functions that enable cooperative scheduling;
$\globalbarrier$ and $\resizingglobalbarrier$, two primitives to allow
inter-workgroup synchronisation.

We now explain the semantics of these language extensions, after which
we explain how the work stealing and graph traveral algorithms of
Section~\ref{sec:background} can be turned into cooperative kernels
that use the extensions.  We present the semantics intuitively, using English.
In Appendix~\ref{appendix:semantics} we present an abstract operational semantics for our language extensions in a simple GPU-like programming model.

\paragraph{Transmitted variables}

A variable declared in the root scope of the cooperative kernel can
optionally be annotated with a new $\transmit$ qualifier.  By
annotating a variable with $\transmit$, the programmer indicates that
when a workgroup spawns new workgroups by calling $\offerfork$, the
workgroup should transmit its current value for the variable to serve
as an initial value for the variable for the threads of the new workgroups.  We detail the semantics
for this when we describe $\offerfork$ below.

\paragraph{Active workroups}

If the host application launches a cooperative kernel requesting $N$
workgroups, this indicates that the kernel should be executed with a
maximum of $N$ workgroups, and that as many workgroups as possible, up
to this limit, are desired.  However, the scheduler may initially
schedule fewer than $N$ workgroups, and as explained below the number
of workgroups that execute the cooperative kernel can change during
the lifetime of the kernel.

The workgroups executing the kernel are called \emph{active
  workgroups}, and $M$ denotes the number of active workgroups.
Initially, the number of active workgroups $M$ is initialised to some
integer between $1$ and $N$.  The initial active workgroups have
consecutive workgroup ids in the range $[0, M-1]$.

The $\getgroupid()$ function retrieves the id of a workgroup as usual
(see Section~\ref{sec:background}).  When executed by a cooperative
kernel, $\getnumgroups()$ returns $M$, the \emph{current} number of
active workgroups.  This is in contrast to $\getnumgroups()$ for
regular kernels, which returns the fixed number of workgroups that
were requested at kernel launch time.

Fair scheduling is guaranteed between active workgroups.  That is, if
some thread in an active workgroup is enabled then eventually some
thread in the active workgroup is guaranteed to execute an
instruction.  Note that our programming model does not mandate that
threads within a workgroup must be fairly scheduled, and indeed this
is often not the case in GPU programming models, because threads are
often organised into \emph{warps} or \emph{wavefronts} within a
workgroup, exhibiting lock-step predicated execution that is
inherently unfair.

\paragraph{Semantics for $\offerkill$}

The $\offerkill$ primitive allows the cooperative kernel to return
compute units to the scheduler by offering to sacrifice workgroups.
The idea is that while having the scheduler be permitted to terminate
the execution of workgroups in an arbitary fashion would be drastic,
the kernel may contain identifiable points of execution at which a
workgroup could gracefully leave the computation.  We give examples of
these for work stealing and graph traversal below.

Similar to the OpenCL workgroup $\mathsf{barrier}$ primitive,
$\offerkill$, is a workgroup-level function---it must be encountered
uniformly by all threads in a workgroup (see
Section~\ref{sec:background}).

Suppose a workgroup with id $m$ executes $\offerkill$.  If $m < M-1$
or $M=1$, i.e.\ the workgroup's id is not the largest among the active
workgroups, or if only a single workgroup is executing the cooperative
kernel, then $\offerkill$ is a no-op.  If instead $M > 1$ and $m =
M-1$, there is a non-deterministic choice between (a) $\offerkill$
executing as a no-op, or (b) execution of the workgroup ceasing and
the number of active workgroups $M$ being decremented by one.  Case
(b) corresponds to the scheduler accepting the workgroup's offer to
cease execution, allowing the scheduler to schedule some other task on
the compute unit on which the workgroup was executing.

The semantics of $\offerkill$ is atomic.  That is, if multiple
workgroups reach $\offerkill$ simultaneously, the semantics is as if
each workgroup executes $\offerkill$ in some arbitrary sequential
order.

In this formulation of cooperative kernels, only the active workgroup
with the largest id can be killed, and workgroup 0 can never be
killed.  We discuss the rationale for these choices, as well as
possible alternative choices, in
Section~\ref{sec:semanticalternatives}.

\paragraph{Semantics for $\offerfork$}

The $\offerfork$ primitive allows the cooperative kernel to indicate
to the scheduler that it could benefit from, and is prepared to
handle, additional workgroups joining the computation.  Recall that
the cooperative kernel was launched with a limit of $N$ workgroups,
indicating the number of workgroups that would ideally execute the
kernel.  However, the number of active workgroups, $M$, may be smaller
than $N$, either because (due to competing workloads) the scheduler
did not provide $N$ workgroups initially, or because the kernel has
given up some number of workgroups via $\offerkill$ calls.  Through
$\offerfork$, the kernel and scheduler can work together to allow new
workgroups to join the computation at an appropriate point and with
appropriate state.

Like $\offerkill$, $\offerfork$ is a workgroup-level function.

Suppose a workgroup with id $m\leq M$ executes $\offerfork$.  An
integer $k$ in the range $[0, N-M]$ is chosen nondeterministically,
$k$ new workgroups are spawned with consecutive ids in the range $[M,
  M+k-1]$, and the active workgroup count $M$ is incremented by $k$.

The $k$ new workgroups commence execution at the program point
immediately following the $\offerfork$ call.  The variables that
describe the state of a thread are all uninitialised for the threads
in the new workgroups; reading from these variables without first
initialising them is an undefined behaviour.  There are two exceptions
to this:

\begin{itemize}

\item because the parameters to a cooperative kernel are immutable,
  the new threads have access to these parameters as part of their
  local state and can safely read from them;

\item for each variable $v$ annotated with $\transmit$, every new
  thread's copy of $v$ is initialised to the value that thread 0 in
  workgroup $m$ held for $v$ at the point of the $\offerfork$ call.

\end{itemize}

In effect, thread 0 of the forking workgroup transmits the relevant
portion of its local state to the threads of the forked workgroups.
We discuss in Section~\ref{sec:semanticalternatives} the rationale for
having all new threads obtain this local state portion from thread 0,
as well as our reasons for selecting which variables to transmit via
annotations rather than transmitting the entire thread state.

Notice that $k=0$ is always a legitimate choice for the number of
workgroups to be spawned by an $\offerfork$ call, and if the number of
active workgroups $M$ is equal to the workgroup limit $N$, $k=0$ is
guaranteed.

\paragraph{Global barriers}

\ADComment{Maybe refer back to motivating examples?}
Many irregular algorithms require synchronisation across workgroups.
This can be achieved via a \emph{global barrier}, but a global barrier
is not provided as a primitive in OpenCL, CUDA or HSA.  This is
because, as discussed in Section~\ref{sec:intro}, an inter-workgroup barrier
requires fair scheduling of workgroups.

Because workgroups of a cooperative kernel are fairly scheduled, a
global barrier primitive can be provided.  We specify two such primitives: $\globalbarrier$
and $\resizingglobalbarrier$.

Our $\globalbarrier$ primitive is a kernel-level function: if it
appears in conditional code then it must be reached by \emph{all}
threads executing the cooperative kernel.  On reaching a
$\globalbarrier()$, a thread waits until all threads have arrived at
the barrier.  Once all threads have arrived, the threads may proceed
past the barrier with the guarantee that all global memory accesses
issued before the barrier have completed.  The $\globalbarrier$
primitive can be implemented following any inter-workgroup barrier
design, \TSComment{This isn't exactly true. For cooperative kernels,
  the barriers must be able to handle a growing or shrinking number of
  workgroups.} e.g.~\cite{...}, and the atomic operations provided by
the OpenCL 2.0 memory model enable a memory-safe
implementation~\cite{...}.  However, implementing such a barrier is
involved, hence why we include this function as a primitive.

The $\resizingglobalbarrier$ primitive is also a kernel-level
function.  It is identical to $\globalbarrier$, except that it caters
for cooperation with the scheduler: by issuing a
$\resizingglobalbarrier$ the programmer indicates that the cooperative
kernel is prepared to proceed after the barrier with fewer workgroups,
or with additional workgroups joining the computation.

Semantically, when all threads have reached $\resizingglobalbarrier$,
the number of active workgroups, $M$, is set to a new value, $M'$ say.
If $M' = M$ then the active workgroups remain unchanged and proceed to
execute after the barrier.  If $M' < M$, workgroups $[M', M-1]$ are
killed.  If $M' > M$ then $M'-M$ new workgroups join the computation,
as if they were forked from workgroup 0.  In particular, the
$\transmit$-annotated local state of thread 0 in workgroup 0 is
transmitted to the threads of the new workgroups.  The change in
number of active workgroups from $M$ to $M'$ is atomic.

The semantics of $\resizingglobalbarrier$ can be modelled via the following sequence of calls:

\lstset{basicstyle=\tt}
\begin{lstlisting}
  $\globalbarrier()$;
  if($\getgroupid()$ == 0) $\offerfork$();
  $\globalbarrier()$;
  $\offerkill()$;
  $\globalbarrier()$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

The enclosing $\globalbarrier$ calls make the operation appear to be
atomic.  The middle $\globalbarrier$ is required to ensure that if
$M'\geq M$, the workgroups with ids smaller than $M$ are left intact.

Because $\resizingglobalbarrier$ can be implemented in the above
manner, we do not regard it conceptually as a primitive of the
cooperative kernels programming model.  However, we discuss in
Section~\ref{sec:resizingbarrier} how $\resizingglobalbarrier$ can be
implemented significantly more efficiently if it is treated as a
primitive operation that interacts directly with the scheduler.

\paragraph{Examples}

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph *g, 
                 global nodes *n0, global nodes *n1) {

(*@\label{line:cgraph:transmit1}@*)  transmit int level = 0;
(*@\label{line:cgraph:transmit2}@*)  transmit global nodes *in_nodes = n0;
(*@\label{line:cgraph:transmit3}@*)  transmit global nodes *out_nodes = n1;

  while(in_nodes.size > 0) {

(*@\label{line:cgraph:rechunking1}@*)    int tid = get_global_id();
(*@\label{line:cgraph:rechunking2}@*)    int stride = get_global_size();

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

    level++;
    swap(in_nodes, out_nodes);
    reset(out_nodes);
(*@\label{line:cgraph:resizing}@*)    resizing_global_barrier();
  }
}
\end{lstlisting}
\caption{Cooperative kernel port for Figure~\ref{fig:graphsearch}}\label{fig:cgraphsearch}
\end{figure}

\ADComment{TODO.  We want to show the examples from Section~\ref{sec:background} ported to use cooperative kernels.  We want to make it clear that it is easy to place the primitives.}

In practice, we found that transforming traditional blocking GPU
kernels into cooperative kernels was straightforward. For example,
Figure~\ref{fig:cgraphsearch} shows the graph traversal example of
Figure~\ref{fig:graphsearch} transformed into a cooperative kernel.
On line~\ref{line:cgraph:resizing}, we change the original barrier
into a resizing barrier. Several variables are marked to be
transmitted in the case of workgroups joining at the resizing barrier
(lines~\ref{line:cgraph:transmit1}, \ref{line:cgraph:transmit2} and
\ref{line:cgraph:transmit3}): {\tt level} must be restored so that
new workgroups know which frontier they are processing; {\tt
  in\_nodes} and {\tt out\_nodes} must be restored so that new
workgroups know which of the node arrays are input and output. Lastly,
the static work distribution of the original kernel is no longer valid
in a cooperative kernel. This is because the stride (which is based on
$M$) may change after each resizing barrier
call. To fix this, we re-distribute the work after each resizing
barrier call by recomputing the thread id and stride
(lines~\ref{line:cgraph:rechunking1} and
\ref{line:cgraph:rechunking2}).

\TSComment{Can Hugues talk about work stealing here?}

For the work stealing example, there is no state to transmit since a
computation is entirely parametrized by a task, which is retrieved from
a task pool located in global memory. Care must be taken on the dynamic
change of the number of workgroups though: we cannot assume that the
total number of task pools is equal to $\getnumgroups$. Thus, the number
of pools must be given as an argument, and the $\mathsf{pool}$ argument
must be obtained from the group id modulo the total number of
pools. This total number of pools must also be passed to
$\mathsf{pop\_or\_steal}$, since when a workgroup wants to steal it must
access other pool indexes, which are bounded by the total number of
pools.

\begin{figure}

\begin{lstlisting}
kernel work_stealing(global Task *pools, const int num_pools) {
  while (more_work(pools)) {
    int pool_id = get_group_id() % num_pools;
    Task *t = pop_or_steal(pools, pool_id, num_pools);
    if (t) {
      // task processing may create more work
      process_task(t, pools, pool_id);
    }
  }
}
\end{lstlisting}

\caption{Cooperative version of work stealing: the number of workpools
must be explicitely passed as arguments since the number of workgroups
is not a constant anymore}\label{fig:workstealing-cooperative}
\end{figure}

\subsection{Programming Guidelines}\label{sec:programmingguidelines}

We remark on some issues that programmer working with cooperative
kernels should be aware of.

\paragraph{A changing number of workgroups}  Unlike in regular OpenCL,
the value returned by $\getnumgroups()$ is not fixed during the
lifetime of a cooperative kernel \TSComment{Neither is {\tt
    get\_global\_size}, is that worth mentioning?}.  Although
$\getnumgroups()$ has atomic semantics, and so can be called at any
time to yield a well-defined value, the value returned by two
successive calls may differ if workgroups have executed $\offerkill$,
$\offerfork$ or $\resizingglobalbarrier$ in-between.  The programmer
must therefore write their cooperative kernel in a manner that is
robust to changes in the value returned by $\getnumgroups()$.

\paragraph{Cooperative kernels that only use barriers}

The situation is more stable if a cooperative kernel does not directly
issue $\offerkill$ and $\offerfork$ calls directly.  In this case,
only calls to $\resizingglobalbarrier$ can cause the number of active
workgroups to change and, as discussed in Section~\ref{sec:semantics},
this function causes synchronisation between all threads and causes
the number of active workgroups to change atomically.  At any point
during execution, the threads of a kernel are executing between some
pair of resizing barrier calls, which we call a \emph{resizing barrier
  intervals} (here we consider the kernel entry and exit points
conceptually to be special cases of resizing barriers).  The number of
workgroups executing the kernel is thus constant within each resizing
barrier interval, i.e.\ the result returned by repeated calls to
$\getnumgroups()$ will be identical during such an interval.

This guarantee can be exploited by algorithms that perform strided
data processing.  \ADComment{Tyler, can we give an example of a data
  processing loop that would have to be changed?} \TSComment{Done,
its in figure~\ref{fig:cgraphsearch}}

\paragraph{Placing the new constructs}

It is up to the programmmer to decide where to place our new language
constructs.  This must be done bearing in mind both the semantics of
the constructs, as detailed in Section~\ref{ }, as well as the
non-functional requirements of cooperative kernels that we describe in
Section~\ref{sec:nonfunctional}.

We note, however, that the programming model and associated language
constructs were inspired by the requirements of the irregular GPU
algorithms proposed in the literature, and that at least for these
algorithms it is usually easy to identify where to place the
constructs.

The irregular algorithms we encountered either employ work stealing or
operate on graph data structures.

Work stealing algorithms have a transactional flavour, whereby tasks
are repeatedly pulled from a queue and processed by a workgroup.  The
point of task completion---the end of a transaction---is a natural
place for a workgroup to invoke both $\offerkill$ and $\offerfork$, to
indicate both that the workgroup could gracefully exit computation,
but also that there is more work to be processed and that further
workgroups would be useful in order to process this work.  Such
algorithms usually require little state to persist between tasks;
typically only the pointers to the queue data structures need to be
pre-initialised, by annotating them with $\transmit$.
\ADComment{Refer back to cooperative versions of examples in
  Section~\ref{sec:semantics}.}

The level-by-level approach employed by many graph algorithms tends to
involve global barrier synchronisation.  In some cases, a global
barrier is required in the midst of a transaction, in which case
$\globalbarrier$, but not $\resizingglobalbarrier$, is suitable for
achieving synchronization without disturbing the set of active
workgroups.  But often it is the case that on completing a level of
the graph algorithm, the next level could be processed by more or
fewer workgroups, something which $\resizingglobalbarrier$
facilitates.  Again, only a small amount of state is relevant to new
workgroups joining the computation, such as the value of the level
being processed; this relevant state must be annotated with
$\transmit$.

\subsection{Non-Functional Requirements of Cooperative Kernels}\label{sec:nonfunctional}

The semantics presented in Section~\ref{sec:semantics} describe the envelope of
behaviour that a developer of a cooperative kernel should be prepared
for.
%
However, the aim of cooperative kernels is to find a balance that
allows \emph{efficient execution} of long-running kernels
(e.g.\ implementing irregular data-parallel algorithms), and
\emph{responsive multi-tasking} so that the GPU can be shared between
one or more cooperative kernels and a number of short, high-priority
tasks with soft real-time constraints.

To achieve this balance, both an implementation of the cooperative
kernels model, and the programmer of a cooperative kernel, must strive
to meet a number of non-functional requirements, which we outline
here.

\ADComment{Sree mentioned TimeGraph paper.}

\paragraph{Sufficient $\offerkill$ calls}

Recall that the purpose of $\offerkill$ is to provide the scheduler
with an opportunity to destroy a workgroup in order to schedule
higher-priority tasks.  Because these higher-priority tasks, such as
graphics rendering, may have soft real-time constraints, the scheduler
relies on the cooperative kernel to execute $\offerkill$ sufficiently
frequently that the scheduler can meet such real-time constraints.

\ADComment{Forward reference to experiments with display freezing.}

\ADComment{Got here.}

\ADComment{Generosity: initial number of workgroups, and responding to offer fork.}

\subsection{Alternative Semantic Choices}\label{sec:semanticalternatives}

\ADComment{Explain here where we could have done things differently.}

\ADComment{What are difficulties with other alternatives and why we didn't use them.}

\ADComment{Allowing zero to be killed; the following is some text we had before:}

\paragraph{Keeping at least one workgroup alive}

The programmer is responsible for keeping at least one workgroup alive.  In particular, if all workgroups execute $\offerkill$, the scheduler is at liberty to destroy all workgroups, causing the cooperative kernel to exit.  This can be guarded against by wrapping each $\offerkill$ call in a conditional statement:

\lstset{basicstyle=\tt}
\begin{lstlisting}
  if($\getgroupid{0}$ != 0) $\offerkill$();
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

We could, alternatively, have formulated the semantics of $\offerkill$
so that workgroup 0 is never killed.  However, there might be a use
case for a cooperative kernel reaching a point where it would be
acceptable for the kernel to exit, although desirable for some
remaining computation to be performed if competing workloads allow it.



\ADComment{Allowing arbitrary workgroups to be killed (not just the largest workgroup)}

\ADComment{Having thread 0 transmit its state, and transmitting only annotated variables.}

\subsection{Backwards Compatibility}\label{sec:backwardscompatibility}

\ADComment{Perhaps this would fit better under ``programming guidelines'' -- not sure.}

\ADComment{Make the point here that if an irregular algorithm is
  written as a cooperative kernel then it will execute just as safely
  as it does today on current hardware if: (1) the $\transmit$
  annotation and $\offerkill$/$\offerfork$ calls are defined away to
  nothing, (2) $\resizingglobalbarrier$ is redefined to
  $\globalbarrier$, (3) a standard implementation of $\globalbarrier$
  that assumes the occupancy-bound execution model is provided.  If the kernel uses blocking features---the global barrier, mutexes, etc., then it may not execute safely due to the possibility of unfair scheduling, but it will execute no less safely than it would if the cooperative kernels style were not used.}


\section{Prototype Implementation}\label{sec:implementation}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}

\ADComment{Give details of how host and device interact.}

\ADComment{Explain kernel merge tool briefly.  Explain library briefly.}

\ADComment{Give most attention to interesting performance and safety issues, e.g.\ reverse ticket lock, efficient barrier, the query functions...}

\ADComment{Host-device interaction through SVM.  Make clear that this is probably the most aggressive use of OpenCL 2.0 ever.}

\subsection{An Efficient Resizing Barrier}\label{sec:resizingbarrier}

\ADComment{Optimised barrier implementation.}

\subsection{Vendor Implementation Defects}

\ADComment{Briefly rant about our pain.  Compiler hang, compiler crashes, deadlocks, defunct processes.  Caution: don't make it sound so terrible that you can't trust our results.  Gently acknowledge that our implementation may be imperfect.}


\section{Applications and Experiments}\label{sec:experiments}

\subsection{Porting Irregular GPU Algorithms to Cooperative Kernels}\label{sec:portingalgorithms}

\ADComment{Describe the algorithms that we ported.}

\ADComment{Give stats on how many variables had to be marked with
  $\transmit$ (i.e., occurred in the restoration context), how many barriers we changed to resizing, how many offer kill and offer fork calls we included.}

\ADComment{Make sure we address the point that placing the primitives was pretty easy.}

\subsection{The GPUs we Tested}

\ADComment{Describe the platforms.  Explain why we could not try other
  platforms.}

\ADComment{Tyler and Hugues, please write.}

\subsection{Determining Representative Workloads}

\ADComment{Better section title?}

\ADComment{Hugues, can you please explain your experimental
  methodology for working out required response times here?}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}

\subsection{The Overhad of Moving to Cooperative Kernels}

\ADComment{Results for stand-alone vs.\ cooperative without co-scheduling.}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}

\subsection{Performance and Responsiveness of Cooperative Scheduling}

\ADComment{Preamble}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}



\section{Related Work}\label{sec:relatedwork}


\cite{DBLP:conf/ics/WuCLSV15} \ADComment{Basic idea: map tasks to
  SMs, not to workgroups, so that tasks taht would benefit from being
  processed by the same SM can be scheduled to the same SM.  Has
  little to do with preemption, but more to do with irregular
  computation.}

\cite{DBLP:conf/isca/TanasicGCRNV14} \ADComment{I have not read, but
  it's about enabling preemptive multiprogramming, so we should read
  it carefully.}

\cite{DBLP:conf/ppopp/Muyan-OzcelikO16} \ADComment{I have not read; we
  should understand this one carefully.}

\cite{DBLP:conf/asplos/ParkPM15} \ADComment{Investigates how one might
  implement preemption, i.e.\ shows that it can be achieved.
  Simulator only preemption.  Not sure whether their approach would
  give fair scheduling.}

\cite{DBLP:journals/tog/SteinbergerKBKDS14} \ADComment{Shows how to
  keep a GPU busy by flooding it with work.  Would absolutely not
  stand up to preemption.}

\cite{DBLP:conf/usenix/KatoLRI11} \ADComment{Idea seems to be to
  accept that GPUs do not offer epreemption, but find a reasonable way
  to schedule tasks to maximise responsiveness or throughput.}


\section{Conclusions and Future Work}\label{sec:conclusion}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\clearpage

\bibliographystyle{abbrvnat}
\bibliography{tyler}

\appendix

\section{Operational Semantics for Cooperative Kernels}\label{appendix:semantics}

\ADComment{TODO: write it up properly.}

\ADComment{TODO: remark that we do not model transmit faithfully.}

Thread state: $(l, \mathit{ss})$, where $l \in L$ is the thread's
local state, and $\mathit{ss}$ is a semi-colon-separated sequence of
program statements that the thread remains to execute.

Workgroup state: $((l_1, \mathit{ss}_1), \dots, (l_d,
\mathit{ss}_d))$, where each $(l_i, \mathit{ss}_i)$ is a thread state.

Kernel state: a pair $(\sigma, (w_1, w_2, \dots, w_M, \bot, \dots, \bot))$, where $\sigma$ is the shared state,
each $w_i$ is a workgroup state, which are
followed by $N-M$ occurrences of $\bot$ to indicate absent workgroups.

We treat thread-level semantics abstractly by assuming a transition
relation $(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l',
\mathit{ss}'))$ that is defined unless $\mathit{ss} =
\mathsf{special}(); \mathit{tt}$, where $\mathsf{special}$ is one of
$\offerkill$, $\offerfork$, $\globalbarrier$ or
$\resizingglobalbarrier$.

To abstractly accounts for the interaction between global barrier
synchronization and the GPU memory model, we assume a function
$\mathsf{sync}$ that is applied to a global state.  This returns a new
global state in which the shared state and thread local states have
been updated to reflect the fact that all stores to memory and loads
from memory to thread-local storate have completed.

\begin{figure*}
\begin{center}

\[
\inferrule{
w_i(j) = (l, \mathit{ss})
\\
(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l', \mathit{ss}'))
\\
w_i' = w_i[j \mapsto (l', \mathit{ss}')]
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma', (\dots, w_i', \dots))
}
(\textsc{Thread-Step})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerkill();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma, (\dots, w_i', \dots))
}
(\textsc{Kill-No-Op})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_M(j) = (l_j, \offerkill();\mathit{ss})
\\
M > 0
}
{
(\sigma, (\dots, w_{M-1}, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_{M-1}, \bot, \bot, \dots, \bot))
}
(\textsc{Kill})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerfork();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
\\
k \in [0, N - M]
\\
\forall a \in [1, k] \;.\; w_{M+a} = ((l_1, \mathit{ss}), \dots, (l_1, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_i', \dots, w_M, w_{M+1}, \dots, w_{M+k}, \bot, \dots, \bot))
}
(\textsc{Fork})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \globalbarrier();\mathit{ss})
\\
\forall i \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow \mathsf{sync}(\sigma , (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Barrier})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \resizingglobalbarrier();\mathit{ss})
\\
\forall j\;.\;w_1'(j) = (l_{1,j}, \globalbarrier(); \offerfork(); \globalbarrier(); \globalbarrier();\mathit{ss})
\\
\forall i \neq 1 \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \globalbarrier(); \globalbarrier(); \offerkill(); \globalbarrier();\mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Resizing-Barrier})
\]

\end{center}

\caption{Abstract operational semantics for our cooperative kernels language extensions}

\end{figure*}





\section{Notes}

\ADComment{A natural exit does not decrement $M$.  We haven't made a point of this, and I don't think we should right now, but we should remember to think this through carefully.}


  \ADComment{Perhaps
  make the point that inter-workgroup synchronization is possible in
  very rare cases without blocking, e.g. threadfence reduction.}

\ADComment{If this watchdog is disabled,
execution of a long-running kernel causes the display to freeze for
the duration of kernel execution.}

\ADComment{Kepler preemption support?  Mention in related work?}

\ADComment{Mention dynamic parallelism and explain why it's no good.}

\section{Remainder of old intro}

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.

\end{document}
