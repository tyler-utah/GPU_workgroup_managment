%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[numbers,nocopyrightspace,10pt]{sigplanconf}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}
\usepackage{mathpartir}

\usepackage{booktabs}
\usepackage{subfig}


\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\newcommand\crule[3][black]{\textcolor{#1}{\rule{#2}{#3}}}


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
\newcommand{\HEComment}[1]{\textcolor{orange}{HE: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}
\newcommand{\TSComment}[1]{\textcolor{purple}{TS: #1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray3}{gray}{0.9}
\definecolor{Gray2}{gray}{0.75}
\definecolor{Gray1}{gray}{0.6}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)},
  numbers=left
}

\newcommand{\transmit}{\mathsf{transmit}}


\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}
\newcommand{\globalbarrier}{\mathsf{global\_barrier}}
\newcommand{\resizingglobalbarrier}{\mathsf{resizing\_global\_barrier}}
\newcommand{\getgroupid}{\mathsf{get\_group\_id}}
\newcommand{\getnumgroups}{\mathsf{get\_num\_groups}}
\newcommand{\getlocalid}{\mathsf{get\_local\_id}}
\newcommand{\getglobalid}{\mathsf{get\_global\_id}}
\newcommand{\getlocalsize}{\mathsf{get\_local\_size}}
\newcommand{\getglobalsize}{\mathsf{get\_global\_size}}

\newcommand{\keyword}[1]{\mathsf{#1}}

\newcommand{\NumAlgorithms}{8}

\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}
There is growing interest in accelerating irregular data-parallel
algorithms on GPUs.  These algorithms are typically \emph{blocking},
so require fair scheduling.  But GPU programming models (e.g.\ OpenCL)
do not mandate fair scheduling, and GPU schedulers are inherently
unfair in practice.  Current irregular algorithms only work on today's
GPUs by identifying hardware compute units and scheduling workgroups
exclusively to these compute units.  As well as being non-portable,
this is unacceptable because the GPU cannot be used in the meantime
for high-priority tasks with soft real-time constraints,
such as graphics rendering.

We propose \emph{cooperative kernels}, an extension to the traditional
GPU programming model geared towards writing blocking algorithms.
Cooperative kernels enable fair scheduling of workgroups executing a
blocking algorithm, as well as co-scheduling of other tasks on the
GPU, via a small set of language extensions that create a contract
between the programmer and the scheduler.  We describe our cooperative
kernels programming model and its semantics in detail, and our
implementation of a portable prototype of cooperative kernels on top
of OpenCL 2.0.  We evaluate the technique by porting a set of
irregular work stealing and graph algorithms to our programming model
and evaluate the performance and responsiveness of the approach across
four GPUs from two vendors.  Because our implementation exploits no
vendor-specific hardware, driver or compiler support, our encouraging
experimental results provide a lower-bound on the efficiency with which
cooperative kernels can be implemented in practice.

\end{abstract}
    
\section{Introduction}\label{sec:intro}

\paragraph{The needs of irregular data-parallel algorithms}
Acceleration of general-purpose computations on graphics processing
units (GPUs) has tended to focus on \emph{regular} data-parallel
algorithms, for which work to be processed can be evenly split between
the workgroups that execute a GPU kernel ahead of time.  However, many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  The last decade has seen growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate linked data structures~\cite{owens-persistent,TPO10,DBLP:conf/ipps/KaleemVPHP16,DBLP:conf/ipps/DavidsonBGO14,DBLP:conf/hipc/HarishN07,DBLP:journals/topc/MerrillGG15,DBLP:conf/egh/VineetHPN09,DBLP:conf/ppopp/NobariCKB12,DBLP:conf/hpcc/SolomonTT10a,DBLP:conf/popl/PrabhuRMH11,DBLP:conf/ppopp/Mendez-LojoBP12,DBLP:conf/oopsla/PaiP16,DBLP:conf/ipps/KaleemVPHP16,DBLP:conf/oopsla/SorensenDBGR16}.  \ADComment{Tyler, can you add relevant workstealing papers if not here already?}

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, many graph algorithms employ a level-by-level strategy, requiring a global barrier between each level.
As another example, work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular
data-parallel algorithms accelerated on a GPU, this translates to
requiring fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if workgroup $A$ holds a mutex, an unfair scheduler may cause
another workgroup $B$ to spin-wait forever for the mutex to be
released.  Similarly, an unfair scheduler can cause a workgroup to spin-wait
indefinitely at a global barrier so that other workgroups do not make progress
towards the barrier.

\paragraph{A degree of fairness: occupancy-bound execution}
GPU schedulers are \emph{not} fair. the current GPU
programming models---OpenCL~\cite{opencl2Spec}, \ADComment{Tyler, please add best CUDA since I've asked you to add it below in any case} CUDA~\cite{...}  and
HSA~\cite{HSAprogramming11}---specify almost no guarantees regarding scheduling of
workgroups, and implementations of these programming models do not
provide fair scheduling in practice.  Roughly speaking, each workgroup
executing a GPU kernel is mapped to a hardware \emph{compute
  unit}.\footnote{In practice the situation can be more complex: if a
  kernel is sufficiently simple and if workgroups are small enough,
  multiple workgroups may be mapped to a single compute unit.  We assume, for the purpose of the current discussion, that the kernel is sufficiently complex that this does not happen.}
%
If a kernel is executed with more workgroups than there are compute
units then at least two workgroups must share a single compute unit.
The simplest way for a GPU driver to handle this is via an \emph{occupancy-bound}
execution model~\cite{DBLP:conf/oopsla/SorensenDBGR16}. \ADComment{Tyler, please add best cite for OB model; I guess our OOPSLA paper introduced the term, but we should cite that + Owens?}  With this model, once a workgroup has commenced execution on a
compute unit (it has become \emph{occupant}), the workgroup has
exclusive access to the compute unit until it has finished execution,
after which a subsequent workgroup can be bound to the compute unit.
Experiments suggest that the occupancy-bound execution model is widely
implemented by today's GPU devices and drivers~\cite{DBLP:conf/oopsla/SorensenDBGR16}. \ADComment{For the previous cite, should we cite other papers as well as ours?}  It is
evidently a simple model for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, because
they are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume that the target GPU device provides the occupancy-bound
execution model.  They exploit the model by scheduling no more workgroups
than there are available compute units~\cite{...}. \ADComment{Tyler, please pick best cite for previous claim.} This works
because \emph{today}'s GPUs seem to provide the occupancy-bound
execution model.

\paragraph{Resistance to guaranteeing occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable from the point-of-view of
GPU vendors and end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all compute units, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.  Precisely because
of this problem, operating systems employ a GPU \emph{watchdog} to
heuristically detect and kill long-running computations on the GPU
being used for display rendering~\cite{DBLP:conf/iwocl/SorensenD16}.  Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets~\cite{DBLP:journals/comsur/Vallina-RodriguezC13}.  In the future, it will be desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling would require being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts
(including~\cite{PersonalCommunicationRichards,PersonalCommunicationHowes}
and various engineers from GPU vendors), is that (1) GPU vendors will
not commit to the occupancy-bound execution model that they currently
implement, due to the need for multi-tasking and energy throttling,
yet (2) GPU vendors will not guarantee fair scheduling, due to the
runtime overhead associated with supporting full preemption of
workgroups~\cite{ISCAPAPERSreeMentioned}.  Vendors instead wish to
retain the essence of the occupancy-bound execution model, which is
simple to implement efficiently, and provide support for preemption
only in key special cases.


\paragraph{Our proposal: cooperative kernels}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and is written using two additional
language primitives, $\offerkill$ and $\offerfork$, which must be
placed carefully by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$, offering to
sacrifice itself to the scheduler.  This indicates to the scheduler
that the workgroup would ideally continue with its execution, but that
if required the scheduler may destroy the workgroup for good: the
cooperative kernel is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates to the scheduler that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commence execution directly after the $\offerfork$ program point.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel, with
both functional and non-functional requirements.  Functionally, the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled, while the cooperative kernel must be
robust to workgroups leaving and joining the computation in response
to $\offerkill$ and $\offerfork$.  Non-functionally, a cooperative
kernel must ensure that $\offerkill$ is executed frequently enough
that the scheduler can accommodate soft-real time constraints,
e.g.\ allowing a smooth frame-rate for graphics, or allowing compute
units to be powered down quickly when required.  The scheduler should
respond to $\offerkill$ and $\offerfork$ calls in a manner that avoids
under-utilisation of hardware resources by the cooperative kernel,
e.g.\ the scheduler should not kill workgroups unless they are
required for another purpose, and should facilitate forking of
additional workgroups when possible.

The cooperative kernels programming model has several appealing
properties:

\begin{enumerate}

\item By providing fair scheduling between workgroups, cooperative
  kernels meet the needs of blocking algorithms, including irregular
  data-parallel algorithms.

\item The model has no impact on the development of regular
  (non-cooperative) kernels: these can be programmed exactly as they
  are now, as can graphics shading routines written using APIs such as
  OpenGL, Vulkan and DirectX.

\item The model is backwards-compatible: a non-blocking algorithm
  implementation that works on today's GPU platforms can be upgraded
  to use our language extensions for cooperative scheduling, and will
  behave exactly as it does at present if the new $\offerkill$ and
  $\offerfork$ functions are simply ignored.

\item The model can be implemented on top of the occupancy-bound
  execution model that current GPU platforms provide---we present a
  proof-of-concept implementation on top of OpenCL 2.0 that requires
  no special-purpose hardware or driver support.

\item Our experiments across four GPUs from two vendors show that our
  portable, proof-of-concept implementation of cooperative kernels can
  provide efficient multi-tasking of cooperative and non-cooperative
  tasks.  A GPU vendor implementation could likely provide even better
  efficiency, by leveraging platform-specific hardware, driver and
  compiler features.

\end{enumerate}

Placing the $\offerkill$ and $\offerfork$ primitives requires manual
effort and care, but our experience porting a representative set of
GPU-accelerated irregular algorithms to use cooperative kernels
suggests that this is straightforward in practice.  We discuss this in
detail in \mysec\ref{sec:portingalgorithms}.

In summary, our main contributions are:

\begin{itemize}

\item \emph{Cooperative kernels}, an extension to the GPU programming model to support the scheduling requirements of blocking algorithms.  We describe the semantics of the programming model in detail (\mysec\ref{sec:cooperativekernels}). 

\item A proof-of-concept portable implementation of cooperative
  kernels on top of the OpenCL 2.0 programming model
  (\mysec\ref{sec:implementation}).

\item An experience report of porting \NumAlgorithms{} irregular GPU
  algorithms to use cooperative kernels, and experimental results
  assessing the performance overhead associated with cooperation and
  the responsiveness to high priority tasks that cooperation enables,
  across four GPUs from two vendors (\mysec\ref{sec:experiments}).

\end{itemize}

We begin by providing an overview of the OpenCL programming model and
two motivating examples of irregular algorithms (\mysec\ref{sec:background}).  At the end of the paper we discuss related work (\mysec\ref{sec:relatedwork}) and avenues for future work (\mysec\ref{sec:conclusion}).

\section{Background and Motivating Examples}\label{sec:background}

We provide background on the industry-standard OpenCL programming
model on which we base cooperative kernels (\mysec\ref{sec:opencl}),
and illustrate OpenCL using two irregular data-parallel algorithms---a
work stealing queue and breadth-first graph search---that also
pinpoint the problems associated with unfair scheduling of workgroups
and motivate our cooperative threads approach (\mysec\ref{sec:openclexamples}).

\subsection{OpenCL Background}\label{sec:opencl}

% \paragraph{OpenCL concepts}
% An OpenCL program is typically executed by many \emph{threads} running
% in parallel. Threads are organized into \emph{workgroups} of equal
% size. The memory is organized in three levels: \emph{global} memory is
% accessible by any thread, \emph{local} memory is shared at the workgroup
% level (i.e., threads of different workgroups cannot access the same
% local memory), and \emph{private} memory is dedicated to thread-level
% storage and cannot be shared between threads. These concepts are
% illustrated over the two examples.


An OpenCL program is divided into \emph{host} and \emph{device}
components.  A host application runs on the CPU and launches one or
more \emph{kernels} that run on accelerator devices---GPUs in the
context of this paper.  A kernel is written in OpenCL C, based on C99.
All threads executing a kernel execute the same entry function with
identical arguments, but a thread may invoke the $\getglobalid()$
function to query its unique thread id in order to access different
data or follow different control flow paths to other threads.

The threads executing a kernel are hierarchically organised.  A kernel
is executed by a number of \emph{workgroups}, and each workgroup
consists of a fixed number of threads.  Builtin functions
$\getlocalid()$ and $\getgroupid()$ return a thread's local id within
its workgroup and the id of the workgroup, respectively.\footnote{In general, an OpenCL kernel is organised as a multi-dimensional grid
of multi-dimensional workgroups, and the above functions each take
an integer dimension argument.  For simplicity, we consider only the
one-dimensional case throughout the paper, which captures all the
GPU-accelerated irregular algorithms we are aware of, and use
e.g.\ $\getglobalid()$ to mean $\getglobalid(0)$.
}  The number
of threads per workgroup and the number of workgroups are obtained via
$\getlocalsize()$ and $\getnumgroups()$.  A thread's local and global
id are related via the equation $\getglobalid() = \getgroupid() \times
\getlocalsize() + \getlocalid()$.  The total number of threads executing the kernel is given by $\getglobalsize()$, which is equal to $\getlocalsize()\times\getnumgroups()$.

The execution of all threads in a workgroup can be synchronised via a
workgroup barrier, which also ensures memory consistency.  This
barrier is a \emph{workgroup-level} function: it must be encountered
uniformly by all or none of the threads of a workgroup, thus can only
appear in conditional code if all threads in a workgroup that reach
the barrier agree on the guards of all enclosing conditional
statements.  As discussed below, a \emph{global} barrier
(synchronising all threads running a kernel) is \emph{not} provided as
a primitive.

\paragraph{Memory spaces and memory model}
An OpenCL kernel has access to four memory spaces.  \emph{Shared virtual
memory} (SVM) is accessible to all threads and the host application concurrently.  \emph{Global} memory is shared among all threads executing a
kernel.  Each workgroup has its own portion of \emph{local} memory,
which can be used for fast intra-workgroup communication.  Every thread has
an allocation of very fast \emph{private} memory, where
function-local variables are allocated.

Communication between threads in the same workgroup can be achieved
using the workgroup barrier construct described above.  Finer-grained
communication within a workgroup, as well as inter-workgroup
communication and communication with the host while the kernel is
running, is enabled by a set of atomic data types and operations.  In
particular, fine-grained host/device communication is via atomic
operations on SVM.

\paragraph{Execution model}

OpenCL specifically makes no guarantees about fair scheduling between
workgroups executing the same kernel, stating~\cite[p.\ 31]{opencl2Spec}: \emph{``A
  conforming implementation may choose to serialize the workgroups so
  a correct algorithm cannot assume that workgroups will execute in
  parallel.  There is no safe and portable way to synchronize across
  the independent execution of workgroups since once in the work-pool,
  they can execute in any order.''}  \ADComment{Tyler: please provide CUDA reference with page} OpenCL is not alone in this
regard: CUDA similarly provides no guarantees~\cite{...}.

HSA provides limited, one-way guarantees,
stating~\cite[p. 46]{HSAprogramming11}: \emph{Work-group A can wait
  for values written by work-group B without deadlock provided ... A
  comes after B in work-group flattened ID order}. However, this
one-way communication is not sufficient to support blocking algorithms that use
mutexes and inter-workgroup barriers, both of which require \emph{symmetric} communication between
threads.


\subsection{Motivating Examples}\label{sec:openclexamples}

\begin{figure}

\begin{lstlisting}
(*@\label{line:wksteal:kernelfunc}@*)kernel work_stealing(global Task * pools) {
(*@\label{line:wksteal:getgroupid}@*)  int pool_id = get_group_id();
  while (more_work(pools)) {
(*@\label{line:wksteal:poporsteal}@*)    Task * t = pop_or_steal(pools, pool_id);
    if (t) {
      // task processing may create more work
(*@\label{line:wksteal:processtask}@*)      process_task(t, pools, pool_id);
    }
  }
}
\end{lstlisting}

\caption{An excerpt of a work stealing algorithm in OpenCL}\label{fig:workstealing}
\end{figure}

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph * g, 
                 global nodes * n0, global nodes * n1) {
  int level = 0;
  global nodes * in_nodes = n0;
  global nodes * out_nodes = n1;

  int tid = get_global_id();
  int stride = get_global_size();

(*@\label{line:graph:iterate}@*)  while(in_nodes.size > 0) {

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

(*@\label{line:graph:swap}@*)    swap(&in_nodes, &out_nodes);
(*@\label{line:graph:gb1}@*)    global_barrier();
(*@\label{line:graph:reset}@*)    reset(out_nodes);
    level++;
(*@\label{line:graph:gb2}@*)    global_barrier();
  }
}
\end{lstlisting}
\caption{An OpenCL graph traversal algorithm, using a global barrier for synchronisation}\label{fig:graphsearch}
\end{figure}

\paragraph{Work stealing example}
%
The work stealing paradigm is useful when a computation can be split
into multiple tasks that can be processed independently, but where the
number of tasks is not known ahead of time because processing a given
task may create an arbitrary number of new tasks. Work stealing enables
dynamic balancing of tasks across several processing units, OpenCL
workgroups in our case. Each workgroup has an associated task pool from
which it obtains tasks to process, and to which it stores newly created
tasks. When a workgroup's pool becomes empty, a workgroup---rather than
terminating---tries to \emph{steal} a task from an other
pool. Workgroups keep on processing tasks until all the work is
done. Previous work addresses the feasibility and efficiency of work
stealing on GPUs~ref\cite{dlb-web,TPO10}.

\myfiglong\ref{fig:workstealing} shows a simplified version of an OpenCL
kernel that uses a work stealing queue. All threads start their
execution at the function marked with the $\mathsf{kernel}$ keyword
(line~\ref{line:wksteal:kernelfunc}). They receive a pointer to the task
pools, which reside in global memory, and which were initialized by the
host program beforehand to contain the initial tasks. Each thread can
identify the workgroup it belongs to via the function $\getgroupid()$
(line~\ref{line:wksteal:getgroupid}). Here the group id is stored into
the variable $\mathsf{pool\_id}$ (which is, by default, in private
memory) and used to access the relevant task pool. The
$\mathsf{pop\_or\_steal}$ function (line~\ref{line:wksteal:poporsteal})
is called by all threads of a workgroup.  Inside
$\mathsf{pop\_or\_steal}$ only the thread with a local id equal to $0$,
also called the \emph{master thread} of the workgroup, will try to pop a
task from the workgroup's pool, or steal a task from a different
pool. The master thread uses local memory inside
$\mathsf{pop\_or\_steal}$ to ensure that the returned value is valid for
all threads of its workgroup. If a task was obtained, then all threads
of the workgroup participate in the processing of the task via the
$\mathsf{process\_task}$ function (line~\ref{line:wksteal:processtask}).
Processing this task may lead to the creation of further tasks, which
the master thread will push to the workgroup's pool.

Although not depicted here, concurrent accesses to each pool inside
$\mathsf{more\_work}$ and $\mathsf{pop\_or\_steal}$ are guarded by a
mutex, implemented using atomic compare and swap operations on
global memory.  As a result, the kernel presents two opportunities for
spin-waiting: spinning to obtain a mutex, and spinning in the main
kernel loop to obtain a task to process.  The algorithm thus depends
on a fair scheduler, which, as discussed in \mysec\ref{sec:intro},
is not guaranteed by current GPU programming models.

\paragraph{Graph traversal example} \myfiglong\ref{fig:graphsearch} shows the
skeleton of a frontier-based graph traversal algorithm; such algorithms have
been shown to be execute efficiently on GPUs~\cite{...}. \ADComment{Tyler, can you please work out which of the graph algorithm papers is the right one to cite here?}
The kernel is
given three arguments in global memory: a graph structure, and two
arrays that can contain graph nodes. Initially, $\keyword{n0}$ contains the
starting nodes to process. Threads keep three local variables: $\keyword{level}$, which indicates the frontier level currently being
processed, and $\keyword{in\_nodes}$ and $\keyword{out\_nodes}$, which point to
distinct node arrays. The nodes to be
processed for the current frontier are contained in $\keyword{in\_nodes}$, while $\keyword{out\_nodes}$ contains
the nodes to be processed during the next frontier.

The application iterates as long as the current frontier contains
nodes to process (line~\ref{line:graph:iterate}). At each frontier,
the nodes to be processed are evenly distributed between the executing
threads. This is done through \emph{stride} based processing. 
%
In this case, the stride is the total number of threads, obtained via 
$\getglobalsize()$.  Each thread processes nodes with the following sequence of indices:
\code{$\keyword{tid}$+$\keyword{stride}$*0},
\code{$\keyword{tid}$+$\keyword{stride}$*1},
\code{$\keyword{tid}$+$\keyword{stride}$*2}, etc., where $\keyword{tid}$ is a thread's global id.
A thread processes a node via the $\keyword{process\_node}$
function, which may (a) push nodes to process in the next frontier to
$\keyword{out\_nodes}$ and (b) use the current frontier, $\keyword{level}$, in
the computation. After processing the frontier, the threads swap their
local node array pointers (line~\ref{line:graph:swap}), i.e.\ the $\keyword{out\_nodes}$ of the current frontier becomes the $\keyword{in\_nodes}$ of the next
frontier.

At this point, the GPU threads must wait for all other threads to
finishing processing the frontier before they can continue. To achieve
this, we use a global barrier construct
(line~\ref{line:graph:gb1}). After all threads reach this point, the
output node array is reset (line~\ref{line:graph:reset}) and the level
is incremented. Again, threads must wait until the output node is
reset at a global barrier (line~\ref{line:graph:gb2}). At this point
threads can continue to the next frontier.

The global barrier used in this application is not provided as a GPU
primitive, though previous works have shown that such a global
barrier can be implemented~\cite{XF10,DBLP:conf/oopsla/SorensenDBGR16}. \ADComment{Are there additional papers we should be citing on GPU barriers?  E.g., is the sense reversal idea part of XF or is that from another paper?}  These implementations use well
defined atomic memory operations to ensure memory consistency, and
employ spinning to ensure threads wait at the barrier until all
threads have arrived.  This spinning behaviour requires fair
scheduling between workgroups for the barrier to operate correctly.
Again, fairness is not guaranteed by current GPU programming models.

The mutexes and barriers required by these two examples appear to run
reliably on current GPUs for kernels that are executed with no more
workgroups than there are compute units; this is due to the fairness
of the occupancy-bound execution model that current GPUs have been
shown, experimentally, to provide.  But, as discussed in
\mysec\ref{sec:intro}, this occupancy-bound model is not mandated by
language standards or vendor implementations, and is unlikely to be
guaranteed in the future.

In \mysec\ref{sec:programmingguidelines} we show how the work stealing
and graph traversal examples of \myfigs\ref{fig:workstealing} and~\ref{fig:graphsearch} can be
updated to use our cooperative kernels programming model to resolve
the scheduling issue.


\section{Cooperative Kernels}\label{sec:cooperativekernels}

We describe our cooperative kernels programming model as an extension
to OpenCL.  However, the cooperative kernels concept is more general,
and could be applied to extend other GPU programming models, such as
CUDA and HSA.

We describe the semantics of cooperative kernels
(\mysec\ref{sec:semantics}) and summarise the important
differences between programming with regular vs.\ cooperative kernels
by showing how the motivating examples of \mysec\ref{sec:openclexamples} can be adapted to use cooperative kernels
(\mysec\ref{sec:programmingguidelines}).
We then focus on the non-functional
properties that a developer of a cooperative kernel, and the
implementer of a scheduler for cooperative kernels, should strive for
(\mysec\ref{sec:nonfunctional}).  The semantics of cooperative
kernels has been guided by the practical applications we have studied
(described in \mysec\ref{sec:portingalgorithms}), and we discuss several cases where we
might have taken different and also reasonable semantic decisions (\mysec\ref{sec:semanticalternatives}).
We conclude the section by discussing the backwards-compatible nature
of cooperative kernels with respect to blocking algorithms designed to
run on current GPU platforms
(\mysec\ref{sec:backwardscompatibility}).

\subsection{Semantics of Cooperative Kernels}\label{sec:semantics}

As with a regular OpenCL kernel (see \mysec\ref{sec:opencl}), a
cooperative kernel is launched by the host application.  The host
application passes parameters to the kernel and specifies a desired
number of workgroups, each consisting of a specified number of
threads.  Unlike in a regular kernel, the parameters to a cooperative kernel are \emph{immutable}, as if
they were qualified with the \texttt{const} keyword (though pointer
parameters can refer to mutable data).

A cooperative kernel is written in OpenCL C plus the following
language extensions: $\transmit$, a
new qualifier on the local variables of a thread; $\offerkill$ and
$\offerfork$, the key functions that enable cooperative scheduling;
$\globalbarrier$ and $\resizingglobalbarrier$, two primitives to allow
inter-workgroup synchronisation.

We now explain the semantics of these language extensions.  We present the semantics intuitively, using English.
In Appendix~\ref{appendix:semantics} (see supplementary material) we present an abstract operational semantics for our language extensions in a simple GPU-like programming model.

\paragraph{Transmitted variables}

A variable declared in the root scope of the cooperative kernel can
optionally be annotated with a new $\transmit$ qualifier.  By
annotating a variable with $\transmit$, the programmer indicates that
when a workgroup spawns new workgroups by calling $\offerfork$, the
workgroup should transmit its current value for the variable to serve
as an initial value for the variable for the threads of the new workgroups.  We detail the semantics
for this when we describe $\offerfork$ below.

\paragraph{Active workroups}

If the host application launches a cooperative kernel requesting $N$
workgroups, this indicates that the kernel should be executed with a
maximum of $N$ workgroups, and that as many workgroups as possible, up
to this limit, are desired.  However, the scheduler may initially
schedule fewer than $N$ workgroups, and as explained below the number
of workgroups that execute the cooperative kernel can change during
the lifetime of the kernel.

The workgroups executing the kernel are called \emph{active
  workgroups}, and $M$ denotes the number of active workgroups.
Initially, the number of active workgroups $M$ is initialised to some
integer between $1$ and $N$---to ensure that at least one workgroup is scheduled, the scheduler will postpone execution of the kernel until a some compute unit becomes available.  The initial active workgroups have
consecutive workgroup ids in the range $[0, M-1]$.

The $\getgroupid()$ function retrieves the id of a workgroup as usual
(see \mysec\ref{sec:opencl}).  When executed by a cooperative
kernel, $\getnumgroups()$ returns $M$, the \emph{current} number of
active workgroups.  This is in contrast to $\getnumgroups()$ for
regular kernels, which returns the fixed number of workgroups that
were requested at kernel launch time.

Fair scheduling is guaranteed between active workgroups.  That is, if
some thread in an active workgroup is enabled then eventually some
thread in the active workgroup is guaranteed to execute an
instruction.  Note that our programming model does not mandate that
threads within a workgroup must be fairly scheduled, and indeed this
is often not the case in GPU programming models, because threads are
often organised into \emph{warps} or \emph{wavefronts} within a
workgroup, exhibiting lock-step predicated execution that is
inherently unfair.

\paragraph{Semantics for $\offerkill$}

The $\offerkill$ primitive allows the cooperative kernel to return
compute units to the scheduler by offering to sacrifice workgroups.
The idea is that while having the scheduler be permitted to terminate
the execution of workgroups in an arbitrary fashion would be drastic,
the kernel may contain identifiable points of execution at which a
workgroup could gracefully leave the computation.  We give examples of
these for work stealing and graph traversal below.

Similar to the OpenCL workgroup $\mathsf{barrier}$ primitive,
$\offerkill$, is a workgroup-level function---it must be encountered
uniformly by all threads in a workgroup (see
\mysec\ref{sec:opencl}).

Suppose a workgroup with id $m$ executes $\offerkill$.  If $m < M-1$
or $M=1$, i.e.\ the workgroup's id is not the largest among the active
workgroups, or if only a single workgroup is executing the cooperative
kernel, then $\offerkill$ is a no-op.  If instead $M > 1$ and $m =
M-1$, there is a non-deterministic choice between (a) $\offerkill$
executing as a no-op, or (b) execution of the workgroup ceasing and
the number of active workgroups $M$ being decremented by one.  Case
(b) corresponds to the scheduler accepting the workgroup's offer to
cease execution, allowing the scheduler to schedule some other task on
the compute unit on which the workgroup was executing.

The semantics of $\offerkill$ is atomic.  That is, if multiple
workgroups reach $\offerkill$ simultaneously, the semantics is as if
each workgroup executes $\offerkill$ in some arbitrary sequential
order.

In this formulation of cooperative kernels, only the active workgroup
with the largest id can be killed, and workgroup 0 can never be
killed.  We discuss the rationale for these choices, as well as
possible alternative choices, in
\mysec\ref{sec:semanticalternatives}.

\paragraph{Semantics for $\offerfork$}

The $\offerfork$ primitive allows the cooperative kernel to indicate
to the scheduler that it could benefit from, and is prepared to
handle, additional workgroups joining the computation.  Recall that
the cooperative kernel was launched with a limit of $N$ workgroups,
indicating the number of workgroups that would ideally execute the
kernel.  However, the number of active workgroups, $M$, may be smaller
than $N$, either because (due to competing workloads) the scheduler
did not provide $N$ workgroups initially, or because the kernel has
given up some number of workgroups via $\offerkill$ calls.  Through
$\offerfork$, the kernel and scheduler can work together to allow new
workgroups to join the computation at an appropriate point and with
appropriate state.

Like $\offerkill$, $\offerfork$ is a workgroup-level function.

Suppose a workgroup with id $m\leq M$ executes $\offerfork$.  An
integer $k$ in the range $[0, N-M]$ is chosen nondeterministically,
$k$ new workgroups are spawned with consecutive ids in the range $[M,
  M+k-1]$, and the active workgroup count $M$ is incremented by $k$.

The $k$ new workgroups commence execution at the program point
immediately following the $\offerfork$ call.  The variables that
describe the state of a thread are all uninitialised for the threads
in the new workgroups; reading from these variables without first
initialising them is an undefined behaviour.  There are two exceptions
to this:

\begin{itemize}

\item because the parameters to a cooperative kernel are immutable,
  the new threads have access to these parameters as part of their
  local state and can safely read from them;

\item for each variable $v$ annotated with $\transmit$, every new
  thread's copy of $v$ is initialised to the value that thread 0 in
  workgroup $m$ held for $v$ at the point of the $\offerfork$ call.

\end{itemize}

In effect, thread 0 of the forking workgroup transmits the relevant
portion of its local state to the threads of the forked workgroups.
We discuss in \mysec\ref{sec:semanticalternatives} the rationale for
having all new threads obtain this local state portion from thread 0,
as well as our reasons for selecting which variables to transmit via
annotations rather than transmitting the entire thread state.

Notice that $k=0$ is always a legitimate choice for the number of
workgroups to be spawned by an $\offerfork$ call, and if the number of
active workgroups $M$ is equal to the workgroup limit $N$, $k=0$ is
guaranteed.

\paragraph{Global barriers}

Many irregular algorithms require synchronisation across workgroups.
This can be achieved via a \emph{global barrier}, as illustrated in the graph traversal example of \mysec\ref{sec:openclexamples} and \myfig\ref{fig:graphsearch}.
Because a global barrier requires fair scheduling of workgroups it cannot be safely implemented in OpenCL, CUDA or HSA, and thus is not provided as a primitive in any of these programming models.

Because workgroups of a cooperative kernel are fairly scheduled, a
global barrier primitive can be provided.  We specify two such primitives: $\globalbarrier$
and $\resizingglobalbarrier$.

Our $\globalbarrier$ primitive is a kernel-level function: if it
appears in conditional code then it must be reached by \emph{all}
threads executing the cooperative kernel.  On reaching a
$\globalbarrier()$, a thread waits until all threads have arrived at
the barrier.  Once all threads have arrived, the threads may proceed
past the barrier with the guarantee that all global memory accesses
issued before the barrier have completed.  The $\globalbarrier$
primitive can be implemented by adapting an inter-workgroup barrier
design, e.g.~\cite{XF10}, to take account of a growing and shrinking number of workgroups, and the atomic operations provided by
the OpenCL 2.0 memory model enable a memory-safe
implementation~\cite{DBLP:conf/oopsla/SorensenDBGR16}.  However, implementing such a barrier is
involved, hence why we include this function as a primitive.

The $\resizingglobalbarrier$ primitive is also a kernel-level
function.  It is identical to $\globalbarrier$, except that it caters
for cooperation with the scheduler: by issuing a
$\resizingglobalbarrier$ the programmer indicates that the cooperative
kernel is prepared to proceed after the barrier with fewer workgroups,
or with additional workgroups joining the computation.

Semantically, when all threads have reached $\resizingglobalbarrier$,
the number of active workgroups, $M$, is set to a new value, $M'$ say, with $M' > 0$.
If $M' = M$ then the active workgroups remain unchanged and proceed to
execute after the barrier.  If $M' < M$, workgroups $[M', M-1]$ are
killed.  If $M' > M$ then $M'-M$ new workgroups join the computation,
as if they were forked from workgroup 0.  In particular, the
$\transmit$-annotated local state of thread 0 in workgroup 0 is
transmitted to the threads of the new workgroups.  The change in
number of active workgroups from $M$ to $M'$ is atomic.

The semantics of $\resizingglobalbarrier$ can be modelled via the following sequence of calls:

\lstset{basicstyle=\tt,numbers=none}
\begin{lstlisting}
  $\globalbarrier()$;
  if($\getgroupid()$ == 0) $\offerfork$();
  $\globalbarrier()$;
  $\offerkill()$;
  $\globalbarrier()$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt,numbers=left}

The enclosing $\globalbarrier$ calls make the operation appear to be
atomic.  The middle $\globalbarrier$ is required to ensure that if
$M'\geq M$, the workgroups with ids smaller than $M$ are left intact.

Because $\resizingglobalbarrier$ can be implemented in the above
manner, we do not regard it conceptually as a primitive of the
cooperative kernels programming model.  However, we discuss in
\mysec\ref{sec:resizingbarrier} how $\resizingglobalbarrier$ can be
implemented significantly more efficiently if it is treated as a
primitive operation that interacts directly with the scheduler.

\subsection{Programming With Cooperative Kernels}\label{sec:programmingguidelines}

Working with cooperative kernels requires some manual effort on the
part of the programmer: they must place the $\transmit$ qualifier and
use the new language primitives described in
\mysec\ref{sec:semantics}, and account for slight changes in the OpenCL semantics induced by these new features.

The programming model and associated language constructs were inspired
by the requirements of the irregular GPU algorithms proposed in the
literature, and that at least for these algorithms it is usually easy
to identify where to place the constructs.

We discuss the main semantic change induced by our programming
model---that the number of workgroups executing a kernel varies
dynamically---and illustrate our programming model and the impact of
this semantic change by adapting the work stealing and graph traversal
algorithms of \mysec\ref{sec:openclexamples} to use cooperative
kernels.

\paragraph{A changing number of workgroups}  Unlike in regular OpenCL,
the value returned by $\getnumgroups()$ is not fixed during the
lifetime of a cooperative kernel.  Although
$\getnumgroups()$ has atomic semantics, and so can be called at any
time to yield a well-defined value, the value returned by two
successive calls may differ if workgroups have executed $\offerkill$,
$\offerfork$ or $\resizingglobalbarrier$ in-between.  Because $\getglobalsize()$, which retrieves the total number of threads executing the kernel, is
a function of $\getnumgroups()$, the value returned by $\getglobalsize()$ is also subject to change.
The programmer
must therefore write their cooperative kernel in a manner that is
robust to changes in the values returned by these functions.

\paragraph{Adapting work stealing}

\myfiglong\ref{fig:workstealing-cooperative} shows a cooperative
version of the work stealing example of \myfig\ref{fig:workstealing} that we
discussed in \mysec\ref{sec:openclexamples}.

For the work stealing example, there is no state to transmit since a
computation is entirely parameterised by a task, which is retrieved from
a task pool located in global memory. In the main loop, before trying to
obtain a task, we make the workgroup offer itself to be forked or killed
(lines~\ref{line:cwksteal:offerfork},\ref{line:cwksteal:offerkill}). Note
that a workgroup may be killed even if its associated task pool is not
empty, since remaining tasks could eventually be stealed by other
workgroups.  Care must be taken on computing pool identifiers: the
original version in \myfig\ref{fig:workstealing} can rely on the fact
that there are as many pools as workgroups, but here the number of
workgroups may change during execution. Therefore, the number of pools
must now be explicitely given as an argument
(line~\ref{line:cwksteal:argnumpool}) and used as an upper bound of pool
indexes when scanning pools in $\mathsf{more\_work}$
(line~\ref{line:cwksteal:morework}) and $\mathsf{pop\_or\_steal}$
(line~\ref{line:cwksteal:poporsteal}). Besides, computation of the
workgroup's pool index (line~\ref{line:cwksteal:getgroupid}) must not
only be bounded by the number of pools but also be moved inside the main
loop since a forked workgroup that starts to execute after $\offerfork$
must re-compute its pool index.

\begin{figure}

\begin{lstlisting}
kernel work_stealing(global Task * pools,
(*@\label{line:cwksteal:argnumpool}@*)                     const int num_pools) {
(*@\label{line:cwksteal:morework}@*)  while (more_work(pools, num_pools)) {
(*@\label{line:cwksteal:offerfork}@*)    offer_fork();
(*@\label{line:cwksteal:offerkill}@*)    offer_kill();
(*@\label{line:cwksteal:getgroupid}@*)    int pool_id = get_group_id() % num_pools;
(*@\label{line:cwksteal:poporsteal}@*)    Task * t = pop_or_steal(pools, pool_id, num_pools);
    if (t) {
      // task processing may create more work
(*@\label{line:cwksteal:processtask}@*)      process_task(t, pools, pool_id);
    }
  }
}
\end{lstlisting}

\caption{Cooperative version of the work stealing example from \myfig\ref{fig:workstealing}; the number of workpools
must be explicitly passed as arguments since the number of workgroups
is not a constant anymore}\label{fig:workstealing-cooperative}
\end{figure}

\paragraph{Cooperative kernels that only use barriers}

The situation is more stable if a cooperative kernel does not directly
issue $\offerkill$ and $\offerfork$ calls directly.  In this case,
only calls to $\resizingglobalbarrier$ can cause the number of active
workgroups to change and, as discussed in \mysec\ref{sec:semantics},
this function causes synchronisation between all threads and causes
the number of active workgroups to change atomically.  At any point
during execution, the threads of a kernel are executing between some
pair of resizing barrier calls, which we call a \emph{resizing barrier
  intervals} (here we consider the kernel entry and exit points
conceptually to be special cases of resizing barriers).  The number of
workgroups executing the kernel is thus constant within each resizing
barrier interval, i.e.\ the result returned by repeated calls to
$\getnumgroups()$ will be identical during such an interval; the same holds for $\getglobalsize()$.

This guarantee can be exploited by algorithms that perform strided
data processing.  We now illustrate this using the graph traversal motivating example.

\paragraph{Adapting graph traversal} 
\myfiglong\ref{fig:cgraphsearch} shows a cooperative version of the
graph traversal kernel of \myfig\ref{fig:graphsearch}, introduced in
\mysec\ref{sec:openclexamples}.  On lines~\ref{line:cgraph:resizing1}
and ~\ref{line:cgraph:resizing2}, we change the original global
barriers into a resizing barriers. Several variables are marked to be
transmitted in the case of workgroups joining at the resizing barriers
(lines~\ref{line:cgraph:transmit1}, \ref{line:cgraph:transmit2} and
\ref{line:cgraph:transmit3}): $\keyword{level}$ must be restored so
that new workgroups know which frontier they are processing;
$\keyword{in\_nodes}$ and $\keyword{out\_nodes}$ must be restored so
that new workgroups know which of the node arrays are input and
output. Lastly, the static work distribution of the original kernel is
no longer valid in a cooperative kernel. This is because the stride
(which is based on $M$) may change after each resizing barrier
call. To fix this, we re-distribute the work after each resizing
barrier call by recomputing the thread id and stride
(lines~\ref{line:cgraph:rechunking1} and
\ref{line:cgraph:rechunking2}). This example exploits the fact that
the cooperative kernel does not issue $\offerkill$ nor $\offerfork$
directly: the value of $\keyword{stride}$ obtained from
$\getglobalsize()$ at line~\ref{line:cgraph:rechunking2} is stable
until the next resizing barrier at line~\ref{line:cgraph:resizing1},
and in particular can be safely used inside the $\keyword{for}$ loop.

\begin{figure}

\begin{lstlisting}
kernel graph_app(global graph *g, 
                 global nodes *n0, global nodes *n1) {

(*@\label{line:cgraph:transmit1}@*)  transmit int level = 0;
(*@\label{line:cgraph:transmit2}@*)  transmit global nodes *in_nodes = n0;
(*@\label{line:cgraph:transmit3}@*)  transmit global nodes *out_nodes = n1;

  while(in_nodes.size > 0) {

(*@\label{line:cgraph:rechunking1}@*)    int tid = get_global_id();
(*@\label{line:cgraph:rechunking2}@*)    int stride = get_global_size();

    for (int i = tid; i < nodes.size; i += stride) {
      process_node(g, in_nodes[i], out_nodes, level);
    }

    swap(&in_nodes, &out_nodes);
(*@\label{line:cgraph:resizing1}@*)    resizing_global_barrier();
    reset(out_nodes);
    level++;
(*@\label{line:cgraph:resizing2}@*)    resizing_global_barrier();
  }
}
\end{lstlisting}
\caption{Cooperative version of the graph traversal kernel of \myfig\ref{fig:graphsearch}, using a resizing global barrier and $\transmit$ annotations}\label{fig:cgraphsearch}
\end{figure}

In \mysec\ref{sec:portingalgorithms} we describe the set of irregular GPU algorithms used
in our experiments, which largely captures the irregular blocking
algorithms that are available as open source GPU kernels.  These all
employ either work stealing or operate on graph data structures.
Like the simplified example of \myfig\ref{fig:workstealing-cooperative}, the work stealing algorithms have a transactional flavour
and require little or no state to be carried between transactions.  Thus the point at which a workgroup is ready to process a new task is a natural point to place $\offerkill$ and $\offerfork$, and few or no $\transmit$ annotations are required.
%
The example of \myfig\ref{fig:cgraphsearch} is simpler than, but representative of,
most level-by-level graph algorithms.  These algorithms tend to
require more state to be annotated with $\transmit$, and in some cases
a global barrier is required in the midst of a transaction, in which
case $\globalbarrier$, but not $\resizingglobalbarrier$, is suitable
for achieving synchronization without disturbing the set of active
workgroups.  But often it is the case that on completing a level of
the graph algorithm, the next level could be processed by more or
fewer workgroups, something which $\resizingglobalbarrier$
facilitates.

In summary, our experience so far indicates that cooperative kernels
is a good fit for current irregular algorithms.  Of course this is, in
part, because we designed the model with these algorithms in mind, but
we argue that the fit is natural not just for the examples that we
study, but more generally for at least these two patterns of irregular
algorithm.


\subsection{Non-Functional Requirements}\label{sec:nonfunctional}

The semantics presented in \mysec\ref{sec:semantics} describe the envelope of
behaviour that a developer of a cooperative kernel should be prepared
for.
%
However, the aim of cooperative kernels is to find a balance that
allows \emph{efficient execution} of long-running kernels
(e.g.\ implementing irregular data-parallel algorithms), and
\emph{responsive multi-tasking} so that the GPU can be shared between
one or more cooperative kernels and a number of short, high-priority
tasks with soft real-time constraints.

To achieve this balance, both an implementation of the cooperative
kernels model, and the programmer of a cooperative kernel, must strive
to meet the following non-functional requirements.

\paragraph{Sufficient $\offerkill$ calls}

The purpose of $\offerkill$ is to provide the scheduler with an
opportunity to destroy a workgroup in order to schedule
higher-priority tasks.  Because these higher-priority tasks, such as
graphics rendering, may have soft real-time constraints, the scheduler
relies on the cooperative kernel to execute $\offerkill$ sufficiently
frequently that the scheduler can meet such real-time constraints.

Using work stealing as an example: in the kernel of \myfig\ref{fig:workstealing-cooperative}, a
workgroup offers itself to the scheduler after processing each task.
If tasks turn out to be very short then it might turn out to be
reasonable to invoke $\offerkill$ less frequently, after every $k$
tasks for some appropriate $k$.  On the other hand, if individual
tasks may be very time-consuming then it may be necessary to rewrite
the algorithm so work on a larger number of shorter tasks, so as to
achieve a higher rate of calls to $\offerfork$.

Getting this non-functional requirement right depends on the
constraints of the tasks to be co-scheduled with a cooperative kernel,
on properties of the cooperative kernel itself, and on the efficiency
of the GPU that is used for execution.  In \mysec\ref{sec:representativeworkloads} we conduct
experiments to understand the response rate that would be required to
co-schedule graphics rendering with a cooperative kernel in order to
achieve a smooth frame rate, and we evaluate responsiveness in
practice in \mysec\ref{sec:responsiveness}.

\paragraph{Sufficient workgroups for the cooperative kernel}

Recall that the cooperative kernel would ideally utilise the $N$
workgroups requested upon kernel launch, in order to conduct its
processing efficiently.  The scheduler should thus aim to provide with
cooperative kernel with as many workgroups, up to this number, as
other constraints allow.  As a result, the scheduler should generally
accept an $\offerkill$ only if a compute unit is required for another
task, and should respond positively to $\offerfork$ calls by
scheduling new workgroups if compute units are available.  Our
implementation of cooperative scheduling (\mysec\ref{sec:implementation}) is as
generous as possible to the cooperative kernel in this regard.
However, these are not hard-and-fast rules: a scheduler might employ
heuristics based on historical workloads to avoid thrashing.  For
example, if graphics calls are known to arrive at a certain rate then
the scheduler might decide that it is worth accepting an $\offerkill$
from the cooperative kernel on the assumption that a graphics task
will be requested momentarily.  Similarly, the scheduler might provide
some, but not all, compute units to the cooperative kernel in response
to an $\offerfork$ call if it has reason to believe that the remaining
compute units are likely to be useful for other high priority tasks
that are likely to arrive imminently.


\subsection{Alternative Semantic Choices}\label{sec:semanticalternatives}

\paragraph{Killing order}

We opted for a semantics whereby only the active workgroup with the
highest id can be killed.  This has an appealing property: it means
that the ids of active workgroups are contiguous, which is important
for processing of contiguous data.  The cooperative graph traversal
algorithm of \myfig\ref{fig:cgraphsearch} illustrates this: the algorithm is prepared
for $\getglobalsize()$ to change after each resizing barrier call, but
depends on the fact that $\getglobalid()$ returns a contiguous range
of thread ids.

A disadvantage of this decision is that it may provide sub-optimal
responsiveness from the point of view of the scheduler.  Suppose the
scheduler requires an additional compute unit, but the active thread
with the larget id is processing some computationally intensive work
and will take a while to reach $\offerkill$.  Our semantics means that
the scheuduler cannot take advantage of the fact that another active
workgroup may invoke $\offerkill$ sooner.

Cooperative kernels that do not require contiguous thread ids might me
more suited to a semantics in which workgroups can be killed in any
order, but where workgroup ids (and thus thread global ids) are not
guaranteed to be contiguous.

\paragraph{Keeping one workgroup alive}

Our semantics dictate that the workgroup with id 0 will not be killed
if it invokes $\offerkill$.  This avoids the possibility of the
cooperative kernel terminating early due to the programmer
inadvertantly allowing all workgroups to be killed, and the decision
to keep workgroup 0 alive fits well with our choice to kill workgroups
in descending order of id.

However, there might be a use case for a cooperative kernel reaching a
point where it would be acceptable for the kernel to exit, although
desirable for some remaining computation to be performed if competing
workloads allow it.  In this case, a semantics where all workgroups can be killed via $\offerkill$ would be appropriate, and the programmer would need to guard each $\offerkill$ with an id check in cases where killing all workgroups would be unacceptable.  For example:
%
\lstset{basicstyle=\tt,numbers=none}
\begin{lstlisting}
  if($\getgroupid{0}$ != 0) $\offerkill$();
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt,numbers=left}
%
would ensure that at least workgroup 0 is kept alive.

\paragraph{Transmission of partial state from a single thread}

Recall from the semantics of $\offerfork$ that newly forked workgroups
inherit the variable valuation associated with thread 0 of the forking
workgroup, but only for $\transmit$-annotated variables.  Alternative
choices here would be to have forked workgroups inherit values for
\emph{all} variables from the forking workgroup, and to have thread
$i$ in the forking workgroup provide the valuation for thread $i$ in
each spawned workgroup, rather than having thread 0 transmit the
valuation to all new threads.

We opted for transmitting only selected varibles based on the
observation that many of a thread's private variables are dead at the
point of issuing $\offerfork$ or $\resizingglobalbarrier$, thus it
would be wasteful to transmit them.  A live variable analysis could
instead be employed to over-approximate the variables that might be
accessed by newly arriving workgroups, so that these are automatically
transmitted.

In all cases, we found that a variable that needed to be transmitted
had the property of being uniform across the workgroup.  That is,
despite each thread having its own copy of the variable, each thread
is in agreement on the variable's value.  As an example, the
$\keyword{level}$, $\keyword{in\_nodes}$ and $\keyword{out\_nodes}$
variables used in \myfig\ref{fig:cgraphsearch} are all stored in thread-private
memory, but all threads in a workgroup agree on the values of these
variables at each $\resizingglobalbarrier$ call.  As a result,
transmitting the thread 0's valuation of the annotated variables is
equivalent to (and more efficient than) transmitting values on a
thread-by-thread basis.  We have not yet encountered a real-world
example where our current semantics would not suffice.



\subsection{Backwards Compatibility}\label{sec:backwardscompatibility}

A goal of coopeative kernels is to allow traditional, non-blocking GPU
tasks, such as graphics shaders (in programming models such as OpenGL,
Vulkan and DirectX) and regular data-parallel computational kernels,
to be co-scheduled with blocking irregular algorithms.  It is
important to note that our new model has no effect on the manner in
which these traditional GPU workloads are programmed.

Furthermore, if our cooperative language extensions are
ignored---$\offerkill$ and $\offerfork$ are treated as no-ops,
$\transmit$ annotations are ignored, and $\globalbarrier$ and
$\resizingglobalbarrier$ calls are redirected to an existing global
barrier implementation (so that no resizing occurs in the latter
case)---then cooperative kernel will behave just like its
non-cooperative counterpart would behave.  As a result, a developer
for whom today's occupancy-bound execution model suffices can upgrade
their kernel to a cooperative form and retain backwards-compatibility
with today's GPU platforms by pre-processing away our language
extensions.


\section{Prototype Implementation}\label{sec:implementation}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}

\ADComment{Give details of how host and device interact.}

\ADComment{Explain kernel merge tool briefly.  Explain library briefly.}

\ADComment{Give most attention to interesting performance and safety issues, e.g.\ reverse ticket lock, efficient barrier, the query functions...}

\ADComment{Host-device interaction through SVM.  Make clear that this is probably the most aggressive use of OpenCL 2.0 ever.}

\subsection{An Efficient Resizing Barrier}\label{sec:resizingbarrier}

\ADComment{Optimised barrier implementation.}

\subsection{Vendor Implementation Defects}

\ADComment{Briefly rant about our pain.  Compiler hang, compiler crashes, deadlocks, defunct processes.  Caution: don't make it sound so terrible that you can't trust our results.  Gently acknowledge that our implementation may be imperfect.}


\section{Applications and Experiments}\label{sec:experiments}

\subsection{Porting Irregular GPU Algorithms to Cooperative Kernels}\label{sec:portingalgorithms}

\begin{table}
\small
\centering
\begin{tabular}{ l c c c }
app & resizing barriers & ckill - cfork & tranmit vars\\
\hline
\rowcolor{Gray1}
color & 2 / 2 & 0 - 0 & 4\\
\rowcolor{Gray1}
mis & 3 / 3 & 0 - 0 & 0\\
\rowcolor{Gray1}
bc & 3 / 6 & 0 - 0 & 3\\
\rowcolor{Gray1}
p-sssp & 3 / 3 & 0 - 0 & 0 \\
\rowcolor{Gray2}
bfs & 2 / 2 & 0 - 0  & 4 \\
\rowcolor{Gray2}
l-sssp & 2 / 2 & 0 - 0  & 4 \\
\rowcolor{Gray3}
octree & 0 / 0 & 1 - 1 & 0\\
\rowcolor{Gray3}
game & 0 / 0 & 1 - 1 & 0\\
\end{tabular} \\
\vspace{.2cm}
\crule[Gray1]{.2cm}{.2cm} - Pannotia \hspace{.4cm} \crule[Gray2]{.2cm}{.2cm} - Lonestar GPU  \hspace{.4cm}  \crule[Gray3]{.2cm}{.2cm} - work stealing
\caption{Blocking GPU applications investigated}
\label{tab:applications}
\end{table}

\ADComment{Describe the algorithms that we ported.}

\HEComment{This is more or less already done in section
\ref{sec:programmingguidelines} , maybe no need to rewrite here?}

\ADComment{Give stats on how many variables had to be marked with
  $\transmit$ (i.e., occurred in the restoration context), how many
  barriers we changed to resizing, how many offer kill and offer fork
  calls we included.}  \TSComment{check out
  Table~\ref{tab:applications}. The barrier column shows how many of
  the original barriers were ported to resizing barriers. In all cases
  except bc we converted all of them. The only reason why bc is
  different is because barriers occur deeper in the call stack and our
  prototype cannot handle that. The algorithm itself is compatable
  (i.e. changing these barriers is as easy as any other, there is
  nothing special about them.)}

\ADComment{Make sure we address the point that placing the primitives was pretty easy.}

\subsection{The GPUs we Tested}

\HEComment{the reason we need SVM fine grain and atomics may be better
discussed directly in the section on implementation?}

Our implementation approach requires two optional features of OpenCL
$2.0$, namely SVM fine-grained buffers and SVM atomics, which are
currently not widely supported. \nvidia chips are ruled out since
\nvidia does not support OpenCL $2.0$. AMD support SVM atomics only
their Kaveri family of chips and only on Linux, requiring to compile the
latest driver.  \HEComment{do we need to mention the struggle for AMD ?}
Among the GPUs available to us, only 4 support our requirements: Intel
HD 520, HD 5500 and IRIS 6100, and AMD Radeon R7.

\ADComment{Describe the platforms.  Explain why we could not try other
platforms.}

\subsection{Determining Representative Workloads}\label{sec:representativeworkloads}

\ADComment{Better section title?}
\HEComment{Dimensioning Non-persistent Workloads}

To assess how cooperative blocking algorithms can be, we fire lightweight
but still meaningful workloads that requires workgroups to be taken from
the blocking algorithms.



We tried to measure how much GPU computation is required to maintain
smooth animation on the screen during typical desktop usage. We model
these transient \HEComment{no sure about meaning if transient here}
using

The idea is
to run an infinite loop that launches a given kernel 


\ADComment{Hugues, can you please explain your experimental
  methodology for working out required response times here?}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}

\subsection{The Overhead of Moving to Cooperative Kernels}

\ADComment{Results for stand-alone vs.\ cooperative without co-scheduling.}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}

\subsection{Performance and Responsiveness of Cooperative Scheduling}\label{sec:responsiveness}

\ADComment{Preamble}

\paragraph{Experimental setup}  \ADComment{Describe the data we are gathering.}

\paragraph{Results}  \ADComment{Discuss the results.}



\section{Related Work}\label{sec:relatedwork}


Lots of work on irregular algorithms (assumes fairness), and persistent threads (assumes OB model, which assumes a degree of fairness).  Our contribution can enable these to be safe.

Work on multi-tasking to improve throughput, etc., but this does not relate to fairness and wouldn't help blocking algorithms.

Proposals to implement preemption, but these require hardware support and may be overkill.  Advantage: fully automatic; disadvantage: doesn't capture domain knowledge.

Dynamic parallelism could be extended to give barriers, but doesn't work with work stealing (less general).  It is in recent OpenCL.

\ADComment{Kepler preemption support?  Mention in related work?}


\cite{DBLP:conf/ics/WuCLSV15} \ADComment{Basic idea: map tasks to
  SMs, not to workgroups, so that tasks that would benefit from being
  processed by the same SM can be scheduled to the same SM.  Has
  little to do with preemption, but more to do with irregular
  computation.}

\cite{DBLP:conf/isca/TanasicGCRNV14} \ADComment{I have not read, but
  it's about enabling preemptive multiprogramming, so we should read
  it carefully.}

\cite{DBLP:conf/ppopp/Muyan-OzcelikO16} \ADComment{I have not read; we
  should understand this one carefully.}

\cite{DBLP:conf/asplos/ParkPM15} \ADComment{Investigates how one might
  implement preemption, i.e.\ shows that it can be achieved.
  Simulator only preemption.  Not sure whether their approach would
  give fair scheduling.}

\cite{DBLP:journals/tog/SteinbergerKBKDS14} \ADComment{Shows how to
  keep a GPU busy by flooding it with work.  Would absolutely not
  stand up to preemption.}

\cite{DBLP:conf/usenix/KatoLRI11} \ADComment{Idea seems to be to
  accept that GPUs do not offer epreemption, but find a reasonable way
  to schedule tasks to maximise responsiveness or throughput.}


\section{Conclusions and Future Work}\label{sec:conclusion}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\clearpage

\bibliographystyle{abbrvnat}
\bibliography{tyler}

\appendix

\section{Operational Semantics for Cooperative Kernels}\label{appendix:semantics}

\ADComment{TODO: write it up properly.}

\ADComment{TODO: remark that we do not model transmit faithfully.}

Thread state: $(l, \mathit{ss})$, where $l \in L$ is the thread's
local state, and $\mathit{ss}$ is a semi-colon-separated sequence of
program statements that the thread remains to execute.

Workgroup state: $((l_1, \mathit{ss}_1), \dots, (l_d,
\mathit{ss}_d))$, where each $(l_i, \mathit{ss}_i)$ is a thread state.

Kernel state: a pair $(\sigma, (w_1, w_2, \dots, w_M, \bot, \dots, \bot))$, where $\sigma$ is the shared state,
each $w_i$ is a workgroup state, which are
followed by $N-M$ occurrences of $\bot$ to indicate absent workgroups.

We treat thread-level semantics abstractly by assuming a transition
relation $(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l',
\mathit{ss}'))$ that is defined unless $\mathit{ss} =
\mathsf{special}(); \mathit{tt}$, where $\mathsf{special}$ is one of
$\offerkill$, $\offerfork$, $\globalbarrier$ or
$\resizingglobalbarrier$.

To abstractly accounts for the interaction between global barrier
synchronization and the GPU memory model, we assume a function
$\mathsf{sync}$ that is applied to a global state.  This returns a new
global state in which the shared state and thread local states have
been updated to reflect the fact that all stores to memory and loads
from memory to thread-local storate have completed.

\begin{figure*}
\begin{center}

\[
\inferrule{
w_i(j) = (l, \mathit{ss})
\\
(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l', \mathit{ss}'))
\\
w_i' = w_i[j \mapsto (l', \mathit{ss}')]
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma', (\dots, w_i', \dots))
}
(\textsc{Thread-Step})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerkill();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma, (\dots, w_i', \dots))
}
(\textsc{Kill-No-Op})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_M(j) = (l_j, \offerkill();\mathit{ss})
\\
M > 0
}
{
(\sigma, (\dots, w_{M-1}, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_{M-1}, \bot, \bot, \dots, \bot))
}
(\textsc{Kill})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerfork();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
\\
k \in [0, N - M]
\\
\forall a \in [1, k] \;.\; w_{M+a} = ((l_1, \mathit{ss}), \dots, (l_1, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_i', \dots, w_M, w_{M+1}, \dots, w_{M+k}, \bot, \dots, \bot))
}
(\textsc{Fork})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \globalbarrier();\mathit{ss})
\\
\forall i \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow \mathsf{sync}(\sigma , (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Barrier})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \resizingglobalbarrier();\mathit{ss})
\\
\forall j\;.\;w_1'(j) = (l_{1,j}, \globalbarrier(); \offerfork(); \globalbarrier(); \globalbarrier();\mathit{ss})
\\
\forall i \neq 1 \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \globalbarrier(); \globalbarrier(); \offerkill(); \globalbarrier();\mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Resizing-Barrier})
\]

\end{center}

\caption{Abstract operational semantics for our cooperative kernels language extensions}

\end{figure*}



\end{document}
