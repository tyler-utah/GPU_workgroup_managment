%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace]{sigplanconf-pldi16}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}



\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}


\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

\paragraph{The needs of irregular data-parallel algorithms}
Acceleration of general-purpose computations on graphics processing
units (GPUs) has tended to focus on \emph{regular} data-parallel
algorithms, for which work to be processed can be evenly split between
the workgroups that execute a GPU kernel ahead of time.  However, many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  There is growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate graphs, with several recent notable
successes~\cite{...}.

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.  As another example, breadth-first graph
traversal uses a frontier-based approach that requires a barrier
synchronization between all workgroups to achieve consensus that
exploration of a frontier has been accomplished.  \ADComment{Perhaps
  make the point that inter-workgroup synchronization is possible in
  very rare cases without blocking, e.g. threadfence reduction.}

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular
data-parallel algorithms accelerated on a GPU, this translates to
requiring fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if workgroup $A$ holds a mutex and workgroup $B$ is
spin-waiting for the mutex to be released, an unfair scheduler can
forever schedule $B$, so that $A$ never releases the mutex.
Similarly, an unfair scheduler can induce starvation if workgroup $A$
is at an inter-workgroup barrier awaiting the arrival of other
workgroups, by allowing A to spin forever so that the other workgroups
do not progress to the barrier.

\paragraph{Occupancy-bound execution}
Current GPU programming models---CUDA~\cite{...}, OpenCL~\cite{...}
and HSA~\cite{...}---specify very few constraints on the manner in
which workgroups are scheduled.  In particular, they do \emph{not}
specify that workgroups must be fairly scheduled, and implementations
of these programming models do not provide fair scheduling in
practice.  Roughly speaking, each workgroup executing a GPU kernel is
mapped to a hardware \emph{compute unit}.  If a kernel is executed
with more workgroups than there are compute units then at least two
workgroups must share a single compute unit.  The simplest way to
handle this is via an \emph{occupancy-bound} execution model, whereby
once a workgroup has commenced execution on a compute unit (is has
become \emph{occupant}), the workgroup has exclusive access to the
compute unit until it has finished execution, after which a subsequent
workgroup can be bound to the compute unit.  Experiments suggest that
the occupancy-bound execution model is widely implemented by today's
GPU devices and drivers~\cite{...}, and it is evidently a simple model
for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, because
they are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume that the target GPU device provides the occupancy-bound
execution model, which they exploit by scheduling no more workgroups
than there are available compute units~\cite{...}.  This works today
because \emph{today}'s GPUs seem to provide the occupancy-bound
execution model.

\paragraph{Resistance to occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable from the point-of-view of
GPU vendors and end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all workgroups, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.  Precisely because
of this problem, operating systems employ a GPU \emph{watchdog} to
heuristically detect and kill long-running computations on the GPU
being used for display rendering.  If this watchdog is disabled,
execution of a long-running kernel causes the display to freeze for
the duration of kernel execution.  Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets.  It is desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling requires being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts~\cite{CitePersonalCommunication}, is that
\textbf{(1)} GPU vendors will not commit to the occupancy-bound
execution model that they currently implement, due to the need for
multi-tasking and energy throttling, yet \textbf{(2)} GPU vendors will
not guarantee fair scheduling, due to the runtime overhead associated
with supporting full preemption of workgroups, e.g.\ based on
time-slicing.  Vendors instead wish to retain the essence of the
occupancy-bound execution model, which is simple to implement
efficiently, and provide preemption for the special cases associated
with multi-tasking for high-priority jobs (most crucially graphics)
and energy throttling.  In particular, they do not want to commit to
fair scheduling of a workgroup once it has been preempted.

\paragraph{Our proposal}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and written using two additional language
primitives, $\offerkill$ and $\offerfork$, which must be placed
carefully by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$ to sacrifice itself to
the scheduler.  This indicates to the scheduler that the workgroup
would ideally continue with its execution, but that if required the
scheduler may destroy the workgroup for good: the cooperative kernel
is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates to the scheduler that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commencing execution at the $\offerfork$ program point, inheriting the
state of the workgroup that executed $\offerfork$.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel, with
both functional and non-functional requirements.  Functionally: the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled; the cooperative kernel must be robust to
workgroups leaving and joining the computation when the $\offerkill$
and $\offerfork$ functions, respectively, are called.
Non-functionally: a cooperative kernel must ensure that $\offerkill$
is executed frequently enough that the scheduler can accommodate
soft-real time constraints, e.g.\ allowing a smooth frame-rate for
graphics or allowing compute units to be powered down quickly when
required; the scheduler should respond to $\offerkill$ and
$\offerfork$ calls in a manner that avoids under-utilisation of
hardware resources by the cooperative kernel, e.g.\ the scheduler
should not kill workgroups unless they are required for another
purpose.

Cooperative kernels have several appealing properties.  First, they
accommodate the needs of blocking algorithms, including irregular
data-parallel algorithms: such algorithms can be implemented naturally
as cooperative kernels \ADComment{boast that we did a good set}, so
that fair scheduling between workgroups is guaranteed.  Second, they
have no impact on the development of regular (non-cooperative
kernels): these can be programmed exactly as they are now.  Third, the
programming model is backwards compatible: a non-blocking algorithm
implementation that works on today's GPU platforms can be upgraded to
use our language extensions for cooperative scheduling, and will
behave exactly as it does at present if the new $\offerkill$ and
$\offerfork$ functions are simply ignored.  Fourth, the programming
model is easy to implement on top of the occupancy-bound execution
model that current GPU platforms provide; we present a
proof-of-concept implementation on top of OpenCL 2.0 that requires no
special-purpose hardware or driver support.  Fifth, our results
suggest that cooperative kernels can provide efficient multi-tasking
of cooperative and non-cooperative tasks: our experiments across a
range of GPUs from two vendors show very good response times for
scheduling short, non-cooperative kernels concurrently with
long-running cooperative kernels.  These performance results are with
respect to our proof-of-concept, portable implementation; a vendor
specific implementation of cooperative kernels could likely yield
better performance still through device-specific driver and compiler
support.

In summary, our main contributions are:

\begin{enumerate}

\item \emph{Cooperative kernels}, an extension to the GPU programming model to support the scheduling requirements of blocking algorithms  \ADComment{Maybe big up to say that we give semantics to the new operations.}

\item A portable implementation of cooperative kernels on top of the OpenCL 2.0 programming model

\item The porting of \ADComment{TODO} irregular graph algorithms \ADComment{tighten} to the cooperative kernels model

\item \ADComment{The experiments}

\end{enumerate}

\ADComment{Text to maybe work in.}

In traditional co-operative multi-tasking, a task relinquishes control
and regains control in a stateful manner. We propose a lighter weight
model we believe to suitible for the type of blocking algorithms we
are aware of for GPUs.

The scheduler agrees to scheduler the workgroups fairly, but is at
liberty to eliminate workgroups for good when they offer themselves
to be killed. This allows the schduler to repond to changing depands
on the GPU, e.g. to implement energy throttling.

Prototype implementation of the programming model using OpenCL 2.0
advance features to build the scheudler and implement a clang based
compiler to transform input programs using the language extensions to
lower-level programs that interact with the scheduler. Our prototype
simulates multiple independent kernels into a mega-kernel. A vendor
specific implementation could achieve higher efficiency through
modications to the GPU driver that may take advantage of hardware
features. (e.g. mega kernel might not be good to compile). Its
reasonable to think our proof of concept provides a lower bound on how
well we can do.

We ported a large set of blocking algorithms to our programming
model. We evaluate co-operative scheduling with a cooperative kernel
and non-coperative kernel. GPUs from multiple vendors and access
various things.

Our results are promising.

In the evaluation, make a note that the applications we worked on
Namely if they use only a barrier or work-stealing idiom, are
straightforward to adopt to our programming model.



\section{Motivating example: work stealing}

\ADComment{Use the work stealing example to both introduce OpenCL and
  also make our key point: that arbitrary preemption will not work.}


\section{The ckill and cfork primitives}

\ADComment{Remind the reader that these are inspired by work stealing
  and barrier synchonization.  In \mysec\ref{ } we describe the
  semantics of kernels that use these primitives.  In \mysec\ref{ } we
  show how work stealing can be implemented.  In \mysec\ref{ } we show
  how a barrier can be implemented.}

\subsection{Semantics of the new primitives}

\ADComment{General semantics: let $N$ be the maximum number of
  workgroups the GPU of interest can support [gloss over or deal with
    issue that this is kernel-dependent].  A kernel is launched with
  $M$ workgroups, for some $0 < M \leq N$.  Initially these are all
  scheduled, and are guaranteed to make independent forward progress.}

\ADComment{The purpose of ckill is for a workgroup to volunteer to
  quit execution of the kernel if the resources that it is using
  (e.g.\ the associated compute unit) are required for other tasks.  A
  ckill statement must be reached under workgroup-uniform control
  flow.  When a workgroup reaches ckill, the workgroup either
  terminates execution (because the environment requires the
  workgroup's resources), or continues execution as if the ckill
  statement were a no=op).  From a semantics perspective this is a
  non-deterministic choice: an application that invokes ckill should
  be prepared for either outcome.  In practice, the effect of ckill
  depends on environmental conditions.}

\ADComment{The purpose of cfork is for a workgroup to indicate that it
  would be useful to have additional workgroups join the computation,
  and to specify that the program point associated with the cfork
  statement would be a suitable place for a new workgroup to start
  executing.  A cfork statement must also be reached under
  workgroup-uniform control flow.  If $C$ workgroups are executing
  when a workgroup reaches a cfork statement, with $0 < C \leq M$,
  then between 0 and $M-C$ new workgroups commence execution from the
  following statement.  From a semantics perspective, the number of
  workgroups that join the computation is nondeterministic, within the
  permitted range.  In practice, how many workgroups join depends on
  environmental conditions.}

\ADComment{TODO: describe persistent variables.}

\ADComment{TODO: explain that ckill and cfork atomically update the
  kernel context (ids).}

\ADComment{TODO: somewhere we need to explain the constraints this
  places on programming: how ids work, what one can assume about them,
  etc.}

\ADComment{Now describe the more refined semantics where IDs are
  relinquished in decreasing order; explain that from a formal
  perspective thie refines the original semantics.}

\ADComment{In the above, explain at some point how a scheduler can
  exploit cfork and ckill to utilise the GPU resources.}

\subsection{Preemption-aware work stealing}

\ADComment{Show how work stealing can now be made safe.}

\subsection{A preemption-aware global barrier}

\ADComment{Show how the global barrier can be written in a manner that
  maintains contiguous ids using the refined semantics.}


\section{Application-specific optimisations}

\ADComment{Explain that for global barriers, we explain how some new
  primitives can lead to a better implementation.  Explain that for
  work stealing there are various ways the existing ckill and cfork
  primitives can be used.}

\subsection{An optimised barrier implementation}

\ADComment{Remark that the barrier of \mysec\ref{sec:naivebarrier} may
  be inefficient, discuss the use of query primitives to make this
  more efficient.}

\subsection{Variations on work stealing}

\ADComment{Three cases here: always ckill/cfork, ckill/cfork only when
  queue is empty, and have a ``mediator'' workgroup to manage things.}


\section{A mega kernel-based prototype}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}


\section{Experimental evaluation}

\ADComment{TODO: write up notes from whiteboard chat.}


\section{Related work}

\section{Conclusions and future work}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\section{Old intro}


\ADComment{Points to make: It is important to be able to execute
  long-running, cooperative applications on GPUs (give examples).
  However, resource sharing and resource flexibility is also important
  (give examples, including sharing resources with graphics, sharing
  resources between compute taks, and being flexible about resources
  under energy constraints), and these are not supported by today's
  GPU platforms: long-running applications cause display freezes, etc.
  Resource-based preemption, whereby workgroup execution is postponed
  to free up resources for other purposes (or to reduce resources to
  save energy) can potentially enable resource sharing.  There is
  evidence in the research literature that efficient resource sharing
  for GPUs is possible (cite academic works), and signs from industry
  that this is coming in future architectures.  [Explain more clearly
    what we mean by resource-based preemption.]  Unfortunately,
  arbitrary resource-based preemption is fundamentally incompatible
  with cooperative applications---only trivial examples of cooperation
  are safe [reduction, load balancing via atomic increment]; the HSA
  model of guaranteeing preemption in reverse order of IDs allows some
  cooperative applications to be implemented.  Still, in the general
  case even a mutex cannot be used safely.}

\ADComment{Say that we believe the problem can only be solved through
  programming language innovations with appropriate runtime support.
  We have observed inter-workgroup barriers and work stealing to be
  the main use cases for cooperation, and so in this paper propose two
  new language primitives, ckill and cfork, designed to meet the needs
  of these primitives.  [Describe them a bit.]}

\ADComment{Move on to our main contributions}

\ADComment{This is what we had before.}

Graphics processing units (GPUs) are highly parallel co-processors that are now incorporated into many
devices, ranging from the world's most powerful supercomputers to battery-powered devices such as tablets and mobile phones. While
originally designed to accelerate graphics applications, GPUs are now
used in a wide range of applications, including molecular simulations
and weather forecasting. \ADComment{Expand the list of application areas a bit more; mention their attractiveness due to energy-efficiency.}

New application domains for GPUs are enabled by general purpose
programming languages with associate compilation tools that target GPU architectures.
The current most popular GPU programming languages are CUDA and OpenCL. These languages allow developers to
write C-like programs that can be dispatched and executed on a GPU
device.

GPUs are massively parallel, employing many threads to execute a
program.  These threads are organised hierarchically: groups of threads
are called \emph{workgroups}, and threads interact differently depending on whether they reside in the same workgroup or not.  Traditionally, applications accelerated by GPUs are
data-parallel, requiring little interaction between threads. A small
set of bulk synchronisation primitives are provided and any
inter-thread communication must be performed around these primitives.  \ADComment{Do we want to make the point that this is traditionally limited to the intra-workgroup case?}

While this traditional way to program GPUs has proved useful, it also
has limitations for some applications, in particular applications that
expose a dynamic amount of parallelism throughout execution. Such
applications are said to have irregular parallelism (we call such
applications \emph{IP applications}). Efficient parallel computation
of IP applications requires finer-grained synchronisation than that
which is provided by GPU language primitives.

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.


\end{document}
