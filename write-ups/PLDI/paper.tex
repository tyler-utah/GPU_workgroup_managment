%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace]{sigplanconf-pldi16}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}
\usepackage{mathpartir}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}

\newcommand{\transmit}{\mathsf{transmit}}


\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}
\newcommand{\globalbarrier}{\mathsf{global\_barrier}}
\newcommand{\resizingglobalbarrier}{\mathsf{resizing\_global\_barrier}}
\newcommand{\getgroupid}{\mathsf{get\_group\_id}}
\newcommand{\getnumgroups}{\mathsf{get\_num\_groups}}
\newcommand{\getlocalid}{\mathsf{get\_local\_id}}
\newcommand{\getglobalid}{\mathsf{get\_global\_id}}
\newcommand{\getlocalsize}{\mathsf{get\_global\_id}}

\newcommand{\NumAlgorithms}{8}

\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

\ADComment{Open to attack: why not just buy another GPU?}

\paragraph{The needs of irregular data-parallel algorithms}
Acceleration of general-purpose computations on graphics processing
units (GPUs) has tended to focus on \emph{regular} data-parallel
algorithms, for which work to be processed can be evenly split between
the workgroups that execute a GPU kernel ahead of time.  However, many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  There is growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate linked data structures, with several recent notable
successes~\cite{...}.

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, many graph algorithms employ a level-by-level strategy, requiring a global barrier between each level.
As another example, work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular
data-parallel algorithms accelerated on a GPU, this translates to
requiring fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if workgroup $A$ holds a mutex, an unfair scheduler may cause
another workgroup $B$ to spin-wait forever for the mutex to be
released.  Similarly, an unfair scheduler can a workgroup to spin-wait
at a global barrier so that other workgroups do not make progress
towards the barrier.

\paragraph{A degree of fairness: occupancy-bound execution}
GPU schedulers are \emph{not} fair. the current GPU
programming models---OpenCL~\cite{...}, CUDA~\cite{...}  and
HSA~\cite{...}---specify almost no guarantees regarding scheduling of
workgroups, and implementations of these programming models do not
provide fair scheduling in practice.  Roughly speaking, each workgroup
executing a GPU kernel is mapped to a hardware \emph{compute
  unit}.\footnote{In practice the situation can be more complex: if a
  kernel is sufficiently simple and if workgroups are small enough,
  multiple workgroups may be mapped to a single compute unit.  We assume, for the purpose of the current discussion, that the kernel is sufficiently complex that this does not happen.}
%
If a kernel is executed with more workgroups than there are compute
units then at least two workgroups must share a single compute unit.
The simplest way for a GPU driver to handle this is via an \emph{occupancy-bound}
execution model~\cite{...}.  With this model, once a workgroup has commenced execution on a
compute unit (it has become \emph{occupant}), the workgroup has
exclusive access to the compute unit until it has finished execution,
after which a subsequent workgroup can be bound to the compute unit.
Experiments suggest that the occupancy-bound execution model is widely
implemented by today's GPU devices and drivers~\cite{...}.  It is
evidently a simple model for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, because
they are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume that the target GPU device provides the occupancy-bound
execution model.  They exploit the model by scheduling no more workgroups
than there are available compute units~\cite{...}.  This works
because \emph{today}'s GPUs seem to provide the occupancy-bound
execution model.

\paragraph{Resistance to guaranteeing occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable from the point-of-view of
GPU vendors and end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all compute units, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.  Precisely because
of this problem, operating systems employ a GPU \emph{watchdog} to
heuristically detect and kill long-running computations on the GPU
being used for display rendering~\cite{...}.  Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets~\cite{...}.  In the future, it will be desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling would require being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts~\cite{CitePersonalCommunication}, is that (1)
GPU vendors will not commit to the occupancy-bound execution model
that they currently implement, due to the need for multi-tasking and
energy throttling, yet (2) GPU vendors will not guarantee fair
scheduling, due to the runtime overhead associated with supporting
full preemption of workgroups~\cite{ISCAPAPERSreeMentioned}.  Vendors
instead wish to retain the essence of the occupancy-bound execution
model, which is simple to implement efficiently, and provide
support for preemption only in key special cases.


\paragraph{Our proposal: cooperative kernels}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and is written using two additional
language primitives, $\offerkill$ and $\offerfork$, which must be
placed carefully by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$, offering to
sacrifice itself to the scheduler.  This indicates to the scheduler
that the workgroup would ideally continue with its execution, but that
if required the scheduler may destroy the workgroup for good: the
cooperative kernel is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates to the scheduler that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commence execution directly after the $\offerfork$ program point.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel, with
both functional and non-functional requirements.  Functionally, the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled, while the cooperative kernel must be
robust to workgroups leaving and joining the computation in response
to $\offerkill$ and $\offerfork$.  Non-functionally, a cooperative
kernel must ensure that $\offerkill$ is executed frequently enough
that the scheduler can accommodate soft-real time constraints,
e.g.\ allowing a smooth frame-rate for graphics, or allowing compute
units to be powered down quickly when required.  The scheduler should
respond to $\offerkill$ and $\offerfork$ calls in a manner that avoids
under-utilisation of hardware resources by the cooperative kernel,
e.g.\ the scheduler should not kill workgroups unless they are
required for another purpose, and should facilitate forking of
additional workgroups when possible.

The cooperative kernels programming model has several appealing
properties:

\begin{enumerate}

\item By providing fair scheduling between workgroups, cooperative
  kernels meet the needs of blocking algorithms, including irregular
  data-parallel algorithms.

\item The model has no impact on the development of regular
  (non-cooperative) kernels: these can be programmed exactly as they
  are now, as can graphics shading routines written using APIs such as
  OpenGL, Vulkan and DirectX.

\item The model is backwards-compatible: a non-blocking algorithm
  implementation that works on today's GPU platforms can be upgraded
  to use our language extensions for cooperative scheduling, and will
  behave exactly as it does at present if the new $\offerkill$ and
  $\offerfork$ functions are simply ignored.

\item The model can be implemented on top of the occupancy-bound
  execution model that current GPU platforms provide---we present a
  proof-of-concept implementation on top of OpenCL 2.0 that requires
  no special-purpose hardware or driver support.

\item Our experiments across four GPUs from two vendors show that our
  portable, proof-of-concept implementation of cooperative kernels can
  provide efficient multi-tasking of cooperative and non-cooperative
  tasks.  A GPU vendor implementation could likely provide even better
  efficiency, by leveraging platform-specific hardware, driver and
  compiler features.

\end{enumerate}

Placing the $\offerkill$ and $\offerfork$ primitives requires manual
effort and care, but our experience porting a representative set of
GPU-accelerated irregular algorithms to use cooperative kernels
suggests that this is straightforward in practice.  We discuss this in
detail in Section~\ref{sec:portingalgorithms}.

In summary, our main contributions are:

\begin{itemize}

\item \emph{Cooperative kernels}, an extension to the GPU programming model to support the scheduling requirements of blocking algorithms.  We describe the semantics of the programming model in detail (Section~\ref{sec:cooperativekernels}). 

\item A proof-of-concept portable implementation of cooperative
  kernels on top of the OpenCL 2.0 programming model
  (Section~\ref{sec:implementation}).

\item An experience report porting \NumAlgorithms{} irregular GPU
  algorithms to use cooperative kernels, and experimental results
  assessing the performance overhead associated with cooperation and
  the responsiveness to high priority tasks that cooperation enables,
  across four GPUs from two vendors (Section~\ref{sec:experiments}).

\end{itemize}

We begin by providing an overview of the OpenCL programming model and
two motivating examples of irregular algorithms (Section~\ref{sec:background}).  At the end of the paper we discuss related work (Section~\ref{sec:relatedwork}) and avenues for future work (Section~\ref{sec:conclusion}).

\section{Background and Examples}\label{sec:background}

We use two examples of irregular data-parallel algorithms---a work stealing queue and breadth-first graph
search---to introduce the industry-standard OpenCL programming model on which we base cooperative kernels.

\begin{figure}
\ADComment{Tyler/Hugues: please insert workstealing example.}
\begin{lstlisting}
TODO
\end{lstlisting}
\caption{An excerpt of a work stealing algorithm in OpenCL}\label{fig:workstealing}
\end{figure}

\begin{figure}
\ADComment{Tyler/Hugues: please insert graph traversal example.}
\begin{lstlisting}
TODO
\end{lstlisting}
\caption{An OpenCL graph search algorithm, using a global barrier for synchronisation}\label{fig:graphsearch}
\end{figure}

\paragraph{OpenCL kernels}

An OpenCL program is divided into \emph{host} and \emph{device}
components.  A host application runs on the CPU and launches one or
more \emph{kernels} that run on accelerator devices---GPUs in the
context of this paper.  A kernel is writen in OpenCL C, based on C99.
All threads executing a kernel execute the same entry function with
identical arguments, but a thread may query its unique thread id in
order to access different data or follow different control flow paths
to other threads.

Figure~\ref{fig:workstealing} shows a simplified version of an OpenCL
kernel that uses a work stealing queue.  \ADComment{Describe some
  aspects of the example, saying that the mutex is not a primitive,
  and can be coded up using the memory model but is not safe due to
  the execution model.  Make it clear that the lock function uses busy-waiting.}  Figure~\ref{fig:graphsearch} shows the
skeleton of a graph traversal algorithm.  \ADComment{Describe some
  aspects of the example, saying that the global barrier is not a
  primitive, and can be coded up using the memory model but is not
  safe due to the execution model.  Make it clear that the barrier uses busy-waiting.}

\paragraph{Execution model}

The threads executing a kernel are hierarchically organised.  A kernel
is executed by a number of \emph{workgroups}, and each workgroup
consists of a fixed number of \emph{threads}.  Within a workgroup,
threads may also be organised into \emph{subgroups} (termed
\emph{warps} and \emph{wavefronts} on NVIDIA and AMD architectures,
respectively).

Two barrier primitives are provided in OpenCL.  A workgroup barrier
synchronises all threads in a workgroup and ensures memory
consistency.  This barrier is a \emph{workgroup-level} function: it
must be encountered uniformly by all or none of the threads of a
workgroup, thus can only appear in conditional code if all threads in
a workgroup that reach the barrier agree on the guards of all
enclosing conditional statements.  A subgroup barrier works
analogously at the subgroup level.  As discussed below, a
\emph{global} barrier (synchronising all threads running a kernel) is
not provided as a primitive.

OpenCL specifically makes no guarantees about fair scheduling between
workgroups, stating~\cite{...}: \emph{``A conforming implementation
  may choose to serialize the workgroups so a correct algorithm cannot
  assume that workgroups will execute in parallel.  There is no safe
  and portable way to synchronize across the independent execution of
  workgroups since once in the work-pool, they can execute in any
  order.''}  OpenCL is not alone in this regard: CUDA similarly
provides no guarantees~\cite{...}, and HSA guarantees only that
\ADComment{Tyler, can you state the HSA guarantee?}~\cite{...}.

Revisiting the examples of Figures~\ref{fig:workstealing}
and~\ref{fig:graphsearch}: the mutex function of
Figure~\ref{fig:workstealing} will cause starvation if a workgroup
that holds the mutex is not scheduled again and so does not release
the mutex.  A similar issue affects the global barrier function of
Figure~\ref{fig:graphsearch}.  Because the barrier is implemented
using busy-waiting, a set of workgroups waiting at the barrier may be
continuously scheduled, starving a workgroup that has not yet arrived
at the barrier and meaning that no workgroup proceeds past the
barrier.

\paragraph{Identity and size functions}

Builtin functions $\getlocalid()$ and $\getglobalid()$ provide a
thread with an id that is unique within its workgroup and unique
across all workgroups, respectively.  Each workgroup has an id,
obtained via $\getgroupid()$.  The number of threads per workgroup and
the number of workgroups are obtained via $\getlocalsize()$ and
$\getnumgroups()$.  A thread's local and global id are related via the
equation $\getglobalid() = \getgroupid() \times \getlocalsize() +
\getlocalid()$.

In general, an OpenCL kernel is organised as a multi-dimensional grid
of multi-dimensional workgroups, and the above functions each take
an integer dimension argument.  For simplicity, we consider only the
one-dimensional case throughout the paper, which captures all the
GPU-accelerated irregular algorithms we are aware of, and use
e.g.\ $\getglobalid()$ to mean $\getglobalid(0)$.


\paragraph{Memory model}

An OpenCL kernel has access to four memory spaces.  Shared virtual
memory (SVM) is accessible to all threads, and to the host
application.  Atomic operations on this memory space enable
fine-grained communication between host and device.  Our
implementation of cooperative kernels
(Section~\ref{sec:implementation}) depends on this recent OpenCL
feature.  Global memory is shared among all threads executing a
kernel, but can only be safely accessed by the host application before
or after kernel execution.  Local memory is shared among all threads
in the same workgroup: each workgroup has its own portion of local
memory, which can be used for fast intra-workgroup communication.
Each thread has an allocation of very fast private memory, where
function-local variables are allocated.

Fine-grained communication between threads, and also with the host in
the case of SVM, is enabled via a set of atomic data types and
operations, based on the C11 memory model~\cite{...}.
\ADComment{Perhaps say more.}  \ADComment{Can we illustrate using the
  examples?}


\section{The Cooperative Kernels Programming Model}\label{sec:cooperativekernels}

We describe our cooperative kernels programming model as an extension
to OpenCL.  However, the cooperative kernels concept is more general,
and could be applied to extend other GPU programming models, such as
CUDA and HSA.

We describe the semantics of cooperative kernels
(Section~\ref{sec:semantics}), then summarise the important
differences between programming with regular vs.\ cooperative kernels
(Section~\ref{sec:programmingguidelines}) and the non-functional
properties that a developer of a cooperative kernel, and the
implementer of a scheduler for cooperative kernels, should strive for
(Section~\ref{sec:nonfunctional}).  The semantics of cooperative
kernels has been guided by the applications that we have studied
(described in Section~\ref{sec:portingalgorithms}), and we discuss several cases where we
might have taken different, equally reasonable, semantic decisions (Section~\ref{sec:semanticalternatives}).
We conclude the section by discussing the backwards-compatible nature
of cooperative kernels with respect to blocking algorithms designed to
run on current GPU platforms
(Section~\ref{sec:backwardscompatibility}).

\subsection{Semantics of Cooperative Kernels}\label{sec:semantics}

As with a regular OpenCL kernel (see Section~\ref{sec:background}), a
cooperative kernel is launched by the host application.  The host
application passes parameters to the kernel and specifies a desired
number of workgroups, each consisting of a specified number of
threads.  The parameters to a cooperative kernel are \emph{immutable}, as if
they were qualified with the \texttt{const} keyword (though pointer
parameters can refer to mutable data).

A cooperative kernel is written in OpenCL C plus the following
language extensions: $\transmit$, a
new qualifier on the local variables of a thread; $\offerkill$ and
$\offerfork$, the key functions that enable cooperative scheduling;
$\globalbarrier$ and $\resizingglobalbarrier$, two primitives to allow
inter-workgroup synchronisation.

We now explain the semantics of these language extensions, after which
we explain how the work stealing and graph traveral algorithms of
Section~\ref{sec:background} can be turned into cooperative kernels
that use the extensions.  We present the semantics intuitively, using English.
In Appendix~\ref{appendix:semantics} we present an abstract operational semantics for our language extensions in a simple GPU-like programming model.

\paragraph{Transmitted variables}

A variable declared in the root scope of the cooperative kernel can
optionally be annotated with a new $\transmit$ qualifier.  By
annotating a variable with $\transmit$, the programmer indicates that
when a workgroup spawns new workgroups by calling $\offerfork$, the
workgroup should transmit its current value for the variable to serve
as an initial value for the variable for the threads of the new workgroups.  We detail the semantics
for this when we describe $\offerfork$ below.

\paragraph{Active workroups}

If the host application launches a cooperative kernel requesting $N$
workgroups, this indicates that the kernel should be executed with a
maximum of $N$ workgroups, and that as many workgroups as possible, up
to this limit, are desired.  However, the scheduler may initially
schedule fewer than $N$ workgroups, and as explained below the number
of workgroups that execute the cooperative kernel can change during
the lifetime of the kernel.

The workgroups executing the kernel are called \emph{active
  workgroups}, and $M$ denotes the number of active workgroups.
Initially, the number of active workgroups $M$ is initialised to some
integer between $1$ and $N$.  The initial active workgroups have
consecutive workgroup ids in the range $[0, M-1]$.

The $\getgroupid()$ function retrieves the id of a workgroup as usual
(see Section~\ref{sec:background}).  When executed by a cooperative
kernel, $\getnumgroups()$ returns $M$, the \emph{current} number of
active workgroups.  This is in contrast to $\getnumgroups()$ for
regular kernels, which returns the fixed number of workgroups that
were requested at kernel launch time.

Fair scheduling is guaranteed between active workgroups.  That is, if
some thread in an active workgroup is enabled then eventually some
thread in the active workgroup is guaranteed to execute an
instruction.  Note that our programming model does not mandate that
threads within a workgroup must be fairly scheduled, and indeed this
is often not the case in GPU programming models, because threads are
often organised into \emph{warps} or \emph{wavefronts} within a
workgroup, exhibiting lock-step predicated execution that is
inherently unfair.

\paragraph{Semantics for $\offerkill$}

The $\offerkill$ primitive allows the cooperative kernel to return
compute units to the scheduler by offering to sacrifice workgroups.
The idea is that while having the scheduler be permitted to terminate
the execution of workgroups in an arbitary fashion would be drastic,
the kernel may contain identifiable points of execution at which a
workgroup could gracefully leave the computation.  We give examples of
these for work stealing and graph traversal below.

Similar to the OpenCL workgroup $\mathsf{barrier}$ primitive,
$\offerkill$, is a workgroup-level function---it must be encountered
uniformly by all threads in a workgroup (see
Section~\ref{sec:background}).

Suppose a workgroup with id $m$ executes $\offerkill$.  If $m < M-1$
or $M=1$, i.e.\ the workgroup's id is not the largest among the active
workgroups, or if only a single workgroup is executing the cooperative
kernel, then $\offerkill$ is a no-op.  If instead $M > 1$ and $m =
M-1$, there is a non-deterministic choice between (a) $\offerkill$
executing as a no-op, or (b) execution of the workgroup ceasing and
the number of active workgroups $M$ being decremented by one.  Case
(b) corresponds to the scheduler accepting the workgroup's offer to
cease execution, allowing the scheduler to schedule some other task on
the compute unit on which the workgroup was executing.

The semantics of $\offerkill$ is atomic.  That is, if multiple
workgroups reach $\offerkill$ simultaneously, the semantics is as if
each workgroup executes $\offerkill$ in some arbitrary sequential
order.

In this formulation of cooperative kernels, only the active workgroup
with the largest id can be killed, and workgroup 0 can never be
killed.  We discuss the rationale for these choices, as well as
possible alternative choices, in
Section~\ref{sec:semanticalternatives}.

\paragraph{Semantics for $\offerfork$}

The $\offerfork$ primitive allows the cooperative kernel to indicate
to the scheduler that it could benefit from, and is prepared to
handle, additional workgroups joining the computation.  Recall that
the cooperative kernel was launched with a limit of $N$ workgroups,
indicating the number of workgroups that would ideally execute the
kernel.  However, the number of active workgroups, $M$, may be smaller
than $N$, either because (due to competing workloads) the scheduler
did not provide $N$ workgroups initially, or because the kernel has
given up some number of workgroups via $\offerkill$ calls.  Through
$\offerfork$, the kernel and scheduler can work together to allow new
workgroups to join the computation at an appropriate point and with
appropriate state.

Like $\offerkill$, $\offerfork$ is a workgroup-level function.

Suppose a workgroup with id $m\leq M$ executes $\offerfork$.  An
integer $k$ in the range $[0, N-M]$ is chosen nondeterministically,
$k$ new workgroups are spawned with consecutive ids in the range $[M,
  M+k-1]$, and the active workgroup count $M$ is incremented by $k$.

The $k$ new workgroups commence execution at the program point
immediately following the $\offerfork$ call.  The variables that
describe the state of a thread are all uninitialised for the threads
in the new workgroups; reading from these variables without first
initialising them is an undefined behaviour.  There are two exceptions
to this:

\begin{itemize}

\item because the parameters to a cooperative kernel are immutable,
  the new threads have access to these parameters as part of their
  local state and can safely read from them;

\item for each variable $v$ annotated with $\transmit$, every new
  thread's copy of $v$ is initialised to the value that thread 0 in
  workgroup $m$ held for $v$ at the point of the $\offerfork$ call.

\end{itemize}

In effect, thread 0 of the spawning workgroup transmits the relevant
portion of its local state to the new threads.  We discuss in
Section~\ref{sec:semanticalternatives} the rationale for having all
new threads obtain this local state portion from thread 0, as well as
our reasons for selecting which variables to transmit via annotations
rather than transmitting the entire thread state.

Notice that $k=0$ is always a legitimate choice for the number of
workgroups to be spawned by an $\offerfork$ call, and if the number of
active workgroups $M$ is equal to the workgroup limit $N$, $k=0$ is
guaranteed.

\paragraph{Global barriers}

Many irregular algorithms require synchronisation across workgroups.
This can be achieved via a \emph{global barrier}, but a global barrier
is not provided as a primitive in OpenCL, CUDA or HSA.  This is
because, as discussed in Section~\ref{sec:intro}, an inter-workgroup barrier
requires fair scheduling of workgroups.

Because workgroups of a cooperative kernel are fairly scheduled, a
global barrier primitive can be provided.  We specify two such primitives: $\globalbarrier()$
and $\resizingglobalbarrier()$.

A $\globalbarrier()$ is a kernel-level function: if it appears in
conditional code then it must be reached by \emph{all} threads
executing the cooperative kernel.  On reaching a $\globalbarrier()$, a
thread waits until all threads have arrived at the barrier.  Once all
threads have arrived, the threads may proceed past the barrier with
the guarantee that all global memory accesses issued before the
barrier have completed.

A $\resizingglobalbarrier()$ is identical to a global barrier, except
that, semantically, every workgroup executes $\offerkill$ on reaching
the barrier, and every workgroup executes $\offerfork$ on leaving the
barrier.  Thus $\resizingglobalbarrier()$ is semantically equivalent to:
%
\lstset{basicstyle=\tt}
\begin{lstlisting}
  $\offerkill()$;
  $\globalbarrier()$;
  $\offerfork()$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

The $\globalbarrier()$ primitive can be implemented directly using the
atomic operations provided by the OpenCL 2.0 memory model~\cite{...},
but the implementation is involved, hence why we include this function
as a primitive.  As noted above, the $\resizingglobalbarrier()$
primitive is, semantically, syntactic sugar for a sequence of other
primitive calls.  We include it as a primitive because (a) we have
found that global barriers often provide a natural point for
cooperation with the scheduler, and (b) a custom implementation of
$\resizingglobalbarrier()$ that provides the semantics described
above can be made significantly more efficient than the above sequence of
calls.  We discuss our efficient implementation of
$\resizingglobalbarrier$ in detail in Section~\ref{sec:resizingbarrier}.

\paragraph{Examples}

\ADComment{TODO}


\subsection{Programming Guidelines}\label{sec:programmingguidelines}

An OpenCL programmer writing a cooperative kernel should pay careful
attention to the following details.

\paragraph{Keeping at least one workgroup alive}

The programmer is responsible for keeping at least one workgroup alive.  In particular, if all workgroups execute $\offerkill$, the scheduler is at liberty to destroy all workgroups, causing the cooperative kernel to exit.  This can be guarded against by wrapping each $\offerkill$ call in a conditional statement:

\lstset{basicstyle=\tt}
\begin{lstlisting}
  if($\getgroupid{0}$ != 0) $\offerkill$();
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

We could, alternatively, have formulated the semantics of $\offerkill$
so that workgroup 0 is never killed.  However, there might be a use
case for a cooperative kernel reaching a point where it would be
acceptable for the kernel to exit, although desirable for some
remaining computation to be performed if competing workloads allow it.

\paragraph{A changing number of workgroups}

In regular OpenCL, $\getnumgroups{0}$ returns a runtime-constant
value: once the number of executing workgroups has been identified, it
is guaranteed not to change.

In contrast, the number of workgroups executing a cooperative kernel
can vary due to workgroups executing $\offerkill$, $\offerfork$ and
$\resizingglobalbarrier$.  The programmer must therefore write their
cooperative kernel in a manner that is robust to changes in the value
returned by $\getnumgroups{0}$.

\paragraph{Cooperative kernels that only use barriers}

\ADComment{Need to account for the issue that $M$ might decrease as
  workgroups actually exit the kernel.}

If a cooperative kernel does not directly issue $\offerkill$ and
$\offerfork$ calls then the only possibility for the number of
workgroups to change is due to $\resizingglobalbarrier$ being invoked.

\ADComment{Shorten sentences.}
In this case, because the resizing barrier is a kernel-level function,
the statements executed by the cooperative kernel can be divided into
\emph{resizing barrier intervals}: a resizing barrier interval starts
at a resizing barrier or the kernel entry point, and ends at a
resizing barrier or a kernel exit point.  At any moment of execution,
all threads are guaranteed to be executing within the same resizing
barrier interval.  As a result, the number of workgroups executing the
kernel is guaranteed to remain unchanged within each resizing barrier
interval, so the result returned by $\getnumgroups{0}$ is guaranteed
to be identical on repeated calls during such an interval.

This guarantee may be exploited by algorithms that perform strided data
processing.  \ADComment{Tyler, can we give an example of a data
  processing loop that would have to be changed?}

\paragraph{Placing the new constructs}

\ADComment{TODO: needs work.}
.  The irregular
algorithms we encountered either employ work stealing or operate on
graph data structures.  Work stealing algorithms have a transactional
flavour, whereby tasks are repeatedly pulled from a queue and
processed by a workgroup.  The point of task completion---the end of a transaction---is a natural place for a workgroup to invoke both $\offerkill$ and $\offerfork$, to indicate that 



\subsection{Non-Functional Requirements of Cooperative Kernels}\label{sec:nonfunctional}

The semantics presented in Section~\ref{sec:semantics} describe the envelope of
behaviour that a developer of a cooperative kernel should be prepared
for.

However, the aim of cooperative kernels is to find a balance that
allows \emph{efficient execution} of long-running kernels
(e.g.\ implementing irregular data-parallel algorithms), and
\emph{responsive multi-tasking} so that the GPU can be shared between
one or more cooperative kernels and a number of short, high-priority
tasks with soft real-time constraints.

To achieve this balance, both an implementation of the cooperative
kernels model, and the programmer of a cooperative kernel, must strive
to meet a number of non-functional requirements, which we outline
here.

\ADComment{Sree mentioned TimeGraph paper.}

\paragraph{Sufficient $\offerkill$ calls}

Recall that the purpose of $\offerkill$ is to provide the scheduler
with an opportunity to destroy a workgroup in order to schedule
higher-priority tasks.  Because these higher-priority tasks, such as
graphics rendering, may have soft real-time constraints, the scheduler
relies on the cooperative kernel to execute $\offerkill$ sufficiently
frequently that the scheduler can meet such real-time constraints.

\ADComment{Forward reference to experiments with display freezing.}

\ADComment{Got here.}

\ADComment{Generosity: initial number of workgroups, and responding to offer fork.}

\subsection{Alternative Semantic Choices}\label{sec:semanticalternatives}

\ADComment{Explain here where we could have done things differently.}

\ADComment{What are difficulties with other alternatives and why we didn't use them.}

\ADComment{Allowing zero to be killed.}

\ADComment{Allowing arbitrary workgroups to be killed (not just the largest workgroup)}

\ADComment{Having thread 0 transmit its state, and transmitting only annotated variables.}

\subsection{Backwards Compatibility}\label{sec:backwardscompatibility}

\ADComment{Perhaps this would fit better under ``programming guidelines'' -- not sure.}

\ADComment{Make the point here that if an irregular algorithm is
  written as a cooperative kernel then it will execute just as safely
  as it does today on current hardware if: (1) the $\transmit$
  annotation and $\offerkill$/$\offerfork$ calls are defined away to
  nothing, (2) $\resizingglobalbarrier$ is redefined to
  $\globalbarrier$, (3) a standard implementation of $\globalbarrier$
  that assumes the occupancy-bound execution model is provided.  If the kernel uses blocking features---the global barrier, mutexes, etc., then it may not execute safely due to the possibility of unfair scheduling, but it will execute no less safely than it would if the cooperative kernels style were not used.}


\section{Prototype Implementation}\label{sec:implementation}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}

\ADComment{Give details of how host and device interact.}

\ADComment{Explain kernel merge tool.}

\subsection{An Efficient Resizing Barrier}\label{sec:resizingbarrier}

\ADComment{Optimised barrier implementation.}



\section{Applications and Experiments}\label{sec:experiments}

\subsection{Porting Irregular GPU Algorithms to Cooperative Kernels}\label{sec:portingalgorithms}

\ADComment{Among other things, make sure we address the point that placing the primitives was pretty easy.}


\section{Related Work}\label{sec:relatedwork}

\section{Conclusions and Future Work}\label{sec:conclusion}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\appendix

\section{Operational Semantics for Cooperative Kernels}\label{appendix:semantics}

\ADComment{TODO: write it up properly.}

\ADComment{TODO: remark that we do not model transmit faithfully.}

Thread state: $(l, \mathit{ss})$, where $l \in L$ is the thread's
local state, and $\mathit{ss}$ is a semi-colon-separated sequence of
program statements that the thread remains to execute.

Workgroup state: $((l_1, \mathit{ss}_1), \dots, (l_d,
\mathit{ss}_d))$, where each $(l_i, \mathit{ss}_i)$ is a thread state.

Kernel state: an $N$-tuple $(\sigma, (w_1, w_2, \dots, w_M, \bot,
\dots, \bot))$, where each $w_i$ is a workgroup state, which are
followed by $N-M$ occurrences of $\bot$ to indicate absent workgroups.

We treat thread-level semantics abstractly by assuming a transition
relation $(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l',
\mathit{ss}'))$ that is defined unless $\mathit{ss} =
\mathsf{special}(); \mathit{tt}$, where $\mathit{special}$ is one of
$\offerkill$, $\offerfork$, $\globalbarrier$ or
$\resizingglobalbarrier$.

To abstractly account for the interaction between global barrier
synchronization and the GPU memory model, we assume a $\mathsf{sync}$
that can be applied to the shared state $\sigma$, or to the shared
state and a thead local state.  In the first case,
$\mathsf{sync}(\sigma)$ returns an updated shared state that reflects
all in-flight memory stores having completed.  In the second case,
$\mathsf{sync}(\sigma, l)$ returns a thread local state reflecting all
in-flight memory loads issued by the thread having completed.

\begin{figure*}
\begin{center}

\[
\inferrule{
w_i(j) = (l, \mathit{ss})
\\
(\sigma, (l, \mathit{ss})) \rightarrow_{\tau} (\sigma', (l', \mathit{ss}'))
\\
w_i' = w_i[j \mapsto (l', \mathit{ss}')]
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma', (\dots, w_i', \dots))
}
(\textsc{Thread-Step})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerkill();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma, (\dots, w_i', \dots))
}
(\textsc{Kill-No-Op})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_M(j) = (l_j, \offerkill();\mathit{ss})
}
{
(\sigma, (\dots, w_{M-1}, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_{M-1}, \bot, \bot, \dots, \bot))
}
(\textsc{Kill})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerfork();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots)) \rightarrow (\sigma, (\dots, w_i', \dots))
}
(\textsc{Fork-No-Op})
\]

\medskip

\[
\inferrule{
\forall j \;.\; w_i(j) = (l_j, \offerfork();\mathit{ss})
\\
w_i' = ((l_1, \mathit{ss}), \dots, (l_d, \mathit{ss}))
\\
k \in [0, N - M]
\\
\forall a \in [1, k] \;.\; w_{M+a} = ((l_1, \mathit{ss}), \dots, (l_1, \mathit{ss}))
}
{
(\sigma, (\dots, w_i, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (\dots, w_i', \dots, w_M, w_{M+1}, \dots, w_{M+k}, \bot, \dots, \bot))
}
(\textsc{Fork})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \globalbarrier();\mathit{ss})
\\
\forall i \;.\;\forall j\;.\; l_{i,j}' = \mathsf{sync}(\sigma, l_{i,j})
\\
\forall i \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}', \mathit{ss})
\\
\sigma' = \mathsf{sync}(\sigma)
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma', (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Barrier})
\]

\medskip

\[
\inferrule{
\forall i \;.\;\forall j\;.\;w_i(j) = (l_{i,j}, \resizingglobalbarrier();\mathit{ss})
\\
\forall i \;.\;\forall j\;.\;w_i'(j) = (l_{i,j}, \offerkill(); \globalbarrier(); \offerfork();\mathit{ss})
}
{
(\sigma, (w_1, \dots, w_M, \bot, \dots, \bot)) \rightarrow (\sigma, (w_1', \dots, w_M', \bot, \dots, \bot))
}
(\textsc{Resizing-Barrier})
\]

\end{center}

\caption{Abstract operational semantics for our cooperative kernels language extensions}

\end{figure*}





\section{Notes}

\ADComment{A natural exit does not decrement $M$.  We haven't made a point of this, and I don't think we should right now, but we should remember to think this through carefully.}


  \ADComment{Perhaps
  make the point that inter-workgroup synchronization is possible in
  very rare cases without blocking, e.g. threadfence reduction.}

\ADComment{If this watchdog is disabled,
execution of a long-running kernel causes the display to freeze for
the duration of kernel execution.}

\ADComment{Kepler preemption support?  Mention in related work?}

\ADComment{Mention dynamic parallelism and explain why it's no good.}

\section{Remainder of old intro}

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.





\end{document}
