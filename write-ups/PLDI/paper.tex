%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace]{sigplanconf-pldi16}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}



\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\begin{document}

\title{Responsive GPU Workgroup Management in the Presence of
  Persistent Kernels}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

\ADComment{Be sure to say that a GPU \emph{kernel} is.}

Although GPU acceleration of general-purpose computation has focused
on regular data-parallel algorithms, many data-parallel algorithms are
\emph{irregular} \ADComment{say what that means}.  There is growing
interest in accelerating irregular data parallel algorithms on GPUs,
including \ADComment{name some algorithms and say why they matter},
with several recent notable successes~\cite{...}.

By their very nature, irregular algorithms demand synchronization
between the workgroups that execute a GPU kernel.  For example, work
stealing algorithms require the ability for one workgroup to steal
from the queue of another workgroup, which can be safely achieved by
protecting the queue of each workgroup using a mutex.  Breadth-first
graph traversal uses a frontier-based approach that requires a barrier
synchronization between all workgroups to allow computation to proceed
from one frontier to the next.  Inter-workgroup synchronization
involves \emph{blocking} between workgroups, e.g.\ workgroup A may be
blocked until workgroup B releases a mutex, or blocked until all other
workgroups arrive at an inter-workgroup barrier.  Thus, irregular data
parallel algorithms that rely on inter-workgroup synchronization are
\emph{blocking algorithms}.\footnote{\ADComment{Perhaps make the point
    that inter-workgroup synchronization is possible in very rare
    cases without blocking, e.g. threadfence reduction.}}

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular data
parallel algorithms accelerated on a GPU, this translates to requiring
fair scheduling of workgroups.  Without fairness, a workgroup may fail
to make progress, leading to starvation.  For example, if workgroup B
holds a mutex and workgroup A is spinning, waiting for the mutex to be
released, an unfair scheduler can forever schedule A, so that B never
releases a mutex.  Similarly, an unfair scheduler can induce
starvation if workgroup A is at an inter-workgroup barrier, spinning
until other workgroups arrive.

Current GPU programming models---CUDA~\cite{...}, OpenCL~\cite{...}
and HSA~\cite{...}---specify very few constraints on the manner in
which workgroups are scheduled.  In particular, they do \emph{not}
guarantee fair scheduling, and fair scheduling is not observed in
practice.  Roughly speaking, each workgroup executing a GPU kernel is
mapped to a hardware \emph{compute unit}.  If a kernel is executed
with more workgroups than there are compute units then at least two
workgroups must share a single compute unit.  The simplest way to
handle this is an \emph{occupancy-bound} execution model, whereby once
a workgroup has commenced execution on a compute unit (is has become
\emph{occupant}), the workgroup has exclusive access to the compute
unit until it has finished execution, after which a subsequent
workgroup can be bound to the compute unit.  There is empirical
evidence that the occupancy-bound execution model is widely
implemented in practice~\cite{...}.

Clearly, the occupancy-bound execution model does not provide fair
scheduling between workgroups in general: if all compute units are
occupied then a not-yet-occupant workgroup will not be scheduled until
some occupant workgroup has completed execution.  Yet the execution
model \emph{does} provide fair scheduling between occupant workgroups,
which are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume this occupancy-bound execution model, exploiting it by
scheduling no more workgroups than there are available compute units.
Because current GPUs do provide the occupancy-bound execution model in
practice, this strategy allows starvation-free blocking
synchronization between workgroups.

However, none of the current GPU programming models mandate the
occupancy-bound execution model, and there are clear reasons why this
model is not actually desirable from the point-of-view of GPU vendors
nor end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all workgroups, the GPU cannot be used for other
tasks, such as graphics rendering or shorter, higher-priority
computations, in the meantime.  Precisely because of this problem,
operating systems employ a GPU \emph{watchdog} to heuristically detect
and kill long-running computations on the GPU that is being used for
rendering.  If one disables this watchdog and launches a long-running
kernel, the display becomes unresponsive for the duration of kernel
execution.  Second, \emph{energy throttling} is an important concern
for energy-constrained devices, such as smartphones and tablets.  In
the context of mobile GPU computing, it is desirable for a GPU driver
to be able to power down one or more compute units if the battery
level is low or if the user puts the device into a low power state.
Implementing energy throttling requires being able to suspend the
execution of a workgroup, power down the compute unit to which the
workgroup was bound, and schedule the workgroup for later execution on
one of the other compute units when it becomes free.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts~\cite{CitePersonalCommunication}, is that GPU
vendors are reluctant to commit to the occupancy-bound execution model
that they currently implement, due to the need for multi-tasking and
energy throttling, yet do not wish to guarantee fair scheduling, due
to the runtime overhead associated with supporting full preemption of
workgroups, e.g.\ based on time-slicing.  Vendors instead wish to
retain the essence of the occupancy-bound execution model, which is
simple to implement efficiently, providing preemption for the special
cases associated with multi-tasking for high-priority jobs (such as
graphics) and energy throttling.  In particular, they do not want to
commit to fair scheduling of a workgroup once it has been preempted.
As argued above, without fairness it is not possible to implement
non-blocking algorithms, including data parallel irregular algorithms,
safely.


  Doesn't fascilitate energy throttling or
  multi-tasking. Multi-tasking is important esp. for responsive
  systems. (Say something colourful here, its an enabler. A use case
  for today and many use cases for tomorrow.)


  Our Proposal: Non-blocking GPU programs (majority of kernels today)
  can be exucuted however the GPU scheduler sees fit. Algorithm that
  require a degree of fairness (e.g. blocking algorithms) must be
  written a small set of language extensions, and launched in a
  cooperative mode. This indicates that the scheduler must interact
  with these kernels in a special manner via the language
  extensionts. These kernels are written with language extensions that
  allow designated points at which a workgroup can be de-scheduled or
  re-scheduled to be identified.

  In traditonal co-operative multi-tasking, a task reliquishes control
  and regains control in a stateful manner. We propose a lighter
  weight model we believe to suitible for the type of blocking
  algorithms we are aware of for GPUs. At a point where the algorithm
  could proceed with fewer workgroups, a workgroup can offer itself
  to be killed. This indicates to the scheduler that the algorithm
  would happily proceed with the workgroup continuing, but is prepared
  for the workgroup to be destroyed for good. In addition, the programmer
  identifies designated points where it would be beneificial to join the
  computation if available. On offering to fork, the algorithm indicates
  that it is prepared to proceed with the existing workgroups, but is able
  to take advantage of one or more additional workgroups commencing at the
  given program point. Inheriting the state of the invoking workgroup.

  The scheduler agrees to scheduler the workgroups fairly, but is at
  liberty to eliminate workgroups for good when they offer themselves
  to be killed. This allows the schduler to repond to changing depands
  on the GPU, e.g. to implement energy throttling.

  *Note: its not really completely fair, its occupancy bound

  This programming model creates a contract between the scheduler
  and cooperative kernel:

  Functional requirements: Scheduler agrees that cooperative kernels
  have fairly scheduled workgroups. The Cooperative kernel is written
  in a way that can tolerate threads leaving or joining at the
  designated areas (e.g. a thread should not opt to be killed if it
  contains local work to do, and a programmer should be prepared for
  the number of executing workgroups to change dynamically during
  computation).

  Non-Functional: The cooperative kernel must ensure that offer_kill
  is executed frequently enough such that the system can accomodate
  soft-real time constraints (e.g. allowing a smooth framerate for
  graphics or throttling the energy before the mobile battery drains).
  To enable the cooperative execute efficiently, the scheduler should
  adopt a policy that avoids under utililasion of hardware resources
  for the cooperative kernel (e.g. the scheduler should not kill
  workgroups unless they are required).

  The programming model is backwards compatible because semantically
  it is fine for these things to be ignored and GPUs already give
  occupant bound fairness. 

  Our contributions:

  - Programming model in detail (semantics of new operations)

  - Prototype implementation of the programming model using OpenCL 2.0
    advance features to build the scheudler and implement a clang
    based compiler to transform input programs using the language
    extensions to lower-level programs that interact with the
    scheduler. Our prototype simulates multiple independent kernels
    into a mega-kernel. A vendor specific implementation could achieve
    higher efficiency through modications to the GPU driver that may
    take advantage of hardware features. (e.g. mega kernel might not
    be good to compile). Its reasonable to think our proof of concept
    provides a lower bound on how well we can do.

  - We ported a large set of blocking algorithms to our programming
    model. We evaluate co-operative scheduling with a cooperative
    kernel and non-coperative kernel. GPUs from multiple vendors and
    access various things. 

  - Our results are promising.

  - In the evaluation, make a note that the applications we worked on
    Namely if they use only a barrier or work-stealing idiom, are
    straightforward to adopt to our programming model.








\ADComment{Points to make: It is important to be able to execute
  long-running, cooperative applications on GPUs (give examples).
  However, resource sharing and resource flexibility is also important
  (give examples, including sharing resources with graphics, sharing
  resources between compute taks, and being flexible about resources
  under energy constraints), and these are not supported by today's
  GPU platforms: long-running applications cause display freezes, etc.
  Resource-based preemption, whereby workgroup execution is postponed
  to free up resources for other purposes (or to reduce resources to
  save energy) can potentially enable resource sharing.  There is
  evidence in the research literature that efficient resource sharing
  for GPUs is possible (cite academic works), and signs from industry
  that this is coming in future architectures.  [Explain more clearly
    what we mean by resource-based preemption.]  Unfortunately,
  arbitrary resource-based preemption is fundamentally incompatible
  with cooperative applications---only trivial examples of cooperation
  are safe [reduction, load balancing via atomic increment]; the HSA
  model of guaranteeing preemption in reverse order of IDs allows some
  cooperative applications to be implemented.  Still, in the general
  case even a mutex cannot be used safely.}

\ADComment{Say that we believe the problem can only be solved through
  programming language innovations with appropriate runtime support.
  We have observed inter-workgroup barriers and work stealing to be
  the main use cases for cooperation, and so in this paper propose two
  new language primitives, ckill and cfork, designed to meet the needs
  of these primitives.  [Describe them a bit.]}

\ADComment{Move on to our main contributions}

\ADComment{This is what we had before.}

Graphics processing units (GPUs) are highly parallel co-processors that are now incorporated into many
devices, ranging from the world's most powerful supercomputers to battery-powered devices such as tablets and mobile phones. While
originally designed to accelerate graphics applications, GPUs are now
used in a wide range of applications, including molecular simulations
and weather forecasting. \ADComment{Expand the list of application areas a bit more; mention their attractiveness due to energy-efficiency.}

New application domains for GPUs are enabled by general purpose
programming languages with associate compilation tools that target GPU architectures.
The current most popular GPU programming languages are CUDA and OpenCL. These languages allow developers to
write C-like programs that can be dispatched and executed on a GPU
device.

GPUs are massively parallel, employing many threads to execute a
program.  These threads are organised hierarchically: groups of threads
are called \emph{workgroups}, and threads interact differently depending on whether they reside in the same workgroup or not.  Traditionally, applications accelerated by GPUs are
data-parallel, requiring little interaction between threads. A small
set of bulk synchronisation primitives are provided and any
inter-thread communication must be performed around these primitives.  \ADComment{Do we want to make the point that this is traditionally limited to the intra-workgroup case?}

While this traditional way to program GPUs has proved useful, it also
has limitations for some applications, in particular applications that
expose a dynamic amount of parallelism throughout execution. Such
applications are said to have irregular parallelism (we call such
applications \emph{IP applications}). Efficient parallel computation
of IP applications requires finer-grained synchronisation than that
which is provided by GPU language primitives.

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.

\section{Motivating example: work stealing}

\ADComment{Use the work stealing example to both introduce OpenCL and
  also make our key point: that arbitrary preemption will not work.}


\section{The ckill and cfork primitives}

\ADComment{Remind the reader that these are inspired by work stealing
  and barrier synchonization.  In \mysec\ref{ } we describe the
  semantics of kernels that use these primitives.  In \mysec\ref{ } we
  show how work stealing can be implemented.  In \mysec\ref{ } we show
  how a barrier can be implemented.}

\subsection{Semantics of the new primitives}

\ADComment{General semantics: let $N$ be the maximum number of
  workgroups the GPU of interest can support [gloss over or deal with
    issue that this is kernel-dependent].  A kernel is launched with
  $M$ workgroups, for some $0 < M \leq N$.  Initially these are all
  scheduled, and are guaranteed to make independent forward progress.}

\ADComment{The purpose of ckill is for a workgroup to volunteer to
  quit execution of the kernel if the resources that it is using
  (e.g.\ the associated compute unit) are required for other tasks.  A
  ckill statement must be reached under workgroup-uniform control
  flow.  When a workgroup reaches ckill, the workgroup either
  terminates execution (because the environment requires the
  workgroup's resources), or continues execution as if the ckill
  statement were a no=op).  From a semantics perspective this is a
  non-deterministic choice: an application that invokes ckill should
  be prepared for either outcome.  In practice, the effect of ckill
  depends on environmental conditions.}

\ADComment{The purpose of cfork is for a workgroup to indicate that it
  would be useful to have additional workgroups join the computation,
  and to specify that the program point associated with the cfork
  statement would be a suitable place for a new workgroup to start
  executing.  A cfork statement must also be reached under
  workgroup-uniform control flow.  If $C$ workgroups are executing
  when a workgroup reaches a cfork statement, with $0 < C \leq M$,
  then between 0 and $M-C$ new workgroups commence execution from the
  following statement.  From a semantics perspective, the number of
  workgroups that join the computation is nondeterministic, within the
  permitted range.  In practice, how many workgroups join depends on
  environmental conditions.}

\ADComment{TODO: describe persistent variables.}

\ADComment{TODO: explain that ckill and cfork atomically update the
  kernel context (ids).}

\ADComment{TODO: somewhere we need to explain the constraints this
  places on programming: how ids work, what one can assume about them,
  etc.}

\ADComment{Now describe the more refined semantics where IDs are
  relinquished in decreasing order; explain that from a formal
  perspective thie refines the original semantics.}

\ADComment{In the above, explain at some point how a scheduler can
  exploit cfork and ckill to utilise the GPU resources.}

\subsection{Preemption-aware work stealing}

\ADComment{Show how work stealing can now be made safe.}

\subsection{A preemption-aware global barrier}

\ADComment{Show how the global barrier can be written in a manner that
  maintains contiguous ids using the refined semantics.}


\section{Application-specific optimisations}

\ADComment{Explain that for global barriers, we explain how some new
  primitives can lead to a better implementation.  Explain that for
  work stealing there are various ways the existing ckill and cfork
  primitives can be used.}

\subsection{An optimised barrier implementation}

\ADComment{Remark that the barrier of \mysec\ref{sec:naivebarrier} may
  be inefficient, discuss the use of query primitives to make this
  more efficient.}

\subsection{Variations on work stealing}

\ADComment{Three cases here: always ckill/cfork, ckill/cfork only when
  queue is empty, and have a ``mediator'' workgroup to manage things.}


\section{A mega kernel-based prototype}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}


\section{Experimental evaluation}

\ADComment{TODO: write up notes from whiteboard chat.}


\section{Related work}

\section{Conclusions and future work}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\end{document}
