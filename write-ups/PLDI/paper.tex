%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace]{sigplanconf-pldi16}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}



\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\newcommand{\offerfork}{\mathsf{offer\_fork}}
\newcommand{\offerkill}{\mathsf{offer\_kill}}
\newcommand{\globalbarrier}{\mathsf{global\_barrier}}
\newcommand{\resizingglobalbarrier}{\mathsf{resizing\_global\_barrier}}
\newcommand{\getgroupid}[1]{\mathsf{get\_group\_id(#1)}}
\newcommand{\getnumgroups}[1]{\mathsf{get\_num\_groups(#1)}}


\begin{document}

\title{Cooperative Kernels: Safe Multitasking for Blocking Algorithms on GPUs}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

\paragraph{The needs of irregular data-parallel algorithms}
Acceleration of general-purpose computations on graphics processing
units (GPUs) has tended to focus on \emph{regular} data-parallel
algorithms, for which work to be processed can be evenly split between
the workgroups that execute a GPU kernel ahead of time.  However, many
interesting data-parallel algorithms are \emph{irregular}: the amount
of work to be processed is unknown ahead of time and may be determined
by the computation itself.  There is growing interest in accelerating irregular
data-parallel algorithms on GPUs, particularly algorithms that
traverse and manipulate graphs, with several recent notable
successes~\cite{...}.

Irregular algorithms usually require \emph{blocking synchronization}
between workgroups, to balance load and communicate intermediate
results.  For example, work stealing algorithms require each workgroup
to maintain a queue, which is typically protected by a mutex to enable
stealing by other workgroups.  As another example, breadth-first graph
traversal uses a frontier-based approach that requires a barrier
synchronization between all workgroups to achieve consensus that
exploration of a frontier has been accomplished.  \ADComment{Perhaps
  make the point that inter-workgroup synchronization is possible in
  very rare cases without blocking, e.g. threadfence reduction.}

To operate correctly, a blocking concurrent algorithm requires
\emph{fair} scheduling of threads.  In the context of irregular
data-parallel algorithms accelerated on a GPU, this translates to
requiring fair scheduling of workgroups.  Without fairness, a
workgroup may fail to make progress, leading to starvation.  For
example, if workgroup $A$ holds a mutex and workgroup $B$ is
spin-waiting for the mutex to be released, an unfair scheduler can
forever schedule $B$, so that $A$ never releases the mutex.
Similarly, an unfair scheduler can induce starvation if workgroup $A$
is at an inter-workgroup barrier awaiting the arrival of other
workgroups, by allowing A to spin forever so that the other workgroups
do not progress to the barrier.

\paragraph{Occupancy-bound execution}
Current GPU programming models---CUDA~\cite{...}, OpenCL~\cite{...}
and HSA~\cite{...}---specify very few constraints on the manner in
which workgroups are scheduled.  In particular, they do \emph{not}
specify that workgroups must be fairly scheduled, and implementations
of these programming models do not provide fair scheduling in
practice.  Roughly speaking, each workgroup executing a GPU kernel is
mapped to a hardware \emph{compute unit}.  If a kernel is executed
with more workgroups than there are compute units then at least two
workgroups must share a single compute unit.  The simplest way to
handle this is via an \emph{occupancy-bound} execution model, whereby
once a workgroup has commenced execution on a compute unit (is has
become \emph{occupant}), the workgroup has exclusive access to the
compute unit until it has finished execution, after which a subsequent
workgroup can be bound to the compute unit.  Experiments suggest that
the occupancy-bound execution model is widely implemented by today's
GPU devices and drivers~\cite{...}, and it is evidently a simple model
for device vendors to implement efficiently.

The occupancy-bound execution model does not guarantee fair scheduling
between workgroups: if all compute units are occupied then a
not-yet-occupant workgroup will not be scheduled until some occupant
workgroup completes execution.  Yet the execution model \emph{does}
provide fair scheduling between \emph{occupant} workgroups, because
they are bound to separate compute units that operate in parallel.
Current implementations of irregular data parallel algorithms on GPUs
assume that the target GPU device provides the occupancy-bound
execution model, which they exploit by scheduling no more workgroups
than there are available compute units~\cite{...}.  This works today
because \emph{today}'s GPUs seem to provide the occupancy-bound
execution model.

\paragraph{Resistance to occupancy-bound execution}

Despite its practical prevalence, none of the current GPU programming
models actually mandate occupancy-bound execution.  Further, there are
clear reasons why this model is undesirable from the point-of-view of
GPU vendors and end users.  First, the execution model does not enable
multi-tasking, because a workgroup effectively \emph{owns} a compute
unit until the workgroup has completed execution.  If a long-running
kernel occupies all workgroups, the GPU cannot be used for other
tasks, such as graphics rendering, in the meantime.  Precisely because
of this problem, operating systems employ a GPU \emph{watchdog} to
heuristically detect and kill long-running computations on the GPU
being used for display rendering.  If this watchdog is disabled,
execution of a long-running kernel causes the display to freeze for
the duration of kernel execution.  Second, \emph{energy throttling} is
an important concern for battery-powered devices, such as smartphones
and tablets.  It is desirable for a mobile GPU driver to be able to
power down one or more compute units if the battery level is low or if
the user puts the device into a low power state.  Implementing energy
throttling requires being able to suspend the execution of a
workgroup, power down the compute unit to which the workgroup was
bound, and schedule the workgroup for later execution on a different
compute unit.

Our assessment, informed by discussions with a number of industrial
practitioners who have been involved in the OpenCL and/or HSA
standardisation efforts~\cite{CitePersonalCommunication}, is that
\textbf{(1)} GPU vendors will not commit to the occupancy-bound
execution model that they currently implement, due to the need for
multi-tasking and energy throttling, yet \textbf{(2)} GPU vendors will
not guarantee fair scheduling, due to the runtime overhead associated
with supporting full preemption of workgroups, e.g.\ based on
time-slicing.  Vendors instead wish to retain the essence of the
occupancy-bound execution model, which is simple to implement
efficiently, and provide preemption for the special cases associated
with multi-tasking for high-priority jobs (most crucially graphics)
and energy throttling.  In particular, they do not want to commit to
fair scheduling of a workgroup once it has been preempted.

\paragraph{Our proposal}
%
To summarise: blocking algorithms, such as most irregular algorithms,
demand a degree of fair scheduling of workgroups, but for good reasons
GPU vendors will not commit to the guarantees of even the
occupancy-bound execution model.

We propose \emph{cooperative kernels}, an extension to the GPU
programming model that aims to resolve this impasse.  A GPU kernel
that requires workgroups to be fairly scheduled is identified as a
\emph{cooperative} kernel, and written using two additional language
primitives, $\offerkill$ and $\offerfork$, which must be placed
carefully by the programmer.

At a point where the cooperative kernel could proceed with fewer
workgroups, a workgroup can execute $\offerkill$ to sacrifice itself to
the scheduler.  This indicates to the scheduler that the workgroup
would ideally continue with its execution, but that if required the
scheduler may destroy the workgroup for good: the cooperative kernel
is prepared to deal with either scenario.

At a point where the cooperative kernel could benefit from having
additional workgroups join the computation, a workgroup can execute
$\offerfork$.  This indicates to the scheduler that the cooperative
kernel is prepared to proceed with the existing set of workgroups, but
is able to take advantage of having one or more additional workgroups
commencing execution at the $\offerfork$ program point, inheriting the
state of the workgroup that executed $\offerfork$.

The use of $\offerfork$ and $\offerkill$ creates a contract between
the scheduler and (the programmer of the) cooperative kernel, with
both functional and non-functional requirements.  Functionally: the
scheduler must guarantee that the workgroups executing a cooperative
kernel are fairly scheduled; the cooperative kernel must be robust to
workgroups leaving and joining the computation when the $\offerkill$
and $\offerfork$ functions, respectively, are called.
Non-functionally: a cooperative kernel must ensure that $\offerkill$
is executed frequently enough that the scheduler can accommodate
soft-real time constraints, e.g.\ allowing a smooth frame-rate for
graphics or allowing compute units to be powered down quickly when
required; the scheduler should respond to $\offerkill$ and
$\offerfork$ calls in a manner that avoids under-utilisation of
hardware resources by the cooperative kernel, e.g.\ the scheduler
should not kill workgroups unless they are required for another
purpose.

Cooperative kernels have several appealing properties.  First, they
accommodate the needs of blocking algorithms, including irregular
data-parallel algorithms: such algorithms can be implemented naturally
as cooperative kernels \ADComment{boast that we did a good set}, so
that fair scheduling between workgroups is guaranteed.  Second, they
have no impact on the development of regular (non-cooperative
kernels): these can be programmed exactly as they are now.  Third, the
programming model is backwards compatible: a non-blocking algorithm
implementation that works on today's GPU platforms can be upgraded to
use our language extensions for cooperative scheduling, and will
behave exactly as it does at present if the new $\offerkill$ and
$\offerfork$ functions are simply ignored.  Fourth, the programming
model is easy to implement on top of the occupancy-bound execution
model that current GPU platforms provide; we present a
proof-of-concept implementation on top of OpenCL 2.0 that requires no
special-purpose hardware or driver support.  Fifth, our results
suggest that cooperative kernels can provide efficient multi-tasking
of cooperative and non-cooperative tasks: our experiments across a
range of GPUs from two vendors show very good response times for
scheduling short, non-cooperative kernels concurrently with
long-running cooperative kernels.  These performance results are with
respect to our proof-of-concept, portable implementation; a vendor
specific implementation of cooperative kernels could likely yield
better performance still through device-specific driver and compiler
support.

In summary, our main contributions are:

\begin{enumerate}

\item \emph{Cooperative kernels}, an extension to the GPU programming model to support the scheduling requirements of blocking algorithms  \ADComment{Maybe big up to say that we give semantics to the new operations.}

\item A portable implementation of cooperative kernels on top of the OpenCL 2.0 programming model

\item The porting of \ADComment{TODO} irregular graph algorithms \ADComment{tighten} to the cooperative kernels model

\item \ADComment{The experiments}

\end{enumerate}

\ADComment{Text to maybe work in.}

In traditional co-operative multi-tasking, a task relinquishes control
and regains control in a stateful manner. We propose a lighter weight
model we believe to suitible for the type of blocking algorithms we
are aware of for GPUs.

The scheduler agrees to scheduler the workgroups fairly, but is at
liberty to eliminate workgroups for good when they offer themselves
to be killed. This allows the schduler to repond to changing depands
on the GPU, e.g. to implement energy throttling.

Prototype implementation of the programming model using OpenCL 2.0
advance features to build the scheudler and implement a clang based
compiler to transform input programs using the language extensions to
lower-level programs that interact with the scheduler. Our prototype
simulates multiple independent kernels into a mega-kernel. A vendor
specific implementation could achieve higher efficiency through
modications to the GPU driver that may take advantage of hardware
features. (e.g. mega kernel might not be good to compile). Its
reasonable to think our proof of concept provides a lower bound on how
well we can do.

We ported a large set of blocking algorithms to our programming
model. We evaluate co-operative scheduling with a cooperative kernel
and non-coperative kernel. GPUs from multiple vendors and access
various things.

Our results are promising.

In the evaluation, make a note that the applications we worked on
Namely if they use only a barrier or work-stealing idiom, are
straightforward to adopt to our programming model.



\section{Motivating example: work stealing}

\ADComment{Use the work stealing example to both introduce OpenCL and
  also make our key point: that arbitrary preemption will not work.}


\section{The Cooperative Kernels Programming Model}

We describe our cooperative kernels programming model as an extension
to the industry-standard OpenCL programming model for heterogenous
many-core architectures.  However, the cooperative kernels concept is
more general, and could be applied to extend other GPU programming
models, such as CUDA and HSA.  \ADComment{However, unclear that we could implement it on top of these, but that's not really the point.}

\subsection{Semantics of Cooperative Kernels}

As with a regular OpenCL kernel, a cooperative kernel is launched by
the host application, with a specified number of workgroups, each
consisting of a specified number of threads.  The kernel's behaviour
is described by a kernel function, written in OpenCL C but using the
additional language extensions that we present below, and a number of
parameters to the kernel function are passed by the host application.

The parameters to a cooperative kernel are \emph{immutable}, as if
they were qualified with the \texttt{const} keyword (though pointer
parameters can refer to mutable data).

A regular OpenCL kernel can be executed by a multi-dimensional grid of
workgroups, called an ND-range.  In our current proposal, the ND-range
for a cooperative kernel must be one-dimensional.  \ADComment{Say why?
  And mention that none of the applications that we deem would benefit
  from cooperative kernels need a multi-dimensional ND-range.}

As in regular OpenCL, each workgroup can be multi-dimensional.


\paragraph{Restored variables}

A variable declared in the root scope of the cooperative kernel can
optionally be annotated with a new \texttt{restore} qualifier.  By
annotating a variable with \texttt{restore}, the programmer indicates
that when new workgroups are spawned via $\offerkill$, the threads in
these workgroups should obtain a well-defined initial value for the
variable, obtained from the workgroup that executed $\offerkill$.  We
detail the semantics for this when we describe $\offerkill$ below.

Only private memory variables can be annotated with \texttt{restore},
which may include pointers to local or global memory so long as the
pointer variables themselves are stored in private memory.  In
particular, variables and arrays in local memory cannot be annotated
with \texttt{restore}.  \ADComment{The preceding is what we have
  implemented, but actually it is pretty clear how we could handle
  local memory restoration if we wanted to; it's just that none of our
  examples needed it.}

\paragraph{Active workroups}

If the host application launches a cooperative kernel specifying that
$N$ workgroups should execute the kernel, then when the kernel
commences execution it should be executed by some number of workgroups
$M$, with $1\leq M \leq N$.  The requested number of workgroups is
called the \emph{workgroup limit} for the cooperative kernel, and $M$
is the number of \emph{active workgroups}.  As explained below, the
number $M$ of active workgroups may change during the lifetime of a
cooperative kernel.

Fair scheduling is guaranteed between active workgroups.  That is, if
some thread in an active workgroup is enabled then eventually some
thread in the active workgroup is guaranteed to execute an
instruction.  Note that our programming model does not mandate that
threads within a workgroup must be fairly scheduled, and indeed this
is often not the case in GPU programming models, because threads are
often organised into \emph{warps} or \emph{wavefronts} within a
workgroup, exhibiting lock-step predicated execution that is
inherently unfair.

The initial workgroups have consecutive workgroup ids in the range
$[0, M-1]$.

The OpenCL primitive $\getgroupid{0}$ retrieves the id of a
workgroup as usual.  When executed by a cooperative kernel, the OpenCL
primitive $\getnumgroups{0}$ returns $M$, the current number
of active workgroups.  Because the ND-ranges of cooperative kernels
are one-dimensional, $\getgroupid{\mathit{i}}$ and
$\getnumgroups{\mathit{i}}$ return 0 and 1, respectively, for $i >
0$.

\ADComment{What happens to $M$ when a workgroup reaches the end of a
  cooperative kernel?  Feels like $M$ should decrease, but what does
  this mean for barrier-only kernels?  Do they need to account for the
  number of active workgroups changing during the final barrier
  interval?  An alternative is to say that there is an implicit global barrier at kernel exit.}

\paragraph{Semantics for $\offerkill$}

Our first additional language primitive, $\offerkill$, is a
workgroup-level function: similar to the OpenCL intra-workgroup
$\mathsf{barrier}$ primitive, if $\offerkill$ appears inside
conditional code then it must be executed by \emph{all} threads in a
workgroup, with all threads having executed the same number of
iterations of any enclosing loops.

Suppose a workgroup with id $m$ executes $\offerkill$.  If $m < M-1$,
i.e.\ the workgroup's id is not the largest among the active
workgroups, then $\offerkill$ is a no-op.  If instead $m = M-1$, there
is a non-deterministic choice between (a) $\offerkill$ executing as a
no-op, or (b) execution of the workgroup ceasing and the number of
active workgroups $M$ being decremented by one.  Case (b) corresponds
to the scheduler accepting the workgroup's offer to cease execution,
allowing the scheduler to schedule some other task on the compute unit
on which the workgroup was executing.

The semantics of $\offerkill$ is atomic.  That is, if multiple
workgroups reach $\offerkill$ simultaneously, the semantics is as if
each workgroup executes $\offerkill$ in some arbitrary sequential
order.  \ADComment{Should we say something about linearisability?}

\paragraph{Semantics for $\offerfork$}

Our second new language primitive, $\offerfork$, is also a workgroup-level
function.

Suppose a workgroup with id $m\leq M$ executes $\offerfork$.  An
integer $k$ in the range $[0, N-M]$ is chosen nondeterministically,
$k$ new workgroups are spawned with consecutive ids in the range
$[M, M+k-1]$, and the active workgroup count $M$ is incremented by
$k$.

The $k$ new workgroups commence execution at the program point
immediately following the $\offerfork$ call.  The private variables of
each thread in a new workgroup are all uninitialised, as are any local
variables and arrays shared across the workgroup; reading from these
locations without initialising them is an undefined behaviour.  There
are two exceptions to this:

\begin{itemize}

\item because the parameters to a cooperative kernel are immutable,
  the new threads have access to these parameters and can safely read
  from them;

\item for each variable $v$ annoated with \texttt{restore}, every new
  thread's copy of $v$ is initialised to the value that thread 0 in
  workgroup $m$ held for $v$ at the point of the $\offerfork$ call.

\end{itemize}

The second exception, for restored variables, can be stated more
formally as follows.  Let $\mathit{WG}[x][y].v$ denote the value
stored in variable $v$ of thread $y$ in workgroup $x$.  Then for each
new workgroup $i \in [M, M+k-1]$ and each thread $j$ in workgroup $i$,
$\mathit{WG}[i][j].v$ is initialised to $\mathit{WG}[m][0].v$ for each
variable $v$ annotated with \texttt{restore}.

Notice that $k=0$ is always a legitimate choice for the number of
workgroups to be spawned by an $\offerfork$ call, and if the number of
active workgrouups $M$ is equal to the workgroup limit $N$, $k=0$ is
guaranteed.

\ADComment{Remark on our decision to have every thread inherit the
  state of the master thread from workgroup $m$.}

\paragraph{Global barriers}

Many irregular algorithms require synchronisation across workgroups.
This can be achieved via a \emph{global barrier}, but a global barrier
is not provided as a primitive in OpenCL, CUDA or HSA.  This is
because, as discussed in Section~\ref{ }, an inter-workgroup barrier
requires fair scheduling of workgroups.  As a result, past works have
either split computation into multiple separate kernels, so that the
end of one kernel and the start of the next kernel implicitly provides
a global barrier~\cite{ }, or have assumed the occupancy-bound
execution model to build a global barrier that works reliably on
today's GPU platforms.

Because workgroups of a cooperative kernel are fairly scheduled, a
global barrier primitive can be provided.  We specify two such primitives: $\globalbarrier()$
and $\resizingglobalbarrier()$.

A $\globalbarrier()$ is a kernel-level function: if it appears in
conditional code then it must be reached by \emph{all} threads
executing the cooperative kernel.  On reaching a $\globalbarrier()$, a
thread waits until all threads have arrived at the barrier.  Once all
threads have arrived, the threads may proceed past the barrier with
the guarantee that all global memory accesses issued before the
barrier have completed.

A $\resizingglobalbarrier()$ is identical to a global barrier, except
that, semantically, every workgroup executes $\offerkill$ on reaching
the barrier, and every workgroup executes $\offerfork$ on leaving the
barrier.  Thus $\resizingglobalbarrier()$ is semantically equivalent to:
%
\lstset{basicstyle=\tt}
\begin{lstlisting}
  $\offerkill()$;
  $\globalbarrier()$;
  $\offerfork()$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

The $\globalbarrier()$ primitive can be implemented directly using the
atomic operations provided by the OpenCL 2.0 memory model~\cite{...},
but the implementation is involved, hence why we include this function
as a primitive.  As noted above, the $\resizingglobalbarrier()$
primitive is, semantically, synactic sugar for a sequence of other
primitive calls.  We include it as a primitive because (a) we have
found that global barriers often provide a natural point for
cooperation with the scheduler, and (b) identifying a known point at
which workgroups are required to synchronize and at which workgroups
can be safely killed and forked enables a custom implementation of
$\resizingglobalbarrier()$ that provides the semantics described
above, but is significantly more efficient than the above sequence of
calls.  We discuss our efficient implementation of
$\resizingglobalbarrier$ in detail in Section~\ref{...}.

\subsection{Backwards compatibility}

\ADComment{Perhaps this would fit better under ``programming guidelines'' -- not sure.}

\ADComment{Make the point here that if an irregular algorithm is
  written as a cooperative kernel then it will execute just as safely
  as it does today on current hardware if: (1) the \texttt{restore}
  annotation and $\offerkill$/$\offerfork$ calls are defined away to
  nothing, (2) $\resizingglobalbarrier$ is redefined to
  $\globalbarrier$, (3) a standard implementation of $\globalbarrier$
  that assumes the occupancy-bound execution model is provided.  If the kernel uses blocking features---the global barrier, mutexes, etc., then it may not execute safely due to the possibility of unfair scheduling, but it will execute no less safely than it would if the cooperative kernels style were not used.}

\subsection{Programming guidelines}

An OpenCL programmer writing a cooperative kernel should pay careful
attention to the following details.

\paragraph{Keeping at least one workgroup alive}

The programer is responsible for keeping at least one workgroup alive.  In particular, if all workgroups execute $\offerkill$, the scheduler is at liberty to destroy all workgroups, causing the cooperative kernel to exit.  This can be guarded against by wrapping each $\offerkill$ call in a conditional statement:

\lstset{basicstyle=\tt}
\begin{lstlisting}
  if($\getgroupid{0}$ != 0) $\offerkill$;
\end{lstlisting}
\lstset{basicstyle=\scriptsize\tt}

We could, alternatively, have formulated the semantics of $\offerkill$
so that workgroup 0 is never killed.  However, there might be a use
case for a cooperative kernel reaching a point where it would be
acceptable for the kernel to exit, although desirable for some
remaining computation to be performed if competing workloads allow it.

\paragraph{A changing number of workgroups}

In regular OpenCL, $\getnumgroups{0}$ returns a runtime-constant
value: once the number of executing workgroups has been identified, it
is guaranteed not to change.

In contrast, the number of workgroups executing a cooperative kernel
can vary due to workgroups executing $\offerkill$, $\offerfork$ and
$\resizingglobalbarrier$.  The programmer must therefore write their
cooperative kernel in a manner that is robust to changes in the value
returned by $\getnumgroups{0}$.

The main impact of this relates to partitioning of data to be
processed.

\ADComment{Tyler, can we give an example of a data processing loop
  that would have to be changed?}

\paragraph{Cooperative kernels that only use barriers}

\ADComment{Need to account for the issue that $M$ might decrease as
  workgroups actually exit the kernel.}

If a cooperative kernel does not directly issue $\offerkill$ and
$\offerfork$ calls then the only possibilty for the number of
workgroups to change is due to $\resizingglobalbarrier$ being invoked.

In this case, because the resizing barrier is a kernel-level function,
the statements executed by the cooperative kernel can be divided into
\emph{resizing barrier intervals}: a resizing barrier interval starts
at a resizing barrier or the kernel entry point, and ends at a
resizing barrier or a kernel exit point.  At any moment of execution,
all threads are guaranteed to be executing within the same resizing
barrier interval.  As a result, the number of workgroups executing the
kernel is guaratneed to remain unchanged within each resizing barrier
interval, so the result returned by $\getnumgroups{0}$ is guaratneed
to be identical on repeated calls during such an interval.  This
guarantee may be exploited for efficiently when writing a cooperative
kernel that uses global barriers but does not call $\offerfork$ and
$\offerkill$ directly.

\subsection{Non-functional Requirements of Cooperative Kernels}

The semantics presented in Section~\ref{...} describe the envelope of
behaviour that a developer of a cooperative kernel should be prepared
for.

However, the aim of cooperative kernels is to find a balance that
allows \emph{efficient execution} of long-running kernels
(e.g.\ implementing irregular data-parallel algorithms), and
\emph{responsive multi-tasking} so that the GPU can be shared between
one or more cooperative kernels and a number of short, high-priority
tasks with soft real-time constraints.

To achieve this balance, both an implementation of the ooperative
kernels model, and the programmer of a cooperative kernel, must strive
to meet a number of non-functional requirements, which we outline
here.

\paragraph{Sufficient $\offerkill$ calls}

Recall that the purpose of $\offerkill$ is to provide the scheduler
with an opportunity to destroy a workgroup in order to schedule
higher-priority tasks.  Because these higher-priority tasks, such as
graphics rendering, may have soft real-time constraints, the scheduler
relies on the cooperative kernel to execute $\offerkill$ sufficiently
frequently that the scheduler can meet such real-time constraints.

\ADComment{Got here.}




\subsection{Preemption-aware work stealing}

\ADComment{Remind the reader that these are inspired by work stealing
  and barrier synchonization.  In \mysec\ref{ } we describe the
  semantics of kernels that use these primitives.  In \mysec\ref{ } we
  show how work stealing can be implemented.  In \mysec\ref{ } we show
  how a barrier can be implemented.}

\ADComment{Show how work stealing can now be made safe.}

\subsection{A preemption-aware global barrier}

\ADComment{Show how the global barrier can be written in a manner that
  maintains contiguous ids using the refined semantics.}


\section{Application-specific optimisations}

\ADComment{Explain that for global barriers, we explain how some new
  primitives can lead to a better implementation.  Explain that for
  work stealing there are various ways the existing ckill and cfork
  primitives can be used.}

\subsection{An optimised barrier implementation}

\ADComment{Remark that the barrier of \mysec\ref{sec:naivebarrier} may
  be inefficient, discuss the use of query primitives to make this
  more efficient.}

\subsection{Variations on work stealing}

\ADComment{Three cases here: always ckill/cfork, ckill/cfork only when
  queue is empty, and have a ``mediator'' workgroup to manage things.}


\section{A mega kernel-based prototype}

\ADComment{Explain carefully that doing this properly would require
  driver support, etc., and then explain what we've actually done.}


\section{Experimental evaluation}

\ADComment{TODO: write up notes from whiteboard chat.}


\section{Related work}

\section{Conclusions and future work}

\ADComment{Next-generation GPUs and drivers for evaluation,
  implementation in a real driver, migration of work between multiple
  GPUs.}

\section{Old intro}


\ADComment{Points to make: It is important to be able to execute
  long-running, cooperative applications on GPUs (give examples).
  However, resource sharing and resource flexibility is also important
  (give examples, including sharing resources with graphics, sharing
  resources between compute taks, and being flexible about resources
  under energy constraints), and these are not supported by today's
  GPU platforms: long-running applications cause display freezes, etc.
  Resource-based preemption, whereby workgroup execution is postponed
  to free up resources for other purposes (or to reduce resources to
  save energy) can potentially enable resource sharing.  There is
  evidence in the research literature that efficient resource sharing
  for GPUs is possible (cite academic works), and signs from industry
  that this is coming in future architectures.  [Explain more clearly
    what we mean by resource-based preemption.]  Unfortunately,
  arbitrary resource-based preemption is fundamentally incompatible
  with cooperative applications---only trivial examples of cooperation
  are safe [reduction, load balancing via atomic increment]; the HSA
  model of guaranteeing preemption in reverse order of IDs allows some
  cooperative applications to be implemented.  Still, in the general
  case even a mutex cannot be used safely.}

\ADComment{Say that we believe the problem can only be solved through
  programming language innovations with appropriate runtime support.
  We have observed inter-workgroup barriers and work stealing to be
  the main use cases for cooperation, and so in this paper propose two
  new language primitives, ckill and cfork, designed to meet the needs
  of these primitives.  [Describe them a bit.]}

\ADComment{Move on to our main contributions}

\ADComment{This is what we had before.}

Graphics processing units (GPUs) are highly parallel co-processors that are now incorporated into many
devices, ranging from the world's most powerful supercomputers to battery-powered devices such as tablets and mobile phones. While
originally designed to accelerate graphics applications, GPUs are now
used in a wide range of applications, including molecular simulations
and weather forecasting. \ADComment{Expand the list of application areas a bit more; mention their attractiveness due to energy-efficiency.}

New application domains for GPUs are enabled by general purpose
programming languages with associate compilation tools that target GPU architectures.
The current most popular GPU programming languages are CUDA and OpenCL. These languages allow developers to
write C-like programs that can be dispatched and executed on a GPU
device.

GPUs are massively parallel, employing many threads to execute a
program.  These threads are organised hierarchically: groups of threads
are called \emph{workgroups}, and threads interact differently depending on whether they reside in the same workgroup or not.  Traditionally, applications accelerated by GPUs are
data-parallel, requiring little interaction between threads. A small
set of bulk synchronisation primitives are provided and any
inter-thread communication must be performed around these primitives.  \ADComment{Do we want to make the point that this is traditionally limited to the intra-workgroup case?}

While this traditional way to program GPUs has proved useful, it also
has limitations for some applications, in particular applications that
expose a dynamic amount of parallelism throughout execution. Such
applications are said to have irregular parallelism (we call such
applications \emph{IP applications}). Efficient parallel computation
of IP applications requires finer-grained synchronisation than that
which is provided by GPU language primitives.

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.


\end{document}
