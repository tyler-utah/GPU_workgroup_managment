%
% LaTeX template for prepartion of submissions to PLDI'16
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'16 web site.
% 
\documentclass[nocopyrightspace]{sigplanconf-pldi16}
% \documentclass[pldi-cameraready]{sigplanconf-pldi16}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink

\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{calc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xspace}
\usepackage{array}
\usepackage{multirow}
\usepackage{varwidth}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\usepackage{xcolor,colortbl}
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{subfig}

\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI


%\newcommand{\todo}[1]{}
%\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}
\newcolumntype{C}[1]{>{\centering}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\ADComment}[1]{\textcolor{green}{AD: #1}}
%\newcommand{\TSComment}[1]{\textcolor{blue}{TS: #1}}
%\newcommand{\COULDCUT}[1]{\textcolor{blue}{#1}}

\newcommand{\myfig}{Fig.~}
\newcommand{\myfiglong}{Figure~}
\newcommand{\myfigs}{Figs.~}
\newcommand{\mytab}{Tab.~}
\newcommand{\mytablong}{Table~}
\newcommand{\mysec}{Sec.~}
\newcommand{\myalg}{Alg.~}
\newcommand{\nvidia}{Nvidia\xspace}
\newcommand{\totalcombinations}{70\xspace}
\newcommand{\positivecombinations}{55\xspace}
\newcommand{\cbedot}{\emph{cbe-dot}\xspace}
\newcommand{\cbeht}{\emph{cbe-ht}\xspace}
\newcommand{\ctoctree}{\emph{ct-octree}\xspace}
\newcommand{\tpotm}{\emph{tpo-tm}\xspace}
\newcommand{\sdkred}{\emph{sdk-red}\xspace}
\newcommand{\sdkrednf}{\emph{sdk-red-nf}\xspace}
\newcommand{\lsbh}{\emph{ls-bh}\xspace}
\newcommand{\lsbhnf}{\emph{ls-bh-nf}\xspace}
\newcommand{\cubscan}{\emph{cub-scan}\xspace}
\newcommand{\cubscannf}{\emph{cub-scan-nf}\xspace}
\newcommand{\heuristiccombinations}{eight\xspace}
\newcommand{\effectivethreshold}{5\%\xspace}
\definecolor{Gray}{gray}{0.85}

% lstlisting macros
\lstset{
  language=C, % choose the language of the code
  showspaces=false, % show spaces adding particular underscores
  showstringspaces=false, % underline spaces within strings
  showtabs=false, % show tabs within strings adding particular underscores
  tabsize=8, % sets default tabsize to 2 spaces
  captionpos=b, % sets the caption-position to bottom
  mathescape=true, % activates special behaviour of the dollar sign
  basicstyle=\scriptsize\tt,
  columns=fullfelxible,
  %commentstyle=\rmfamily\itshape,
  morekeywords={barrier,kernel,global,__device__,__syncthreads,__global__,bool,then},
  escapeinside={(*@}{@*)}
}



\newcommand{\code}[1]{\lstset{basicstyle=\tt}\lstinline!#1!\lstset{basicstyle=\scriptsize\tt}}

\newcommand{\MP}{\textsf{MP}}
\newcommand{\LB}{\textsf{LB}}
\newcommand{\SB}{\textsf{SB}}
\newcommand{\noisethreshold}{3\xspace}
\newcommand{\TotalLitmusTests}{196.6\xspace}
\newcommand{\loadop}{\textsf{ld}\xspace}
\newcommand{\storeop}{\textsf{st}\xspace}

\newcommand{\Td}[2]{{#1}_{#2}}
\newcommand{\Tdl}[3]{\langle \Td{#1}{#2}, #3 \rangle}
\newcommand{\Tdlsigma}[4]{\Tdl{#1}{#2}{#4@#3}}

\newcommand{\randstrsolo}{\emph{rand-str}\xspace}
\newcommand{\sysstrsolo}{\emph{sys-str}\xspace}
\newcommand{\nostrsolo}{\emph{no-str}\xspace}
\newcommand{\cachestrsolo}{\emph{cache-str}\xspace}

\newcommand{\randstr}[1]{\emph{rand-str}#1}
\newcommand{\sysstr}[1]{\emph{sys-str}#1}
\newcommand{\nostr}[1]{\emph{no-str}#1}
\newcommand{\cachestr}[1]{\emph{cache-str}#1}


\newcommand{\NumGPUs}{seven\xspace}
\newcommand{\NumApplications}{ten\xspace}
\newcommand{\NumApplicationsUnique}{seven\xspace}

\begin{document}

\title{Responsive GPU Workgroup Management in the Presence of
  Persistent Kernels}

%
% any author declaration will be ignored  when using 'pldi' option (for double blind review)
%

\authorinfo{}
{\makebox{} \\
}
{}

%\authorinfo{Tyler Sorensen}
%{\makebox{Imperial College London, UK} \\
%}
%{t.sorensen15@imperial.ac.uk}

%\authorinfo{Alastair F. Donaldson}
%{\makebox{Imperial College London, UK} \\
%}
%{alastair.donaldson@imperial.ac.uk}


\maketitle

\begin{abstract}


\end{abstract}
    
\section{Introduction}\label{sec:intro}

Graphics processing units (GPUs) are highly parallel co-processors that are now incorporated into many
devices, ranging from the world's most powerful supercomputers to battery-powered devices such as tablets and mobile phones. While
originally designed to accelerate graphics applications, GPUs are now
used in a wide range of applications, including molecular simulations
and weather forecasting. \ADComment{Expand the list of application areas a bit more; mention their attractiveness due to energy-efficiency.}

New application domains for GPUs are enabled by general purpose
programming languages with associate compilation tools that target GPU architectures.
The current most popular GPU programming languages are CUDA and OpenCL. These languages allow developers to
write C-like programs that can be dispatched and executed on a GPU
device.

GPUs are massively parallel, employing many threads to execute a
program.  These threads are organised hierarchically: groups of threads
are called \emph{workgroups}, and threads interact differently depending on whether they reside in the same workgroup or not.  Traditionally, applications accelerated by GPUs are
data-parallel, requiring little interaction between threads. A small
set of bulk synchronisation primitives are provided and any
inter-thread communication must be performed around these primitives.  \ADComment{Do we want to make the point that this is traditionally limited to the intra-workgroup case?}

While this traditional way to program GPUs has proved useful, it also
has limitations for some applications, in particular applications that
expose a dynamic amount of parallelism throughout execution. Such
applications are said to have irregular parallelism (we call such
applications \emph{IP applications}). Efficient parallel computation
of IP applications requires finer-grained synchronisation than that
which is provided by GPU language primitives.

In an effort to achieve high performance for IP applications, new GPU
programming paradigms have been developed that include custom GPU
synchronisation constructs. Here we focus on two \ADComment{The way this is worded it is unclear whether you are describing two programming paradigms, or two synchronisation constructs}:

\begin{enumerate}
\item {\bf Inter-workgroup barrier}: Workgroups are able to
  efficiently synchronise frequently using a custom inter-workgroup
  barrier that is not provided as GPU primitive.

\item {\bf Work stealing}: Computation is divided into tasks that are
  stored in workgroup-local queues. If a workgroup runs out of local
  tasks to compute, it is able to steal a task from a different
  workgroup.
\end{enumerate}

Both paradigms fit into what are called \emph{persistent kernels}, the
defining characteristic being that persistent kernels use a relatively
small number of long-running threads to perform computation. In
contrast, traditional GPU programs employ many short running threads
to perform computation. In this work, the persistent kernels we consider
use either work stealing or inter-workgroup barrier paradigms.

Persistent kernels rely on strong forward progress properties (FPPs)
between workgroups; that is, workgroups must make forward progress with respect to
the other workgroups that are executing. In the case of the inter-workgroup barrier, all
workgroups spin at a barrier instruction waiting for all workgroups to
arrive. If one (or more) workgroups stop making forward progress, the
application will deadlock as the live workgroups will wait
indefinitely for the halted workgroups \ADComment{Can we re-word the preceding to make it clearer that the workgroups have been stalled by a scheduler: they have not actually terminated their execution by reaching the end of the kernel}. Less obviously, in the work
stealing paradigm, the workgroups terminate only when \emph{all} tasks
have been completed. If a workgroup takes a task, but stops making
forward progress before the task is completed, the application will
deadlock as the live workgroups will spin waiting for the halted
workgroup to finish its task.

The current GPU programming models generally do not provide strong
enough inter-workgroup FPP to allow persistent kernels. \ADComment{This might be a good place to cite the specs.}  This is
because GPUs do not provide workgroup preemption. That is, GPU
programming models allow GPU programs to be launched with many more
workgroups than the available hardware resources are able run
concurrently. The GPU runtime will schedule as many workgroups as the
available resources can provide, and the remaining workgroups will
wait until hardware resources become available, e.g.  when one of the
scheduled workgroups finishes execution. There is no mechanism to swap
workgroups from 'executing' to 'waiting' and vice versa. Workgroups
that have been scheduled and have not finished execution are called
\emph{occupant}.  \ADComment{Note: it's ironic that lack of preemption is what leads to the problem of workgroups being scheduled in waves, and yet also what means we might not be able to guarantee the properties of the occupancy-bound execution model in the future.}

Previous work experimenting with persistent kernels has required
\emph{a priori} knowledge of the number of workgroups that the GPU
hardware is able to execute the program with concurrently. This
number, called the \emph{maximal launch} or \emph{occupancy bound}, is
a function of both a GPU chip and a compiled program. Because occupant
workgroups are never preempted, if the GPU program is launched with a
number of workgroups which doesn't exceed the occupancy bound, then
all workgroups will be occupant and the strong inter-workgroup FPP
required by persistent kernels are provided. Using the occupancy
bound, previous work has successfully implemented persistent kernels
and shown them to be performant on current GPUs. Recent work has even
shown that the occupancy bound can be accurately discovered
programitically, removing the need for any \emph{a priori} knowledge
about the GPU.

Despite the promising performance and ease of programming persistent
kernels with occupant workgroups, GPU language specification
committees are reluctant to commit to the inter-workgroup FPP
currently provided by occupant workgroups. That is, even if current
GPUs provide strong FPP between workgroups, future GPUs may not. Thus
the performance gains enabled by persistent kernels may not be
safely achievable for future GPUs.

We believe that two fundamental issues stopping committees from
committing to strong inter-workgroup FPP are:

\begin{itemize}
\item {\bf Responsive GPU multi-tasking} A high priority GPU task
  (e.g. graphics) may need to quickly gain access to the GPU. To do
  this, some of the resources will have to be taken from the
  persistent kernel in order to run the high priority task. This will
  reduce the number of occupant workgroups executing the persistent
  kernel.

\item {\bf Energy throttling} Some GPUs, especially in mobile devices,
  may wish to monitor the energy consumed (i.e. heat output) of the
  GPU chip.  A GPU that is consuming too much energy may be throttled
  by limiting the amount of resources (and thus the number of occupant
  workgroups) to conserve battery life.
\end{itemize}

Both issues are a case of the GPU runtime needing to adjust the
resources used by the persistent kernel during execution. This causes
occupant workgroups to potentially be preempted, which may lead to
deadlock in persistent kernels. \ADComment{It might be good to illustrate the previous points with an example; e.g.\ we could envisage a kernel that is being executed concurrently by 16 workgroups.  Some (4 say) have reached an inter workgroup barrier and are awaiting the others.  But then a graphics tasks causes the 12 workgroups that have not yet reached the barrier to be preempted, and in anticipation of further graphics processing requirements the scheduler decides not to reduce the workgroup allocation for the kernel to 4 indefinitely.  The 4 workgroups waiting at the barrier cannot now make progress.  Energy throttling example is analogous, except that the count is reduced from 16 to 12 due to low battery.}
A less severe, but still undesirable
issue is if the runtime determines more GPU resources are available
during a persistent kernel execution. In this case, the persistent
kernel will be under-utilising GPU resources. \ADComment{I'm not sure I get this one.  Do you mean here that at the point of kernel launch there may be e.g. only 4 workgroup slots available, say due to low battery, but then later on (e.g., due to a phone being charged) there may be scope for more workgroups to run in parallel?}

\ADComment{We should mention that GPU watchdogs are the rather drastic solution to this problem, to keep the overall system responsive, but that they do not help from the point of view of IP applications themselves.}

In this work, we propose a solution that unites persistent kernels
with the need for GPU runtimes to dynamically (and quickly) adjust the
resources available to the persistent kernel. We propose a new GPU
programming paradigm called \emph{dynamic persistent kernels}, or
DPK. DPKs provide strong inter-workgroup FPP while still allowing the
GPU runtime to adjust the number of workgroups executing the DPK in a
timely manner, enabling responsive GPU multi-tasking or energy
throttling.

The key insight behind DPK, is that many persistent kernels contain
points in their execution where workgroups can easily be either
terminated or started. We propose several new instructions to mark
these entry and exit points. These instructions serve as hooks for the
GPU runtime to adjust the amount of resources available to the DPK,
and thus the number of occupant workgroups the DPK can rely on to have
strong FPPs. \ADComment{We should make it clear early on that some care on the part of the programmer is required to make this work, and that if the programmer does not insert sufficient yield points, the GPU watchdog may still have to kill a kernel.}

For example, many applications using the inter-workgroup barrier
paradigm do not carry workgroup-local information across calls to the
inter-workgroup barrier. Thus the number of workgroups can be adjusted
at every call to the inter-workgroup barrier. Many applications using
the work-stealing paradigm do not carry state between task
computation; thus workgroups can be created or terminated immediately
after ever task computation.

While we envision support for DPKs to ultimately be implemented in
proprietary GPU drivers, we are able to provide a prototype
implementation in software using the strong inter-workgroup FPP
available on current GPUs. This allows us to test the feasibility of
our approach and as well get upper bounds for the performance cost and
the responsiveness of multi-tasking.

\end{document}
