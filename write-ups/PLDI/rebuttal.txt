Thanks for the insightful reviews and references, they are really
helpful!

* Reviewer A: thanks for the very positive review. Could you consider
championing the paper?  You wrote: "Overall, this was a very nice
piece of work ... All I could really ask for is some additional detail
and cleanup in the writing", which sounds like you could :-) We can
certainly provide this detail, taking on board points from all the
reviewers.

* Hardware support for pre-emption (reviewers C and D):

Hardware support would actually complement what we propose. First,
efficient HW pre-emption would allow cooperative kernels to be
implemented very efficiently. Second, our method for allowing the
programmer to specify smart pre-emption points would allow the driver
to invoke hardware-based pre-emption judiciously (minimising
overhead). Third, the ability to arbitrarily pre-empt when necessary
would alleviate the valid concern that a rogue cooperative kernel
might cause starvation. Fourth and finally, just because HW
pre-emption exists does not mean it will provide fairness guarantees
to software. "Async Compute", while interesting, does not make any
fairness guarantees.

We recently presented the work to the ARM Mali GPU team, who were
positive about our ideas, since their proposed hardware pre-emption will
be very coarse-grained and thus expensive.

* Chicken and egg problem:

Until pre-emption is more widely supported in GPUs, or the primitives
necessary to implement software solutions such as ours become widely
available, it is impossible to fully understand the fairness
guarantees that future HW support might provide, and the relative
overheads. Our submission is a forward-looking paper that we believe
is high risk, in that future developments in the area may provide
other alternatives, but with potentially very high value, because we
offer a platform-independent language-based solution that could be
integrated into OpenCL. A PLDI paper on topic would be a platform for
conversations around this topic in the years to come, as hardware
solutions are actually being implemented.

* Performance (reviewers C and D):

we will clarify each issue in the final version

- 1.4x slowdown: It appears you are referring to figure 5 (overhead
for N-1) and we believe that there is a misunderstanding. This is the
overhead of only the cooperative kernel while running a *heavy*
graphics workload simultaneously. Sharing the GPU between multiple
workloads *should* provide a slowdown for an individual workload. 1.4x
for sharing resources with a heavy workload is not surprising. There
is no slowdown due to multi-tasking on the duration of the
non-cooperative; only due to the number of resources executing the
task (i.e. a non-cooperative kernel run with half as many cores will
take twice as long).

- Performance slowdowns for adding our constructs (table 3): The
highest slowdowns were on the oldest chip (520) meaning that newer
chips are not suffering as much slowdown (mean of 1.08x slowdown on
the newest chip).

- Our mega-kernel approach is not how we envision our ideas being
implemented in practice. Our software solution is an upper bound on
what a driver/runtime/hardware solution could achieve. Given this, we
believe our results are encouraging.

- Acceptable performance characteristics vary between domains. We have
made the effort to implement this (which required a great deal of
engineering) and it is surely useful to have these performance
figures, to inform future research.

* Specific questions raised by the reviews

* Reviewer A:

- Question 1: Yes, the barrier we used was based on the Xiao and Feng
implementation of [44], and subsequently examined in [33].

- Question 6: Good observation; in all of our cases offer_fork and
offer_kill are paired up. None of our use cases required them being
split, although having them split provides flexibility for future
applications.

- Question 4: We envision that the HW support for the occupancy bound
execution model will continue to be available (it is straightforward
after all), but new GPU features (multi-tasking, energy throttling,
etc.) may break the occupancy bound model fairness guarantees to
programmers. Cooperative kernels are a contract between the runtime
and programmer. The runtime must guarantee to provide cooperative
kernels the occupancy bound fairness properties either by (a) actually
implementing the occupancy bound model or (b) simulating it through
pre-emption that provides sufficient fairness properties.

* Reviewer B:

- w.r.t. only allowing 2 kernels to multitask: Our use case is
graphics and compute co-existing, thus two kernels is the simplest
form of that, which we believe provides meaningful (and easy to
understand) benchmark data. Additionally, the mega-kernel approach
requires significant engineering. Because we envision our approach to
be implemented in runtime/driver/hardware, the value of an
over-engineered software prototype is unclear.

* Reviewer C:

- Question 1: as opposed to dynamic parallelism, offer_fork does not
require new workgroups be spawned (only if they are available).
Additionally, dynamic parallelism is Nvidia/CUDA exclusive. We target a
broader range of GPUs. The OpenCL equivalent, nested parallelism,
enqueues a kernel to be executed, but only after the parent finishes.

- Question 2: We believe a compiler-based approach could offer hints
(based on some kind of live variable analysis), but in general to be
very complex. This is because the number of executing workgroups
changes with offer_fork, offer_kill. Along these lines, we expect
frameworks which generate OpenCL code (e.g. [27]) to be able to
produce these automatically.

* Reviewer D:

- w.r.t. transmitting thread 0's values: The values we needed to
transmit in our application were uniform across the workgroup so it
did not matter.  You are right about bugs and we can easily change
this in the semantics and implementation.

- w.r.t. richer query semantics: See answer below; query() has some
tricky semantics that we did not want to expose to the programmer (it
should be used only inside a resizing_barrier().

- w.r.t. to the gap between query and kill: You are right, and this is
why query() is slightly trickier. Essentially, it creates a contract
between the cooperative kernel and scheduler that the number of
workgroups it specifies *must* be killed regardless of what else
happens. Because of this complexity, we did not want to expose this as
a primitive and instead just use it inside the efficient barrier.
