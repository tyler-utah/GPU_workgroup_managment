R1: Choice of primitives (our semantics vs. yield semantics)

In the case of a global barrier, adopting yield() semantics would
force the cooperative kernel to block *completely* in the case where a
single workgroup executes yield(), stalling the kernel until the given
workgroup resumes.  Instead, our offer_kill allows a kernel to make
progress with a smaller number of workgroups, with workgroups
potentially joining again later via request_fork.  Thus our semantic
deviation is by design, with specific benefits that we will explain
better.

R1: Explain why GPU vendors would burden programmers with
cooperative multitasking, rather than offering preemptive multitasking.

Preemptive multitasking for GPU workgroups is expensive as it requires
managing the state of all the threads in the workgroup (256 for Intel
HD5500) as well as the workgroup local memory (65 KB for Intel
HD5500). Our cooperative kernel primitives are simple: in most
applications, they reside in a barrier primitive. Additionally, our
method would complement, not compete with, future devices that may
support hardware preemption (see next point).

R1: The possibility of more mature hardware preemption in the future

We argue in the paper (and can emphasise more strongly) that
cooperative kernels can *complement* the scenario where there is
hardware support for preemption: the programmer can use domain
knowledge to suggest optimal points for preemption, but the GPU driver
can exploit arbitrary hardware preemption to improve responsiveness if
needed.

R1: Scientific novelty

There is scientific novelty in applying the general idea of
cooperative multitasking in the specific and emerging domain of GPU
programming, and showing that in this context (which differs in many
respects from previous applications of cooperative multitasking) it is
possible to achieve promising performance results for a range of
benchmarks.

R1: Engineering novelty

There is engineering novelty in the design of our scheduler, the
design of our smart resizing global barrier, and in the adaptation of
a set of benchmarks to work with our programming model.  While our
scheduler is a mock-up, the same design could be used for an
implementation inside a GPU driver, by separating out the various
components of the mega-kernel.  It is clear that a GPU driver
implementation would only perform *better* than our mock-up, hence our
experiments via the mock-up demonstrate that our approach is viable
performance-wise.

R2: Thank you for your supportive remarks.  It could indeed be
interesting to investigate the impact of imposing a minimum number of
workgroups.

R3: What type of applications would benefit from this model?

Applications that contain irregular parallelism would benefit from
this approach. Examples of day-to-day applications that use these, or
similar, algorithms are speech decoding (graph traversal), GPS
routeing (graph traversal), and mesh rendering (octree
partitioning). We will add references for these applications.

R3: Choice of device-based scheduling

We present the scheduling mechanism and semantics of cooperative
kernels in a manner that is agnostic to whether it is implemented on
host or device (section 4.2); Our prototype implements the logic on
the device to minimize expensive host-device communication.
